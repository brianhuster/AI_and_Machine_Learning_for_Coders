{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/Chapter_7_Recurrent_Neural_Networks_for_Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQHyC-A4Gqg_"
      },
      "source": [
        "# Chapter 7: Recurrent Neural Networks for Natural Language Processing üåê  \n",
        "\n",
        "Ch√†o m·ªçi ng∆∞·ªùi nha, t·ª•i m√¨nh l·∫°i g·∫∑p nhau r·ªìi üåª.  \n",
        "·ªû ch∆∞∆°ng n√†y, t·ª•i m√¨nh s·∫Ω c√πng t√¨m hi·ªÉu v·ªÅ vi·ªác s·ª≠ d·ª•ng c√°c **m·∫°ng h·ªìi quy (Recurrent Neural Networks - RNNs)** ƒë·ªÉ x·ª≠ l√Ω c√°c b√†i to√°n v·ªÅ **ng√¥n ng·ªØ t·ª± nhi√™n (Natural Language Processing - NLP)** nha. üòä"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVRrBIfdtFiC"
      },
      "source": [
        "![RNN image](https://www.simplilearn.com/ice9/free_resources_article_thumb/Fully_connected_Recurrent_Neural_Network.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esbtLHrzI3m-"
      },
      "source": [
        "### Tr∆∞·ªõc khi b∆∞·ªõc v√†o ch∆∞∆°ng m·ªõi, t·ª•i m√¨nh c√πng √¥n l·∫°i 2 ch∆∞∆°ng tr∆∞·ªõc ƒë√≥ nha üìñ  \n",
        "\n",
        "·ªû **Ch∆∞∆°ng 5**, m·ªçi ng∆∞·ªùi ƒë√£ ƒë∆∞·ª£c t√¨m hi·ªÉu v·ªÅ vi·ªác **m√£ h√≥a c√°c c√¢u** th√†nh d·∫°ng **chu·ªói** hay **tensor** c·ªßa s·ªë th√¥ng qua c∆° ch·∫ø **Tokenizer**.  \n",
        "\n",
        "·ªû **Ch∆∞∆°ng 6**, ch√∫ng ta l·∫°i t√¨m hi·ªÉu th√™m v·ªÅ **vector bi·ªÉu di·ªÖn** hay **c∆° ch·∫ø embedding**, th√¥ng qua vi·ªác ƒë∆∞a c√°c t·ª´ l√™n **kh√¥ng gian cao h∆°n** gi√∫p **truy xu·∫•t ƒë∆∞·ª£c nhi·ªÅu ng·ªØ nghƒ©a h∆°n**, ƒë·ªìng th·ªùi x√¢y d·ª±ng **m·ªëi li√™n k·∫øt gi·ªØa c√°c vectors** ƒë·∫°i di·ªán t·ª´ c√≥ **nh√≥m ng·ªØ nghƒ©a t∆∞∆°ng ƒë∆∞∆°ng nhau**.  \n",
        "\n",
        "Tuy nhi√™n, b·∫°n c≈©ng nh·∫≠n th·∫•y r·∫±ng v·ªÅ c∆° b·∫£n th√¨ c√°c ph∆∞∆°ng ph√°p tr√™n ƒë·ªÅu ho·∫°t ƒë·ªông gi·ªëng nh∆∞ **m·ªôt t√∫i ch·ª©a c√°c t·ª´**, kh√¥ng ph√¢n bi·ªát v·ªÅ **th·ª© t·ª± c·ªßa ch·ªØ**. Trong khi ƒë√≥, trong **ng√¥n ng·ªØ**, **th·ª© t·ª±** c·ªßa c√°c t·ª´ l·∫°i ƒë√≥ng vai tr√≤ v√¥ c√πng **quan tr·ªçng v·ªÅ m·∫∑t ng·ªØ nghƒ©a**.  \n",
        "\n",
        "V√≠ d·ª•:  \n",
        "- T·ª´ **\"blue\"** ƒë·ª©ng m·ªôt m√¨nh c√≥ v·∫ª kh√° thi·∫øu nghƒ©a, n√≥ c√≥ th·ªÉ l√† **bu·ªìn** ho·∫∑c ch·ªâ **m√†u xanh**, c≈©ng gi·ªëng nh∆∞ t·ª´ **\"sky\"** v·∫≠y. Nh∆∞ng khi k·∫øt h·ª£p l·∫°i, **\"blue sky\"** s·∫Ω c√≥ nghƒ©a l√† **\"b·∫ßu tr·ªùi xanh\"**, trong ƒë√≥ **t√≠nh t·ª´ b·ªï nghƒ©a cho danh t·ª´**.  \n",
        "- C√°c t·ª´ kh√°c nh∆∞ **\"writing desk\"**, **\"rain cloud\"** c≈©ng t∆∞∆°ng t·ª±.  \n",
        "- Ho·∫∑c v√≠ d·ª• nh∆∞ t·ª´ **\"brain\"** v√† **\"storm\"**, m·ªôt t·ª´ nghƒ©a l√† **n√£o**, m·ªôt t·ª´ l√† **b√£o**, nh∆∞ng khi k·∫øt h·ª£p th√†nh **\"brainstorm\"**, ch√∫ng l·∫°i c√≥ nghƒ©a l√† **ƒë·ªông n√£o**.  \n",
        "\n",
        "M·ªôt v√≠ d·ª• kh√°c r√µ h∆°n l√† trong c√°c c·ª•m **idioms**, khi gh√©p c√°c t·ª´ l·∫°i, ch√∫ng mang **nghƒ©a b√≥ng** thay v√¨ **nghƒ©a ƒëen**.  \n",
        "\n",
        "---\n",
        "\n",
        "C√°c m√¥ h√¨nh c∆° b·∫£n, th√¥ng th∆∞·ªùng tr∆∞·ªõc ƒë√≥ **kh√¥ng quan t√¢m ƒë·∫øn th·ª© t·ª± t·ª´**, ch√∫ng ch·ªâ bi·∫øt r·∫±ng ch√∫ng c√≥ nh·ªØng t·ª´ ƒë√≥ v√† ƒë∆∞a l√™n **kh√¥ng gian chi·ªÅu cao h∆°n**, sau ƒë√≥ ƒë·∫øn c√°c l·ªõp tuy·∫øn t√≠nh ƒë·ªÉ t√≠nh to√°n. ƒêi·ªÅu n√†y **kh√¥ng mang t√≠nh √Ω nghƒ©a cao** khi x√©t theo chu·ªói.  \n",
        "\n",
        "M·ªçi ng∆∞·ªùi c√≥ th·ªÉ li√™n t∆∞·ªüng ƒë·∫øn tr∆∞·ªùng h·ª£p **antigram** m√† t·ª•i m√¨nh ƒë√£ t√¨m hi·ªÉu trong **ch∆∞∆°ng 4** ‚Äì ·ªü ƒë√¢y c≈©ng t∆∞∆°ng t·ª±, nh∆∞ng l√† ·ªü c·∫•p t·ª´ thay v√¨ k√Ω t·ª±.  \n",
        "\n",
        "---\n",
        "\n",
        "Ch√≠nh v√¨ nh·ªØng **m·∫∑t h·∫°n ch·∫ø ng·ªØ nghƒ©a** n√†y m√† ta c·∫ßn m·ªôt ph∆∞∆°ng ph√°p ti·∫øp c·∫≠n m·ªõi ƒë·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ. M√¥ h√¨nh c·∫ßn c√≥ kh·∫£ nƒÉng **nh√¨n l·∫°i qu√° kh·ª©**, **nh·ªõ l·∫°i ƒë∆∞·ª£c nh·ªØng t·ª´ ph√≠a tr∆∞·ªõc**. ƒê√¢y ch√≠nh l√† kh·ªüi ƒë·∫ßu cho **c∆° ch·∫ø h·ªìi quy, t√°i ph√°t, l·∫∑p l·∫°i hay nh·ªõ l·∫°i (recurrence)** trong ki·∫øn tr√∫c c·ªßa c√°c m√¥ h√¨nh NLP sau n√†y. üîÑ‚ú®  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGjmONzfrpOA"
      },
      "source": [
        "![RNN image](https://research.aimultiple.com/wp-content/uploads/2021/08/rnn-text.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwTnA1Joryzi"
      },
      "source": [
        "Trong **ch∆∞∆°ng 7** n√†y, m·ªçi ng∆∞·ªùi s·∫Ω t√¨m hi·ªÉu v·ªÅ c√°c **ph∆∞∆°ng ph√°p ƒë·ªÉ hi·ªán th·ª±c h√≥a c∆° ch·∫ø h·ªìi quy** nha. üåü  \n",
        "\n",
        "T·ª•i m√¨nh s·∫Ω **ƒë√†o s√¢u h∆°n v·ªÅ th√¥ng tin ng·ªØ nghƒ©a c·ªßa c√°c chu·ªói**, c√°ch m√† ch√∫ng ƒë∆∞·ª£c h·ªçc c≈©ng nh∆∞ t√¨m hi·ªÉu v·ªÅ **m·ªôt lo·∫°i ki·∫øn tr√∫c m√¥ h√¨nh m·ªõi** c√≥ kh·∫£ nƒÉng hi·ªÉu t·ªët h∆°n c√°c vƒÉn b·∫£n.  \n",
        "\n",
        "V√† ƒë√≥ ch√≠nh l√† **M·∫°ng n∆°-ron h·ªìi quy - Recurrent Neural Network (RNN)**. üîÑüß†  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uSuCn9G0Mu-"
      },
      "source": [
        "# The Basic of Recurrence üåÄ  \n",
        "### Kh√°i ni·ªám c∆° b·∫£n v·ªÅ c∆° ch·∫ø h·ªìi quy, t√°i ph√°t, nh·ªõ l·∫°i - Recurrence üéØ  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhVlT-xs3K7N"
      },
      "source": [
        "Tr∆∞·ªõc khi t·ª•i m√¨nh ƒëi s√¢u v√†o hi·ªÉu c√°ch m√† **c∆° ch·∫ø h·ªìi quy (Recurrence)** ho·∫°t ƒë·ªông, t·ª•i m√¨nh s·∫Ω nh√¨n l·∫°i v·ªÅ nh·ªØng **gi·ªõi h·∫°n** c·ªßa c√°c m√¥ h√¨nh tr∆∞·ªõc ƒë√≥ m·ªôt l·∫ßn n·ªØa nha. üß†  \n",
        "\n",
        "V·ªÅ c∆° b·∫£n th√¨ m·ªçi th·ª© v·∫´n ho·∫°t ƒë·ªông theo ki·ªÉu: b·∫°n c√≥ **d·ªØ li·ªáu** n√†y, b·∫°n c√≥ **nh√£n** c·ªßa ch√∫ng n√†y, v√† r·ªìi ch√∫ng ta ƒë·∫©y t·∫•t c·∫£ v√†o m√¥ h√¨nh, √©p ch√∫ng h·ªçc v√† t√¨m ra ƒë∆∞·ª£c **c√°c quy lu·∫≠t b√™n trong**. üìä Sau ƒë√≥, ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng nh·ªØng quy lu·∫≠t c√≥ ƒë∆∞·ª£c, hay c√≤n ƒë∆∞·ª£c g·ªçi l√† tr·ªçng s·ªë c·ªßa m√¥ h√¨nh, ƒë·ªÉ d·ª± ƒëo√°n c√°c d·ªØ li·ªáu sau n√†y. üîÆ  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QORUlxs6bYIU"
      },
      "source": [
        "![c7_model](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/c7_model.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndy_g_Sh4w7D"
      },
      "source": [
        "Tuy nhi√™n, m·ªçi ng∆∞·ªùi c√≥ th·ªÉ th·∫•y r·∫±ng d·ªØ li·ªáu ƒë∆∞·ª£c ƒë∆∞a v√†o v√† x·ª≠ l√Ω m·ªôt c√°ch t·ªïng th·ªÉ, kh√¥ng c√≥ s·ª± c·ªë g·∫Øng t√¨m ki·∫øm c√°c **chi ti·∫øt li√™n quan ƒë·∫øn tr√¨nh t·ª± xu·∫•t hi·ªán c·ªßa d·ªØ li·ªáu**. ü§î ƒêi·ªÅu n√†y ph·∫ßn n√†o l√†m cho m√¥ h√¨nh tr·ªü n√™n **\"ngu\"** h∆°n trong vi·ªác n·∫Øm b·∫Øt ng·ªØ c·∫£nh v√† m·ªëi li√™n k·∫øt gi·ªØa c√°c ph·∫ßn t·ª≠ trong d·ªØ li·ªáu vƒÉn b·∫£n. üòÖ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPI29jNrb2Do"
      },
      "source": [
        "![c7_stupid](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/c7_stupid.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp7G0v5a-ooN"
      },
      "source": [
        "M√¨nh s·∫Ω l·∫•y v√≠ d·ª• m·ªôt s·ªë tr∆∞·ªùng h·ª£p c·ª• th·ªÉ v·ªÅ s·ª± **\"ngu\"** c·ªßa n√≥ nha. üòÖ  \n",
        "\n",
        "N·∫øu b·∫°n ƒë∆∞a 2 c√¢u nh∆∞ sau:  \n",
        "> Today I am **blue**, because the **sky** is gray. üå•Ô∏è  \n",
        "\n",
        "and  \n",
        "\n",
        "> Today I am happy, and there's a beautiful **blue sky**. üåû  \n",
        "\n",
        "ƒê·ªëi v·ªõi ch√∫ng ta s·∫Ω hi·ªÉu ngay r·∫±ng t·ª´ **\"blue\"** trong 2 c√¢u n√†y c√≥ √Ω nghƒ©a ho√†n to√†n kh√°c nhau:  \n",
        "- ·ªû c√¢u ƒë·∫ßu, **\"blue\"** c√≥ nghƒ©a l√† **\"bu·ªìn\"**. üòî  \n",
        "- ·ªû c√¢u sau, **\"blue\"** l·∫°i mang √Ω nghƒ©a l√† **\"m√†u xanh\"**. üíô  \n",
        "\n",
        "Tuy nhi√™n, v·ªõi m√¥ h√¨nh th√¨ kh√°c. ü§ñ Ch√∫ng ch·ªâ ƒë∆°n gi·∫£n hi·ªÉu r·∫±ng c·∫£ 2 c√¢u tr√™n ƒë·ªÅu ch·ª©a t·ª´ **\"blue\"** v√† **\"sky\"**, do ƒë√≥ ch√∫ng cho r·∫±ng 2 c√¢u n√†y t∆∞∆°ng t·ª± v√† c√≥ √Ω nghƒ©a g·∫ßn gi·ªëng nhau tr√™n kh√¥ng gian vector bi·ªÉu di·ªÖn. üòµ‚Äçüí´  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jpHXwLZAQ-l"
      },
      "source": [
        "> **V·∫≠y l√†m th·∫ø n√†o ƒë·ªÉ ch√∫ng c√≥ th·ªÉ kh√¥n ra ƒë∆∞·ª£c?** ü§î  \n",
        "\n",
        "ƒê√≥ l√† cho ch√∫ng hi·ªÉu v√† n·∫Øm b·∫Øt ƒë∆∞·ª£c **th·ª© t·ª± c√°c t·ª´ k·∫øt h·ª£p v·ªõi nhau**. M√† ƒë·ªÉ l√†m ƒë∆∞·ª£c vi·ªác n√†y, ch√∫ng ta c·∫ßn m√¥ h√¨nh **nh·ªõ l·∫°i ƒë∆∞·ª£c ki·∫øn th·ª©c tr∆∞·ªõc ƒë√≥**. üß† ƒê√¢y ch√≠nh l√† n·ªÅn t·∫£ng ch√≠nh cho vi·ªác x·ª≠ l√Ω c√°c d·ªØ li·ªáu c√≥ c·∫•u tr√∫c d·∫°ng **chu·ªói**.  \n",
        "\n",
        "Ch·∫Øc m·ªçi ng∆∞·ªùi ai c≈©ng g·∫∑p qua c·ª•m **\"h·ªìi quy ti·ªÅn ki·∫øp\"** r·ªìi ha. N√≥i ƒë∆°n gi·∫£n th√¨ h·ªìi quy c√≥ nghƒ©a l√† **\"nh·ªõ l·∫°i\"** nh∆∞ng m√† n√≥i cho n√≥ sang h∆°n =))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPdxzRXrgSwX"
      },
      "source": [
        "### B√¢y gi·ªù t·ª•i m√¨nh m·ªõi ch√≠nh th·ª©c ƒëi v√†o t√¨m hi·ªÉu **b·∫£n ch·∫•t** c·ªßa c∆° ch·∫ø **h·ªìi quy**, qua ƒë√≥ hi·ªÉu ƒë∆∞·ª£c c√°ch th·ª©c ho·∫°t ƒë·ªông c·ªßa m·∫°ng **RNN** nha. üîç‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgRS_fMhB9Xv"
      },
      "source": [
        "T·ª•i m√¨nh s·∫Ω l·∫•y v√≠ d·ª• ƒë∆°n gi·∫£n v·ªÅ **d√£y Fibonacci nha**. üåü ƒê√¢y l√† m·ªôt d·∫°ng c∆° b·∫£n c·ªßa chu·ªói tu·∫ßn t·ª± khi m√† **s·ªë ph√≠a sau b·∫±ng t·ªïng c·ªßa hai s·ªë tr∆∞·ªõc**, do ƒë√≥ ch√∫ng c√≥ **m·ªëi quan h·ªá m·∫≠t thi·∫øt** v·ªÅ m·∫∑t **tr√¨nh t·ª± theo th·ªùi gian**. üï∞Ô∏è‚ûï"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHz8PwZXCKS_"
      },
      "source": [
        "![fibonacci numbers](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/fibonacci_number.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMQGq-wMDA0V"
      },
      "source": [
        "![image.png](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/fibonacci_algorithms.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp1Ubf0aGbTr"
      },
      "source": [
        "√ù t∆∞·ªüng c·ªßa m·∫°ng h·ªìi quy - **RNN** c≈©ng s·∫Ω ho·∫°t ƒë·ªông t∆∞∆°ng t·ª± nh∆∞ v·∫≠y nha. üí°  \n",
        "\n",
        "·ªû ƒë√¢y m√¨nh ƒë·ªãnh nghƒ©a m·ªói **√¥ c·ªông m√†u cam** s·∫Ω ƒë·∫°i di·ªán cho **ph√©p t√≠nh to√°n t·∫°i 1 th·ªùi ƒëi·ªÉm hay m·ªói tr·∫°ng th√°i th·ªùi gian**. V·∫≠y l√† ta c√≥ t·ªïng c·ªông **3 tr·∫°ng th√°i th·ªùi gian**. ‚è≥‚ûï  \n",
        "\n",
        "***M·ªçi ng∆∞·ªùi nh·ªõ kƒ© gi√∫p m√¨nh ƒëo·∫°n n√†y nha!*** üß†  \n",
        "\n",
        "- ·ªû **tr·∫°ng th√°i th·ª© nh·∫•t**, thu·∫≠t to√°n s·∫Ω l·∫•y **d·ªØ li·ªáu kh·ªüi ƒë·∫ßu** l√† **1** v√† **2**, sau ƒë√≥ t√≠nh ra **output c·ªßa tr·∫°ng th√°i 1** l√† **3**.  \n",
        "- Ti·∫øp ƒë·∫øn, sang ph√©p t√≠nh ·ªü **tr·∫°ng th√°i th·ª© 2**, thu·∫≠t to√°n l·∫•y **d·ªØ li·ªáu ti·∫øp theo l√† s·ªë 2** v√† k·∫øt h·ª£p v·ªõi **output ·ªü tr·∫°ng th√°i 1 (l√† 3)**. K·∫øt qu·∫£ **output ·ªü tr·∫°ng th√°i 2** n√†y l√† **5**.  \n",
        "- T∆∞∆°ng t·ª± nh∆∞ v·∫≠y, ·ªü **tr·∫°ng th√°i th·ª© 3**, thu·∫≠t to√°n s·∫Ω ti·∫øp t·ª•c t√≠nh to√°n d·ª±a tr√™n d·ªØ li·ªáu v√† tr·∫°ng th√°i tr∆∞·ªõc ƒë√≥.  \n",
        "\n",
        "Qua qu√° tr√¨nh tr√™n, khi ƒë·∫øn tr·∫°ng th√°i cu·ªëi c√πng, **k·∫øt qu·∫£ ƒë·∫ßu ra c·ªßa ch√∫ng ta s·∫Ω c√≥ s·ª± ƒë√≥ng g√≥p √Ω nghƒ©a c·ªßa c√°c k·∫øt qu·∫£, tr·∫°ng th√°i tr∆∞·ªõc ƒë√≥** trong chu·ªói. T·ª´ ƒë√≥, m√¥ h√¨nh c√≥ th·ªÉ n·∫Øm b·∫Øt ƒë∆∞·ª£c **√Ω nghƒ©a c·ªßa to√†n b·ªô chu·ªói**. üåê  \n",
        "\n",
        "D∆∞·ªõi ƒë√¢y l√† **h√¨nh v·∫Ω thu nh·ªè** m√¥ t·∫£ kh√°i qu√°t **qu√° tr√¨nh ho·∫°t ƒë·ªông c·ªßa n∆°-ron h·ªìi quy**:  \n",
        "‚ú® *(B·∫°n c√≥ th·ªÉ h√¨nh dung c√°c √¥ n·ªëi ti·∫øp nhau, truy·ªÅn th√¥ng tin qua l·∫°i gi·ªØa c√°c tr·∫°ng th√°i)*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOYTbAiJNMdT"
      },
      "source": [
        "![architecture](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/architecture_rnn.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNl9ClPRIiPZ"
      },
      "source": [
        "**H√¨nh n√†y m√¥ t·∫£ t·ªïng qu√°t v·ªÅ c√°ch m√† m·ªôt n∆°-ron h·ªìi quy ho·∫°t ƒë·ªông.** üåÄ  \n",
        "\n",
        "- Ta g·ªçi **F** l√† **qu√° tr√¨nh t√≠nh to√°n c·ªßa m√¥ h√¨nh t·∫°i m·ªói th·ªùi ƒëi·ªÉm**.  \n",
        "- **t** ƒë·∫°i di·ªán cho **th·ªùi ƒëi·ªÉm x·∫£y ra ph√©p to√°n**.  \n",
        "- **x** l√† **d·ªØ li·ªáu ƒë·∫ßu v√†o** v√† **y** l√† **k·∫øt qu·∫£ ƒë·∫ßu ra**.  \n",
        "\n",
        "üåü **T·∫°i m·ªói th·ªùi ƒëi·ªÉm t**, m√¥ h√¨nh s·∫Ω:  \n",
        "1. Nh·∫≠n **d·ªØ li·ªáu ƒë·∫ßu v√†o** l√† **x(t)**.  \n",
        "2. Th·ª±c hi·ªán **ph√©p t√≠nh to√°n th√¥ng qua F** ƒë·ªÉ t√≠nh ra **k·∫øt qu·∫£ ƒë·∫ßu ra** l√† **y(t)**.  \n",
        "3. K·∫øt qu·∫£ **y(t)** n√†y s·∫Ω ƒë∆∞·ª£c **ƒë∆∞a v√†o t·ªïng h·ª£p** v·ªõi d·ªØ li·ªáu ·ªü **th·ªùi ƒëi·ªÉm ti·∫øp theo t+1**.  \n",
        "\n",
        "üí° **Quy tr√¨nh n√†y l·∫∑p l·∫°i li√™n t·ª•c**, d·ªØ li·ªáu trong qu√° kh·ª© s·∫Ω ƒë∆∞·ª£c t·ªïng h·ª£p d·∫ßn v√†o cho ƒë·∫øn khi h·∫øt chu·ªói, ƒë·∫£m b·∫£o m√¥ h√¨nh c√≥ kh·∫£ nƒÉng **ghi nh·ªõ ng·ªØ c·∫£nh v√† m·ªëi li√™n h·ªá gi·ªØa c√°c ph·∫ßn t·ª≠ trong chu·ªói**.  \n",
        "\n",
        "üéØ ƒê√¢y ch√≠nh l√† **n·ªÅn t·∫£ng quan tr·ªçng** c·ªßa c∆° ch·∫ø h·ªìi quy trong **RNN**, gi√∫p m√¥ h√¨nh hi·ªÉu ƒë∆∞·ª£c tr√¨nh t·ª± v√† m·ªëi li√™n h·ªá gi·ªØa c√°c t·ª´ trong b√†i to√°n ng√¥n ng·ªØ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK4K6L2KebEV"
      },
      "source": [
        "![detail_neural](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/detail_architecture_rnn.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHp1uPQdeGyn"
      },
      "source": [
        "üí° **Ph√¢n t√≠ch chi ti·∫øt ho·∫°t ƒë·ªông c·ªßa n∆°-ron h·ªìi quy qua t·ª´ng tr·∫°ng th√°i th·ªùi gian:**  \n",
        "\n",
        "1Ô∏è‚É£ **Th·ªùi ƒëi·ªÉm 0 (x‚ÇÄ, y‚ÇÄ):**  \n",
        "   - N∆°-ron th·ª±c hi·ªán ph√©p t√≠nh to√°n **F** v·ªõi d·ªØ li·ªáu ƒë·∫ßu v√†o **x‚ÇÄ**.  \n",
        "   - Tr·∫£ v·ªÅ k·∫øt qu·∫£ ƒë·∫ßu ra **y‚ÇÄ**.  \n",
        "   - **y‚ÇÄ** s·∫Ω ƒë∆∞·ª£c ƒë∆∞a v√†o t·ªïng h·ª£p c√πng v·ªõi d·ªØ li·ªáu t·∫°i th·ªùi ƒëi·ªÉm ti·∫øp theo.\n",
        "\n",
        "2Ô∏è‚É£ **Th·ªùi ƒëi·ªÉm 1 (x‚ÇÅ, y‚ÇÅ):**  \n",
        "   - N∆°-ron ti·∫øp t·ª•c th·ª±c hi·ªán ph√©p t√≠nh to√°n **F**, l·∫ßn n√†y v·ªõi ƒë·∫ßu v√†o l√† **x‚ÇÅ** v√† k·∫øt qu·∫£ t·ª´ th·ªùi ƒëi·ªÉm tr∆∞·ªõc ƒë√≥ **y‚ÇÄ**.  \n",
        "   - Tr·∫£ v·ªÅ k·∫øt qu·∫£ **y‚ÇÅ**, ƒë·∫°i di·ªán cho tr·∫°ng th√°i ·ªü th·ªùi ƒëi·ªÉm n√†y.  \n",
        "   - K·∫øt qu·∫£ **y‚ÇÅ** l·∫°i ti·∫øp t·ª•c ƒë∆∞·ª£c ƒë∆∞a v√†o t·ªïng h·ª£p cho th·ªùi ƒëi·ªÉm ti·∫øp theo.\n",
        "\n",
        "3Ô∏è‚É£ **Th·ªùi ƒëi·ªÉm 2 (x‚ÇÇ, y‚ÇÇ):**  \n",
        "   - T∆∞∆°ng t·ª±, **F** ƒë∆∞·ª£c t√≠nh v·ªõi ƒë·∫ßu v√†o **x‚ÇÇ** v√† **y‚ÇÅ** t·ª´ tr·∫°ng th√°i tr∆∞·ªõc.  \n",
        "   - Tr·∫£ v·ªÅ k·∫øt qu·∫£ **y‚ÇÇ**, ƒë·∫°i di·ªán cho tr·∫°ng th√°i t·∫°i th·ªùi ƒëi·ªÉm n√†y.  \n",
        "\n",
        "üåü **Qu√° tr√¨nh n√†y ti·∫øp t·ª•c l·∫∑p l·∫°i cho ƒë·∫øn khi h·∫øt chu·ªói d·ªØ li·ªáu**, ƒë·∫£m b·∫£o r·∫±ng **k·∫øt qu·∫£ ·ªü m·ªói th·ªùi ƒëi·ªÉm ƒë·ªÅu c√≥ s·ª± t·ªïng h·ª£p t·ª´ c√°c tr·∫°ng th√°i tr∆∞·ªõc ƒë√≥**.  \n",
        "\n",
        "üéØ **√ù nghƒ©a:**  \n",
        "- N∆°-ron h·ªìi quy kh√¥ng ch·ªâ x·ª≠ l√Ω d·ªØ li·ªáu hi·ªán t·∫°i m√† c√≤n **ghi nh·ªõ v√† t√≠ch h·ª£p th√¥ng tin t·ª´ c√°c tr·∫°ng th√°i tr∆∞·ªõc ƒë√≥**.  \n",
        "- ƒêi·ªÅu n√†y gi√∫p m√¥ h√¨nh hi·ªÉu ƒë∆∞·ª£c **b·ªëi c·∫£nh v√† tr√¨nh t·ª± d·ªØ li·ªáu**, l√†m tƒÉng kh·∫£ nƒÉng ph√¢n t√≠ch v√† d·ª± ƒëo√°n trong c√°c b√†i to√°n li√™n quan ƒë·∫øn chu·ªói. üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDKQZ32oKTPF"
      },
      "source": [
        "*L∆∞u √Ω: M√¨nh ch·ªâ d√πng d√£y s·ªë Fibonacci ƒë·ªÉ m·ªçi ng∆∞·ªùi c√≥ th·ªÉ d·ªÖ hi·ªÉu v√† m∆∞·ªùng t∆∞·ª£ng c√°ch m√† ki·∫øn tr√∫c m·∫°ng h·ªìi quy - RNN ho·∫°t ƒë·ªông. C√≤n v·ªÅ chi ti·∫øt th√¨ ph√©p to√°n m√† m√¥ h√¨nh th·ª±c hi·ªán b√™n trong s·∫Ω ph·ª©c t·∫°p h∆°n nhi·ªÅu.* üòä"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrjvRgqGK280"
      },
      "source": [
        "# Extending Recurrence for Language  \n",
        "### üåê T·ª•i m√¨nh s·∫Ω m·ªü r·ªông t√¨m hi·ªÉu v·ªÅ t√≠nh h·ªìi quy trong x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n nha. üí¨  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA7oi4bQpf2_"
      },
      "source": [
        "·ªû ph·∫ßn tr∆∞·ªõc, t·ª•i m√¨nh ƒë√£ t√¨m hi·ªÉu v·ªÅ c∆° ch·∫ø ho·∫°t ƒë·ªông c·ªßa **RNN**, c√°ch m√† ch√∫ng c√≥ th·ªÉ n·∫Øm b·∫Øt ƒë∆∞·ª£c ng·ªØ c·∫£nh xuy√™n su·ªët m·ªôt c√¢u hay chu·ªói r·ªìi ha m·ªçi ng∆∞·ªùi. üí° ƒêi·ªÅu n√†y th·∫≠t s·ª± l√† m·ªôt b∆∞·ªõc ƒë·ªôt ph√° v√† c·∫£i ti·∫øn ·ªü th·ªùi ƒëi·ªÉm ƒë√≥. C√°c ki·∫øn tr√∫c li√™n quan ƒë·∫øn **RNN** s·∫Ω ƒë∆∞·ª£c ·ª©ng d·ª•ng v√† s·ª≠ d·ª•ng nhi·ªÅu ·ªü c√°c ch∆∞∆°ng ti·∫øp theo. üìò\n",
        "\n",
        "Tuy nhi√™n, t·∫°i th·ªùi ƒëi·ªÉm ƒë√≥, ng∆∞·ªùi ta c≈©ng nh·∫≠n th·∫•y m·ªôt **nh∆∞·ª£c ƒëi·ªÉm ch√≠ t·ª≠** v·ªõi ki·∫øn tr√∫c **RNN c∆° b·∫£n**:  \n",
        "\n",
        "‚û°Ô∏è **D·ªØ li·ªáu v·ªÅ ng·ªØ c·∫£nh s·∫Ω b·ªã m·∫•t d·∫ßn theo th·ªùi gian.**\n",
        "\n",
        "> **V·∫≠y t·∫°i sao l·∫°i c√≥ ƒëi·ªÅu n√†y x·∫£y ra?** ü§î\n",
        "\n",
        "T·ª•i m√¨nh s·∫Ω ph√¢n t√≠ch l·∫°i c√°ch th·ª©c ho·∫°t ƒë·ªông c·ªßa **RNN** th√¥ng qua v√≠ d·ª• v·ªÅ d√£y **Fibonacci** nha.  \n",
        "Ch√∫ng ta bi·∫øt r·∫±ng k·∫øt qu·∫£ c·ªßa m·ªôt s·ªë ph√≠a sau s·∫Ω b·∫±ng **hai s·ªë tr∆∞·ªõc c·ªông l·∫°i**, do ƒë√≥ gi√° tr·ªã c·ªßa c√°c s·ªë ph√≠a tr∆∞·ªõc s·∫Ω c√≥ **t√°c ƒë·ªông v√† ·∫£nh h∆∞·ªüng** ƒë·∫øn s·ªë ph√≠a sau. üí¨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43uZVOGWwtlM"
      },
      "source": [
        "![image.png](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/fibonacci_algorithms.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ3gxvGpwt7P"
      },
      "source": [
        "**S·ªë 1 v√† s·ªë 2 s·∫Ω c√≥ t√°c ƒë·ªông m·∫°nh, ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp t·ªõi k·∫øt qu·∫£ ƒë·∫ßu ra ·ªü th·ªùi ƒëi·ªÉm ƒë·∫ßu ti√™n l√† 3**. T∆∞∆°ng t·ª± nh∆∞ v·∫≠y, **s·ªë 2 v√† s·ªë 3 s·∫Ω c√≥ t√°c ƒë·ªông m·∫°nh ƒë·∫øn k·∫øt qu·∫£ ƒë·∫ßu ra l√† 5 ·ªü th·ªùi ƒëi·ªÉm th·ª© 2**. Tuy nhi√™n, **t√°c ƒë·ªông c·ªßa s·ªë 1 v√† s·ªë 2 ban ƒë·∫ßu ƒë·∫øn s·ªë 5 ·ªü th·ªùi ƒëi·ªÉm 2 ƒë√£ y·∫øu h∆°n**, b·ªüi gi·ªù ƒë√¢y t·ªïng c·ªßa ch√∫ng tr√™n 5 **ch·ªâ c√≤n 3/5**. T∆∞∆°ng t·ª± nh∆∞ v·∫≠y, chu·ªói c√†ng d√†i v·ªÅ sau th√¨ t√°c ƒë·ªông c·ªßa c√°c s·ªë ·ªü ƒë·∫ßu s·∫Ω b·ªã y·∫øu ƒëi cho ƒë·∫øn khi b·∫°n kh√¥ng c√≤n nh·∫≠n th·∫•y ƒë∆∞·ª£c s·ª± ƒë√≥ng g√≥p, ·∫£nh h∆∞·ªüng c·ªßa n√≥ n·ªØa. V√≠ d·ª•, khi ƒë·∫øn **s·ªë Fibonacci th·ª© 8 l√† 89 th√¨ t√°c ƒë·ªông c·ªßa 1 v√† 2 ch·ªâ c√≤n l√† 3/89**. üîÑ\n",
        "\n",
        "Qua ƒë√≥, m·ªçi ng∆∞·ªùi th·∫•y r·∫±ng v·ªõi **chu·ªói c√†ng d√†i th√¨ kh·∫£ nƒÉng m·∫•t m√°t d·ªØ li·ªáu, ng·ªØ c·∫£nh c√†ng l·ªõn**, v√† m√¥ h√¨nh s·∫Ω d·∫ßn **qu√™n ƒëi c√°c d·ªØ li·ªáu tr∆∞·ªõc ƒë√≥**. üòî  \n",
        "\n",
        "---\n",
        "\n",
        "**V√≠ d·ª• c·ª• th·ªÉ trong c√°c c√¢u vƒÉn** ha. ·ªû ƒë√¢y, token **< ? >** ƒë·∫°i di·ªán cho t·ª´ kh√¥ng bi·∫øt tr∆∞·ªõc, n√≥ c√≥ th·ªÉ l√† b·∫•t k·ª≥ t·ª´ n√†o.  \n",
        "\n",
        "> **Today has a beautiful blue < ? >.**  \n",
        "\n",
        "Th√¨ ·ªü ƒë√¢y, t·ª´ **\"blue\"** s·∫Ω c√≥ **t√°c ƒë·ªông, ·∫£nh h∆∞·ªüng l·ªõn nh·∫•t ƒë·∫øn t·ª´ ti·∫øp theo**, do ƒë√≥ m√¥ h√¨nh c√≥ th·ªÉ ƒëo√°n t·ª´ ti·∫øp theo l√† **\"sky\"**. üå§Ô∏è  \n",
        "\n",
        "Tuy nhi√™n, v·ªõi nh·ªØng c√¢u d√†i h∆°n nh∆∞:  \n",
        "\n",
        "> **I lived in Ireland, so in high school I had to learn how to speak < ? >.**  \n",
        "\n",
        "·ªû ƒë√¢y, m·ªçi ng∆∞·ªùi c√≥ th·ªÉ ƒëo√°n t·ª´ ·∫©n ƒë√≥ l√† **m·ªôt ng√¥n ng·ªØ** g√¨ ƒë√≥, v√† th√¥ng th∆∞·ªùng ch√∫ng ta s·∫Ω x√°c ƒë·ªãnh ch√≠nh th√¥ng qua t·ª´ **\"speak\", \"high school\" v√† \"Ireland\"**, ƒë√∫ng kh√¥ng? üó£Ô∏è Tuy nhi√™n, **hai t·ª´ \"high school\" v√† \"Ireland\" l·∫°i ·ªü qu√° xa so v·ªõi t·ª´ ·∫©n**, khi·∫øn m·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng c·ªßa ch√∫ng ƒë·∫øn t·ª´ ·∫©n **kh√¥ng nhi·ªÅu**. Do ƒë√≥, m√¥ h√¨nh kh√¥ng nh·∫≠n ƒë·ªãnh v√† d·ª± ƒëo√°n ƒë∆∞·ª£c t·ª´ ti·∫øp theo chu·∫©n x√°c, k·∫øt qu·∫£ c√≥ th·ªÉ s·∫Ω sai nhi·ªÅu h∆°n.  \n",
        "\n",
        "V√≠ d·ª•, t·ª´ ·∫©n ƒë√∫ng ra ph·∫£i l√† **\"Gaelic\"**, tuy nhi√™n l·∫°i b·ªã d·ª± ƒëo√°n th√†nh **\"Ireland\"** ho·∫∑c m·ªôt t·ª´ b·∫•t k·ª≥ kh√°c. üö©"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cImWsR-1zQVJ"
      },
      "source": [
        "### V·∫≠y v·∫•n ƒë·ªÅ b√¢y gi·ªù c·ªßa ch√∫ng l√† l√†m c√°ch n√†o kh·∫Øc ph·ª•c v·∫•n ƒë·ªÅ m·∫•t m√°t ng·ªØ c·∫£nh tr√™n? ü§î\n",
        "\n",
        "ƒê·ªÉ l√†m ƒë∆∞·ª£c vi·ªác n√†y, ch√∫ng ta c·∫ßn ph·∫£i **gia tƒÉng kh·∫£ nƒÉng nh·ªõ, b·ªô nh·ªõ ng·∫Øn h·∫°n c·ªßa RNN** ƒë·ªÉ c√≥ th·ªÉ nh·ªõ ƒë∆∞·ª£c l√¢u h∆°n v·ªõi c√°c chu·ªói d√†i h∆°n. V√† ƒë√≥ c≈©ng l√† √Ω t∆∞·ªüng kh·ªüi ƒë·∫ßu cho s·ª± ra ƒë·ªùi c·ªßa m·ªôt ki·∫øn tr√∫c m·ªõi, n√¢ng c·∫•p h∆°n mang t√™n b·ªô nh·ªõ ng·∫Øn h·∫°n d√†i - **Long Short-term Memory (LSTM)**. üß†üí°\n",
        "\n",
        "---\n",
        "\n",
        "*P/s: Thi·ªát s·ª± l√† m·ªçi ng∆∞·ªùi n√™n ƒë·ªçc t√™n em n√≥ b·∫±ng ti·∫øng Anh nha, ƒë·ªçc sang ti·∫øng Vi·ªát th·∫•y n√≥ k√¨ k√¨ qu√° =)) , m√¨nh c≈©ng kh√¥ng bi·∫øt d·ªãch sao cho h·ª£p l√Ω. üôÉ*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTrqo8T84Kds"
      },
      "source": [
        "### Ti·∫øp theo, t·ª•i m√¨nh s·∫Ω t√¨m hi·ªÉu v·ªÅ **LSTM**. üöÄ\n",
        "\n",
        "Nh∆∞ng m√† theo ti√™u ch√≠ c·ªßa t·ª•i m√¨nh l√† **kh√¥ng ƒëi s√¢u qu√° v·ªÅ thu·∫≠t to√°n**, n√™n m√¨nh s·∫Ω t√¨m hi·ªÉu c√°ch ch√∫ng ho·∫°t ƒë·ªông ·ªü **c·∫•p ƒë·ªô ·ª©ng d·ª•ng** th√¥i nha. üõ†Ô∏è\n",
        "\n",
        "N·∫øu c√°c b·∫°n v·∫´n mu·ªën t√¨m hi·ªÉu s√¢u h∆°n v·ªÅ **code v√† l√Ω thuy·∫øt chi ti·∫øt**, c√≥ th·ªÉ tham kh·∫£o th·ª≠ [blog n√†y](https://nttuan8.com/bai-14-long-short-term-memory-lstm/). üìö"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0WQVJx65FA0"
      },
      "source": [
        "![lstm](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/lstm.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULw_zmlR5JIC"
      },
      "source": [
        "M·ªçi ng∆∞·ªùi hi·ªÉu ƒë∆°n gi·∫£n l√† **LSTM** s·∫Ω n√¢ng c·∫•p **RNN c∆° b·∫£n** l√™n b·∫±ng c√°ch th√™m v√†o m·ªôt c√°i g·ªçi l√† **√¥ tr·∫°ng th√°i (Cell state)**. üåü  \n",
        "\n",
        "ƒêi·ªÅu n√†y cho ph√©p ng·ªØ c·∫£nh ƒë∆∞·ª£c **duy tr√¨** kh√¥ng ch·ªâ qua t·ª´ng b∆∞·ªõc m√† c√≤n xuy√™n su·ªët c·∫£ **to√†n b·ªô chu·ªói c√°c b∆∞·ªõc**. üõ†Ô∏è  \n",
        "\n",
        "*üí° L∆∞u √Ω:* Ch√∫ng l√† **n∆°-ron**, do ƒë√≥ s·∫Ω h·ªçc theo c√°ch m√† n∆°-ron ho·∫°t ƒë·ªông, gi√∫p ƒë·∫£m b·∫£o c√°c **ng·ªØ c·∫£nh quan tr·ªçng** s·∫Ω ƒë∆∞·ª£c h·ªçc d·∫ßn theo th·ªùi gian. üß†"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRouFFvE-ZWT"
      },
      "source": [
        "M·ªôt ph·∫ßn **quan tr·ªçng** v√† **c·∫£i ti·∫øn** v∆∞·ª£t b·∫≠c c·ªßa **LSTM** ch√≠nh l√† kh·∫£ nƒÉng **Bidirectional**. üöÄ  \n",
        "\n",
        "ƒêi·ªÅu n√†y cho ph√©p m√¥ h√¨nh **h·ªçc theo c·∫£ 2 chi·ªÅu** c·ªßa chu·ªói, t·ª©c l√†:  \n",
        "- **T·ª´ tr∆∞·ªõc v·ªÅ sau (forward)**. ‚è©  \n",
        "- **T·ª´ sau ra tr∆∞·ªõc (backward)**. ‚è™  \n",
        "\n",
        "M·ªçi ng∆∞·ªùi th∆∞·ªùng g·ªçi n√≥ l√† **BLSTM (Bidirectional LSTM)** nha. üåü  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5srMz4-g-U8K"
      },
      "source": [
        "![bblstm](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/blstm.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dxX2BFN6I_E"
      },
      "source": [
        "·ªû c√°ch n√†y, k·∫øt qu·∫£ ƒë√°nh gi√° t·∫°i m·ªói b∆∞·ªõc th·ªùi gian s·∫Ω ƒë∆∞·ª£c th·ª±c hi·ªán **theo c·∫£ hai h∆∞·ªõng**:  \n",
        "\n",
        "1. **Forward**: T·ª´ **step 0 ƒë·∫øn step n** ‚è©.  \n",
        "2. **Backward**: T·ª´ **step n ƒë·∫øn step 0** ‚è™.  \n",
        "\n",
        "T·∫°i m·ªói b∆∞·ªõc th·ªùi gian, k·∫øt qu·∫£ c·ªßa **y** l√† **s·ª± t·ªïng h·ª£p** c·ªßa c·∫£ hai qu√° tr√¨nh tr√™n. ƒêi·ªÅu n√†y gi√∫p m√¥ h√¨nh **hi·ªÉu s√¢u h∆°n** v√† **n·∫Øm b·∫Øt ng·ªØ c·∫£nh t·ªët h∆°n** t·ª´ chu·ªói d·ªØ li·ªáu.\n",
        "\n",
        "T·ª•i m√¨nh c√πng **ph√¢n t√≠ch k·ªπ h∆°n** v·ªõi bi·ªÉu ƒë·ªì d∆∞·ªõi ƒë√¢y nha! üìä  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA7nxAAc_Fwx"
      },
      "source": [
        "![detail_blstm](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/detail_blstm.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6Oi3UZp_Ind"
      },
      "source": [
        "·ªû ƒë√¢y, ch√∫ng ta c√≥:  \n",
        "\n",
        "- **3 n∆°-ron** t∆∞∆°ng ·ª©ng v·ªõi **3 tr·∫°ng th√°i th·ªùi gian**: **F1, F2, F3**.  \n",
        "- **M≈©i t√™n** ƒë·∫°i di·ªán cho h∆∞·ªõng ƒëi **forward** ‚è© ho·∫∑c **backward** ‚è™.  \n",
        "\n",
        "### ƒêi·ªÉm n·ªïi b·∫≠t:  \n",
        "- **BLSTM** t·ªïng h·ª£p c·∫£ hai h∆∞·ªõng (**forward** v√† **backward**) t·∫°i m·ªói b∆∞·ªõc th·ªùi gian ƒë·ªÉ t·∫°o ra k·∫øt qu·∫£ **y**.  \n",
        "- **√î tr·∫°ng th√°i hai chi·ªÅu (Cell state)** c·ªßa BLSTM gi√∫p l∆∞u gi·ªØ v√† qu·∫£n l√Ω n·ªôi dung chu·ªói ho·∫∑c c√¢u vƒÉn m·ªôt c√°ch hi·ªáu qu·∫£ h∆°n.  \n",
        "\n",
        "---\n",
        "\n",
        "#### T·ª•i m√¨nh s·∫Ω ƒëi v√†o v√≠ d·ª• minh h·ªça:  \n",
        "\n",
        "C√¢u vƒÉn:  \n",
        "> **I lived in Ireland, so in high school I had to learn how to speak < ? >.**  \n",
        "\n",
        "- Nh·ªù vi·ªác h·ªçc theo **c·∫£ hai chi·ªÅu**, m√¥ h√¨nh c·ªßa ch√∫ng ta c√≥ th·ªÉ nh·∫≠n bi·∫øt ƒë∆∞·ª£c **·∫£nh h∆∞·ªüng c·ªßa c√°c t·ª´ quan tr·ªçng** nh∆∞ **\"Ireland\"** v√† **\"high school\"** t·ªët h∆°n, t·ª´ ƒë√≥ ƒëo√°n ch√≠nh x√°c t·ª´ ·∫©n l√† **\"Gaelic\"**.  \n",
        "- N·∫øu ƒë·∫£o ng∆∞·ª£c l√†, **t·ª´ ·∫©n l√† \"Ireland\"**, m√¥ h√¨nh v·∫´n **d·ª± ƒëo√°n ch√≠nh x√°c** ƒë∆∞·ª£c nh·ªù c∆° ch·∫ø **backward**.  \n",
        "\n",
        "---\n",
        "\n",
        "#### V·∫≠y l·ª£i √≠ch c·ªßa BLSTM l√†:  \n",
        "- **Hi·ªÉu ng·ªØ c·∫£nh s√¢u h∆°n** trong vƒÉn b·∫£n.  \n",
        "- **Kh·∫£ nƒÉng d·ª± ƒëo√°n t·ª´ ·∫©n ch√≠nh x√°c h∆°n** nh·ªù t·ªïng h·ª£p th√¥ng tin hai chi·ªÅu.  \n",
        "\n",
        "---\n",
        "\n",
        "### *L∆∞u √Ω:*\n",
        "- **T√†i nguy√™n t√≠nh to√°n c≈©ng s·∫Ω tƒÉng g·∫•p ƒë√¥i** do ph·∫£i th·ª±c hi·ªán c·∫£ **forward** v√† **backward** tr√™n chu·ªói d·ªØ li·ªáu.  \n",
        "- Do ƒë√≥ m√¨nh ƒë·ªÅ xu·∫•t m·ªçi ng∆∞·ªùi n√™n s·ª≠ d·ª•ng **GPU** nha. üíª‚ö°  \n",
        "\n",
        "Trong c√°c ch∆∞∆°ng ti·∫øp theo, t·ª•i m√¨nh s·∫Ω ·ª©ng d·ª•ng **BLSTM** r·∫•t nhi·ªÅu nha. üöÄ\n",
        "\n",
        "B√¢y gi·ªù t·ª•i m√¨nh s·∫Ω ti·∫øn h√†nh th·ª≠ x√¢y d·ª±ng m·ªôt v√†i m√¥ h√¨nh ph√¢n lo·∫°i vƒÉn b·∫£n v·ªõi **BLSTM**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLP75bnQDM6b"
      },
      "source": [
        "# Creating a Text Classifier with RNNs  \n",
        "### T·ª•i m√¨nh s·∫Ω x√¢y d·ª±ng m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i vƒÉn b·∫£n v·ªõi RNN nha. üåü  \n",
        "\n",
        "·ªû ƒë√¢y t·ª•i m√¨nh d√πng l·∫°i lu√¥n b√†i to√°n v·ªõi b·ªô d·ªØ li·ªáu **Sarcam** tr∆∞·ªõc ƒë√≥ nha. C√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu th√¨ v·∫´n nh∆∞ nhau. üòä  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKrEbryTvMH0"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.15 -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Keras-Preprocessing"
      ],
      "metadata": {
        "id": "4XxCbLmlzyeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import bs4 as BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import numpy as np\n",
        "import string\n",
        "import kagglehub\n",
        "import pandas"
      ],
      "metadata": {
        "id": "2O5CWCb12Xvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGpWIBrerya_"
      },
      "outputs": [],
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"rmisra/news-headlines-dataset-for-sarcasm-detection\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCm5rOyPFgEv"
      },
      "outputs": [],
      "source": [
        "# Ki·ªÉm tra c√°c th∆∞ m·ª•c hi·ªán c√≥\n",
        "import os\n",
        "for dir_name in os.listdir(path):\n",
        "  print(dir_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0uNnTl4uwkB"
      },
      "outputs": [],
      "source": [
        "dataset = pandas.read_json(path + \"/Sarcasm_Headlines_Dataset.json\", lines=True)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kAJGPX9u3HY"
      },
      "outputs": [],
      "source": [
        "sentences = dataset[\"headline\"].values\n",
        "labels = dataset[\"is_sarcastic\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wyil7CYew5gQ"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "table = str.maketrans('', '', string.punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBF1L_k6Fxrd"
      },
      "outputs": [],
      "source": [
        "# Ti·∫øn h√†nh l√†m s·∫°ch vƒÉn b·∫£n\n",
        "sentences_preprocessed = []\n",
        "for s in sentences:\n",
        "  sentence = s.lower()\n",
        "  # Ti·∫øn h√†nh x·ª≠ l√Ω c√°c k√≠ t·ª± ƒë·∫∑c bi·ªát, ph√≤ng tr∆∞·ªùng h·ª£p ng∆∞·ªùi d√πng vi·∫øt li·ªÅn ch√∫ng v·ªõi c√°c t·ª´.\n",
        "  sentence = sentence.replace(\",\", \" , \")\n",
        "  sentence = sentence.replace(\".\", \" . \")\n",
        "  sentence = sentence.replace(\"/\", \" / \")\n",
        "  sentence = sentence.replace(\"?\", \" ? \")\n",
        "  sentence = sentence.replace(\"!\", \" ! \")\n",
        "  sentence = sentence.replace(\"-\", \" - \")\n",
        "  sentence = sentence.replace(\"(\", \" ( \")\n",
        "  sentence = sentence.replace(\")\", \" ) \")\n",
        "  sentence = sentence.replace(\"$\", \" $ \")\n",
        "  sentence = sentence.replace(\"%\", \" % \")\n",
        "  sentence = sentence.replace(\"&\", \" & \")\n",
        "\n",
        "  # Lo·∫°i b·ªè c√°c th·∫ª HTML b·∫±ng BeautifulSoup\n",
        "  soup = BeautifulSoup.BeautifulSoup(sentence)\n",
        "  sentence = soup.get_text()\n",
        "\n",
        "  # Lo·∫°i b·ªè c√°c k√≠ t·ª± ƒë·∫∑c bi·ªát\n",
        "  sentence = sentence.translate(table)\n",
        "\n",
        "  # Chu·∫©n h√≥a l·∫°i c√°c k√≠ t·ª± kho·∫£ng tr·∫Øng d∆∞ th·ª´a li√™n ti·∫øp th√†nh 1 kho·∫£ng tr·∫Øng\n",
        "  sentence = re.sub('\\s+', ' ', sentence)\n",
        "\n",
        "  # Ti·∫øn h√†nh l·ªçc v√† lo·∫°i b·ªè t·ª´ d·ª´ng\n",
        "  words = sentence.split()\n",
        "  filtered_sentence = []\n",
        "\n",
        "  for w in words:\n",
        "    if w not in stop_words:\n",
        "      filtered_sentence.append(w)\n",
        "\n",
        "  filtered_sentence = \" \".join(filtered_sentence)\n",
        "  # Th√™m v√†o danh s√°ch headline ƒë√£ ti·ªÅn x·ª≠ l√Ω\n",
        "  sentences_preprocessed.append(filtered_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jo1m60P3xTRt"
      },
      "outputs": [],
      "source": [
        "# Chia t·∫≠p train test, ·ªü ƒë√¢u m√¨nh chia theo t√°c gi·∫£ gi·ªëng b√†i tr∆∞·ªõc lu√¥n ƒëi.\n",
        "\n",
        "training_size = 24000\n",
        "train_sequences = sentences_preprocessed[0:training_size]\n",
        "train_labels = labels[0:training_size]\n",
        "test_sequences = sentences_preprocessed[training_size:]\n",
        "test_labels = labels[training_size:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch30upznz0cL"
      },
      "source": [
        "### Ti·∫øn h√†nh m√£ h√≥a d·ªØ li·ªáu, l∆∞u √Ω ·ªü ph·∫ßn n√†y s·∫Ω kh√°c v·ªõi tr∆∞·ªõc ƒë√≥ nha. üõ†Ô∏è  \n",
        "\n",
        "B√¢y gi·ªù ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng b·ªô m√£ h√≥a v·ªõi k√≠ch th∆∞·ªõc t·ª´ v·ª±ng l√† **20.000**, l·ªõn h∆°n r·∫•t nhi·ªÅu so v·ªõi tr∆∞·ªõc, v√† s·ªë chi·ªÅu c·ªßa c√°c vector bi·ªÉu di·ªÖn c≈©ng ƒë∆∞·ª£c tƒÉng l√™n **64**. üìä  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTyDiqt0yaG-"
      },
      "outputs": [],
      "source": [
        "# T·∫°o tokenizer\n",
        "vocab_size = 20000\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlUf_baTzGSp"
      },
      "outputs": [],
      "source": [
        "train_sequences_encoded = tokenizer.texts_to_sequences(train_sequences)\n",
        "test_sequences_encoded = tokenizer.texts_to_sequences(test_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCTk-6_yzWJw"
      },
      "outputs": [],
      "source": [
        "# Ti·∫øn h√†nh padding cho chu·ªói\n",
        "max_len = 100\n",
        "train_padded = pad_sequences(train_sequences_encoded, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
        "test_padded = pad_sequences(test_sequences_encoded, maxlen=max_len, padding=\"post\", truncating=\"post\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOV4u4JP0tew"
      },
      "source": [
        "# Simple Model use LSTM  \n",
        "\n",
        "### B√¢y gi·ªù t·ª•i m√¨nh s·∫Ω ti·∫øn h√†nh x√¢y d·ª±ng m·ªôt m√¥ h√¨nh ƒë∆°n gi·∫£n v·ªõi LSTM tr∆∞·ªõc nha. ‚ú®  \n",
        "\n",
        "·ªû ƒë√¢y s·∫Ω c√≥ m·ªôt ƒëi·ªÉm ƒë·∫∑c bi·ªát kh√°c v·ªõi tr∆∞·ªõc. N·∫øu nh∆∞ ·ªü ch∆∞∆°ng tr∆∞·ªõc, d·ªØ li·ªáu c·ªßa m·ªçi ng∆∞·ªùi sau khi qua l·ªõp **Embedding** c·∫ßn ph·∫£i ƒë∆∞·ª£c t·ªïng h·ª£p b·∫±ng l·ªõp **Global Average Pooling** ƒë·ªÉ chuy·ªÉn th√†nh vector tr∆∞·ªõc khi ƒë∆∞a v√†o l·ªõp **Dense**, nh∆∞ng ·ªü ƒë√¢y ch√∫ng ta kh√¥ng c·∫ßn v·∫≠y n·ªØa.  \n",
        "\n",
        "Khi l√†m vi·ªác v·ªõi c√°c l·ªõp **RNN** nh∆∞ **LSTM**, m·ªçi ng∆∞·ªùi kh√¥ng c·∫ßn ph·∫£i t·ªïng h·ª£p vector m√† c√≥ th·ªÉ ƒë∆∞a tr·ª±c ti·∫øp ƒë·∫ßu ra c·ªßa l·ªõp **Embedding** v√†o l·ªõp **RNN** nh∆∞ **LSTM** lu√¥n. üîÑ  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import timedelta"
      ],
      "metadata": {
        "id": "wXlhQV3h-pGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dims = 64\n",
        "epochs = 30"
      ],
      "metadata": {
        "id": "wix2gc4b-9eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiiTKUbE13gA"
      },
      "outputs": [],
      "source": [
        "# X√¢y d·ª•ng m√¥ h√¨nh\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dims),\n",
        "    Bidirectional(LSTM(embedding_dims)),\n",
        "    Dense(24, activation=\"relu\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6JYdJXM3R5D"
      },
      "source": [
        "·ªû ƒë√¢y t√°c gi·∫£ c√≥ ch·ªânh **learning_rate** xu·ªëng r·∫•t nh·ªè, ch·ªâ **0.00001**. üîß ƒêi·ªÅu n√†y khi·∫øn vi·ªác h·ªôi t·ª• s·∫Ω di·ªÖn ra **r·∫•t l√¢u**, c≈©ng nh∆∞ **s·ªë l∆∞·ª£ng ph√©p t√≠nh to√°n c√≥ th·ªÉ tƒÉng th√™m**. V√¨ v·∫≠y, khuy·∫øn kh√≠ch m·ªçi ng∆∞·ªùi s·ª≠ d·ª•ng **GPU** ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô hu·∫•n luy·ªán nha. üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiuV-NRuxiAl"
      },
      "outputs": [],
      "source": [
        "adam = Adam(learning_rate=0.00001, beta_1=0.9, beta_2=0.999)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-DDfXsG29--"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rudg8CNeqeYh"
      },
      "source": [
        "T·ª•i m√¨nh s·∫Ω ti·∫øn h√†nh hu·∫•n luy·ªán trong kho·∫£ng **30 epochs** th√¥i nha ‚è≥ v√¨ th·ªùi gian x·ª≠ l√Ω c·ªßa nh·ªØng m√¥ h√¨nh n√†y kh√° l√¢u. N·∫øu t√¨nh tr·∫°ng **overfiting** di·ªÖn ra ngay t·ª´ m·ªôt s·ªë epochs ƒë·∫ßu, th√¨ nh·ªØng epochs sau ƒë·ªÅu tr·ªü n√™n **v√¥ nghƒ©a**. Khi ƒë√≥, vi·ªác ƒë·ªÉ s·ªë l∆∞·ª£ng epochs l·ªõn ch·ªâ l√†m **t·ªën th√™m th·ªùi gian v√† t√†i nguy√™n** th√¥i. üö¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcoimprX21bV"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "history = model.fit(train_padded, train_labels, epochs=epochs, validation_data=(test_padded, test_labels))\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8DBqDhk35qj"
      },
      "outputs": [],
      "source": [
        "print(f\"Th·ªùi gian hu·∫•n luy·ªán: {str(timedelta(seconds=end_time - start_time))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rppzbDyS8aCl"
      },
      "outputs": [],
      "source": [
        "# V·∫Ω bi·ªÉu ƒë·ªì ƒë√°nh gi√° qu√° tr√¨nh\n",
        "train_acc = history.history[\"accuracy\"]\n",
        "val_acc = history.history[\"val_accuracy\"]\n",
        "train_loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
        "axs = axs.flatten()\n",
        "axs[0].plot(train_acc, label=\"train\")\n",
        "axs[0].plot(val_acc, label=\"val\")\n",
        "axs[0].set_title(\"Accuracy\")\n",
        "axs[0].legend()\n",
        "axs[1].plot(train_loss, label=\"train\")\n",
        "axs[1].plot(val_loss, label=\"val\")\n",
        "axs[1].set_title(\"Loss\")\n",
        "axs[1].legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgWENRS2KOj7"
      },
      "outputs": [],
      "source": [
        "# ƒê·ªô hi·ªáu qu·∫£ c·ªßa m√¥ h√¨nh\n",
        "eval = model.evaluate(test_padded, test_labels)\n",
        "print(f\"Accuracy: \", eval[1])\n",
        "print(f\"Loss: \", eval[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9WpPlod9gn2"
      },
      "source": [
        "ƒê√¢y l√† k·∫øt qu·∫£ ·ªü l·∫ßn ch·∫°y c·ªßa m√¨nh nha."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![model](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/model.png?raw=true)"
      ],
      "metadata": {
        "id": "aHE89EpGHEFW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6LuQl499qpZ"
      },
      "source": [
        "M·ªçi ng∆∞·ªùi c√≥ th·ªÉ th·∫•y m√¥ h√¨nh ch·ª©ng minh ƒë∆∞·ª£c ƒë·ªô hi·ªáu qu·∫£ c·ªßa m√¨nh ·ªü **30 epochs** ƒë·∫ßu th√¥ng qua k·∫øt qu·∫£ ki·ªÉm tra tr√™n t·∫≠p **val**. üìâ ƒê∆∞·ªùng **loss** gi·∫£m xu·ªëng d·∫ßn v√† c√≥ ph·∫ßn **th·∫•p h∆°n** t√≠ so v·ªõi bi·ªÉu ƒë·ªì ·ªü **ch∆∞∆°ng 6**, m·∫∑c d√π k√≠ch th∆∞·ªõc t·ª´ v·ª±ng c·ªßa ch√∫ng ta l√™n ƒë·∫øn t·∫≠n **20.000** v√† s·ªë chi·ªÅu **embedding** l√† **64**. üß©\n",
        "\n",
        "Tuy nhi√™n, m·ªçi ng∆∞·ªùi v·∫´n c√≥ th·ªÉ th·∫•y ƒë∆∞·ª£c t√¨nh tr·∫°ng **Overfit** ƒëang c√≥ xu h∆∞·ªõng x·∫£y ra. üö® ƒê∆∞·ªùng **loss val** c√≥ d·∫•u hi·ªáu **ƒëi ngang**, kh√¥ng gi·∫£m n·ªØa ·ªü kho·∫£ng **epoch 15 ƒë·∫øn 25**, v√† sau ƒë√≥ c√≥ d·∫•u hi·ªáu **tƒÉng d·∫ßn** l√™n. M√¨nh c√≥ th·ª≠ hu·∫•n luy·ªán ·ªü **100 epochs** v√† th·∫≠t s·ª± c√†ng v·ªÅ sau t√¨nh tr·∫°ng **Overfit** hi·ªán ra r√µ r·ªát.\n",
        "\n",
        "> **V·∫≠y ƒëi·ªÅu n√†y do ƒë√¢u m√† ra?** ü§î\n",
        "\n",
        "C√≥ th·ªÉ m√¥ h√¨nh c·ªßa ch√∫ng ta ƒë√£ b·∫Øt ƒë·∫ßu ƒë·∫°t ƒë·∫øn **gi·ªõi h·∫°n**, ƒëi·ªÉm h·ªôi t·ª• c·ªßa n√≥ v·ªõi h∆∞·ªõng ti·∫øp c·∫≠n v√† ki·∫øn tr√∫c hi·ªán t·∫°i. N·∫øu c√†ng h·ªçc th√™m, m√¥ h√¨nh s·∫Ω b·ªã ch√∫ tr·ªçng **qu√° v√†o d·ªØ li·ªáu train** v√† l√†m m·∫•t ƒëi t√≠nh **kh√°i qu√°t** c·ªßa ch√∫ng. Khi ƒë√≥, c√†ng h·ªçc l√¢u ·ªü c√°c epochs v·ªÅ sau, m·∫∑c d√π m√¥ h√¨nh s·∫Ω tƒÉng ƒë·ªô hi·ªáu qu·∫£ tr√™n t·∫≠p **train**, nh∆∞ng **loss** c·ªßa ch√∫ng tr√™n **val** s·∫Ω ng√†y c√†ng **tƒÉng d·∫ßn**, d·∫´n ƒë·∫øn t√¨nh tr·∫°ng **Overfit** di·ªÖn ra n·∫∑ng h∆°n. Khi ƒë√≥, m√¥ h√¨nh s·∫Ω kh√¥ng c√≤n hi·ªáu qu·∫£ v√† kh√¥ng th·ªÉ √°p d·ª•ng th·ª±c t·∫ø n·ªØa. üö´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbFsm-_i_nA-"
      },
      "source": [
        "# Stacking LSTMs  \n",
        "### T·ª•i m√¨nh s·∫Ω ti·∫øn h√†nh x√¢y d·ª±ng m√¥ h√¨nh v·ªõi **nhi·ªÅu l·ªõp LSTM h∆°n** nha. üåü  \n",
        "\n",
        "Vi·ªác s·ª≠ d·ª•ng **nhi·ªÅu l·ªõp LSTM x·∫øp ch·ªìng** l√™n nhau (stacking) s·∫Ω gi√∫p m√¥ h√¨nh c√≥ kh·∫£ nƒÉng **h·ªçc c√°c ƒë·∫∑c tr∆∞ng ph·ª©c t·∫°p h∆°n**, t·∫≠n d·ª•ng ƒë∆∞·ª£c s·ª©c m·∫°nh c·ªßa nhi·ªÅu t·∫ßng bi·ªÉu di·ªÖn. V·ªõi c√°ch n√†y, ch√∫ng ta c√≥ th·ªÉ c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh, ƒë·∫∑c bi·ªát v·ªõi nh·ªØng b√†i to√°n c√≥ t√≠nh **ng·ªØ nghƒ©a cao**.  \n",
        "\n",
        "*L∆∞u √Ω: M·∫∑c d√π stacking c√°c l·ªõp LSTM s·∫Ω tƒÉng kh·∫£ nƒÉng bi·ªÉu di·ªÖn c·ªßa m√¥ h√¨nh, nh∆∞ng c≈©ng s·∫Ω **tƒÉng th·ªùi gian hu·∫•n luy·ªán v√† t√†i nguy√™n s·ª≠ d·ª•ng**, n√™n m·ªçi ng∆∞·ªùi c·∫ßn c√¢n nh·∫Øc k·ªπ v·ªÅ s·ªë l∆∞·ª£ng l·ªõp sao cho h·ª£p l√Ω nha!* üöÄ  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IbX4zWaf_xFk"
      },
      "outputs": [],
      "source": [
        "model1 = Sequential([\n",
        "    Embedding(vocab_size, embedding_dims),\n",
        "    Bidirectional(LSTM(embedding_dims, return_sequences=True)),\n",
        "    Bidirectional(LSTM(embedding_dims)),\n",
        "    Dense(24, activation=\"relu\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "adam = Adam(learning_rate=0.00001, beta_1=0.9, beta_2=0.999)\n",
        "model1.compile(loss=\"binary_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk__FrOVBlog"
      },
      "source": [
        "·ªû **l·ªõp LSTM cu·ªëi**, m·ªçi ng∆∞·ªùi c≈©ng c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh tham s·ªë `return_sequences=True` nha. üåü ƒêi·ªÅu n√†y s·∫Ω khi·∫øn l·ªõp **LSTM** tr·∫£ v·ªÅ **chu·ªói c√°c gi√° tr·ªã** t·∫°i m·ªói b∆∞·ªõc th·ªùi gian t·ª©c chu·ªói ƒë·∫ßu ra ƒë·∫ßy ƒë·ªß, thay v√¨ ch·ªâ tr·∫£ v·ªÅ **chu·ªói gi√° tr·ªã cu·ªëi c√πng**.  \n",
        "\n",
        "üí° **Khi n√†o c·∫ßn s·ª≠ d·ª•ng `return_sequences=True`?**\n",
        "- N·∫øu b√†i to√°n y√™u c·∫ßu ph√¢n t√≠ch to√†n b·ªô chu·ªói ƒë·∫ßu ra c·ªßa m√¥ h√¨nh thay v√¨ ch·ªâ d·ª±a v√†o tr·∫°ng th√°i cu·ªëi c√πng.  \n",
        "- ƒêi·ªÅu n√†y ƒë·∫∑c bi·ªát h·ªØu √≠ch khi c·∫ßn x·ª≠ l√Ω c√°c b√†i to√°n nh∆∞ **g·∫Øn nh√£n chu·ªói (sequence labeling)** ho·∫∑c **d·ªãch m√°y (machine translation)**.  \n",
        "\n",
        "‚ú® Tuy nhi√™n, trong b√†i to√°n ph√¢n lo·∫°i hi·ªán t·∫°i, vi·ªác tr·∫£ v·ªÅ gi√° tr·ªã cu·ªëi c√πng (`return_sequences=False`) l√† ph√π h·ª£p h∆°n. Ph·∫ßn ph√¢n t√≠ch s√¢u h∆°n v·ªÅ t√≠nh ·ª©ng d·ª•ng c·ªßa `return_sequences=True`m·ªçi ng∆∞·ªùi c√≥ th·ªÉ tham kh·∫£o ·ªü [blog](https://vi.eitca.org/tr%C3%AD-tu%E1%BB%87-nh%C3%A2n-t%E1%BA%A1o/eitc-ai-tff-tensorflow-nguy%C3%AAn-t%E1%BA%AFc-c%C6%A1-b%E1%BA%A3n/x%E1%BB%AD-l%C3%BD-ng%C3%B4n-ng%E1%BB%AF-t%E1%BB%B1-nhi%C3%AAn-v%E1%BB%9Bi-tensorflow/b%E1%BB%99-nh%E1%BB%9B-ng%E1%BA%AFn-h%E1%BA%A1n-d%C3%A0i-h%E1%BA%A1n-cho-nlp/ki%E1%BB%83m-tra-%C4%91%C3%A1nh-gi%C3%A1-tr%C3%AD-nh%E1%BB%9B-ng%E1%BA%AFn-h%E1%BA%A1n-d%C3%A0i-cho-nlp/t%E1%BA%A7m-quan-tr%E1%BB%8Dng-c%E1%BB%A7a-vi%E1%BB%87c-%C4%91%E1%BA%B7t-tham-s%E1%BB%91-return_sequences-th%C3%A0nh-true-khi-x%E1%BA%BFp-ch%E1%BB%93ng-nhi%E1%BB%81u-l%E1%BB%9Bp-lstm-l%C3%A0-g%C3%AC/) n√†y nha. üåü"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "R2iXAjKlDLdt"
      },
      "outputs": [],
      "source": [
        "model1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm5V2jkeDOXw"
      },
      "source": [
        "V√¨ t·ª•i m√¨nh ƒë√£ th√™m m·ªôt l·ªõp **LSTM** n·ªØa, s·ªë l∆∞·ª£ng tham s·ªë c·ªßa m√¥ h√¨nh ƒë√£ tƒÉng th√™m kho·∫£ng **100.000**, t∆∞∆°ng ƒë∆∞∆°ng v·ªõi t·∫ßm **8%**. üìà ƒêi·ªÅu n√†y d·∫´n ƒë·∫øn:  \n",
        "\n",
        "- **Th·ªùi gian hu·∫•n luy·ªán** s·∫Ω l√¢u h∆°n. ‚è≥  \n",
        "- **Chi ph√≠ t√≠nh to√°n** c≈©ng tƒÉng l√™n. üíª  \n",
        "\n",
        "‚ú® **Tuy nhi√™n**, n·∫øu ki·∫øn tr√∫c m·ªõi n√†y mang l·∫°i **hi·ªáu qu·∫£ t·ªët h∆°n**, th√¨ s·ª± ƒë√°nh ƒë·ªïi n√†y ho√†n to√†n **x·ª©ng ƒë√°ng** nha! üòä"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BK-qRdPJCemL"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "history1 = model1.fit(train_padded, train_labels, epochs=epochs, validation_data=(test_padded, test_labels))\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4aGq316jCiK-"
      },
      "outputs": [],
      "source": [
        "training_time = timedelta(seconds=end_time - start_time)\n",
        "print(f\"Th·ªùi gian hu·∫•n luy·ªán: {training_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cBZrLj47Cv8x"
      },
      "outputs": [],
      "source": [
        "# v·∫Ω bi·ªÉu ƒë·ªì hu·∫•n luy·ªán\n",
        "train_acc = history1.history[\"accuracy\"]\n",
        "val_acc = history1.history[\"val_accuracy\"]\n",
        "train_loss = history1.history[\"loss\"]\n",
        "val_loss = history1.history[\"val_loss\"]\n",
        "\n",
        "fig,axs = plt.subplots(1, 2, figsize=(12,4))\n",
        "axs = axs.flatten()\n",
        "axs[0].plot(train_acc, label=\"train\")\n",
        "axs[0].plot(val_acc, label=\"val\")\n",
        "axs[0].set_title(\"Accuracy\")\n",
        "axs[0].legend()\n",
        "axs[1].plot(train_loss, label=\"train\")\n",
        "axs[1].plot(val_loss, label=\"val\")\n",
        "axs[1].set_title(\"Loss\")\n",
        "axs[1].legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "A2SxcNypKfW6"
      },
      "outputs": [],
      "source": [
        "# ƒê·ªô hi·ªáu qu·∫£ c·ªßa m√¥ h√¨nh\n",
        "eval = model1.evaluate(test_padded, test_labels)\n",
        "print(f\"Accuracy: \", eval[1])\n",
        "print(f\"Loss: \", eval[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avzvmm1BPyf2"
      },
      "source": [
        "ƒê√¢y l√† k·∫øt qu·∫£ ·ªü l·∫ßn ch·∫°y c·ªßa m√¨nh nha."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![model1](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/model1.png?raw=true)"
      ],
      "metadata": {
        "id": "3WFh2EcfHWBx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b6LHXNBJMo5"
      },
      "source": [
        "T∆∞∆°ng t·ª± nh∆∞ m√¥ h√¨nh c√≥ **1 l·ªõp LSTM** tr∆∞·ªõc ƒë√≥, m√¥ h√¨nh n√†y c≈©ng g·∫∑p ph·∫£i t√¨nh tr·∫°ng **Overfitting**. üòî  \n",
        "\n",
        "Th·∫≠m ch√≠, qu√° tr√¨nh **Overfitting** c√≤n x·∫£y ra **s·ªõm h∆°n**, c·ª• th·ªÉ:  \n",
        "\n",
        "- **Epoch th·ª© 10**: ƒê∆∞·ªùng **loss tr√™n t·∫≠p val** ƒë√£ b·∫Øt ƒë·∫ßu tƒÉng. üìà  \n",
        "- **Epoch th·ª© 30**: **Loss tr√™n t·∫≠p val** ƒë·∫°t kho·∫£ng **0.7**, cao h∆°n ƒë√°ng k·ªÉ so v·ªõi m√¥ h√¨nh tr∆∞·ªõc ƒë√≥. üî∫  \n",
        "\n",
        "ƒêi·ªÅu n√†y cho th·∫•y vi·ªác th√™m l·ªõp **LSTM** kh√¥ng ph·∫£i l√∫c n√†o c≈©ng mang l·∫°i hi·ªáu qu·∫£ n·∫øu kh√¥ng ƒë∆∞·ª£c x·ª≠ l√Ω v√† t·ªëi ∆∞u ƒë√∫ng c√°ch. üöß"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh0qoBQ1K3Hb"
      },
      "source": [
        "> **V·∫≠y c√≥ c√°ch n√†o ƒë·ªÉ t·ªëi ∆∞u h√≥a ch√∫ng kh√¥ng?** ü§î  \n",
        "\n",
        "T·∫°m th·ªùi, t·ª•i m√¨nh s·∫Ω th·ª≠ nghi·ªám nhi·ªÅu c√°ch kh√°c nhau ƒë·ªÉ **t·ªëi ∆∞u h√≥a** m√¥ h√¨nh, gi·∫£m thi·ªÉu t√¨nh tr·∫°ng **Overfitting** nha. üöÄ  \n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj218DMbLFh7"
      },
      "source": [
        "# Optimizing Stacked LSTM  \n",
        "### T·ªëi ∆∞u h√≥a m√¥ h√¨nh s·ª≠ d·ª•ng **c√°c l·ªõp LSTM x·∫øp ch·ªìng** üõ†Ô∏è  \n",
        "\n",
        "Trong ph·∫ßn n√†y, t·ª•i m√¨nh s·∫Ω th·ª≠ √°p d·ª•ng c√°c ph∆∞∆°ng ph√°p **t·ªëi ∆∞u h√≥a** ƒë·ªÉ c·∫£i thi·ªán m√¥ h√¨nh **Stacked LSTM** nh·∫±m gi·∫£m thi·ªÉu t√¨nh tr·∫°ng **Overfitting** v√† n√¢ng cao hi·ªáu qu·∫£ t·ªïng th·ªÉ c·ªßa m√¥ h√¨nh. üåü  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK0uW7dkLTzc"
      },
      "source": [
        "Nh∆∞ m·ªçi ng∆∞·ªùi th·∫•y ·ªü **ch∆∞∆°ng 6** tr∆∞·ªõc ƒë√≥, c√°ch ƒë∆°n gi·∫£n v√† hi·ªáu qu·∫£ nh·∫•t m√† t·ª•i m√¨nh ƒë√£ ti·∫øp x√∫c l√† gi·∫£m t·ªëc ƒë·ªô h·ªçc `learning_rate`. üåü  \n",
        "\n",
        "C√°ch n√†y ƒë√°ng ƒë·ªÉ th·ª≠ nghi·ªám xem n√≥ c√≥ mang l·∫°i hi·ªáu qu·∫£ t√≠ch c·ª±c ƒë·ªëi v·ªõi **m√¥ h√¨nh m·∫°ng h·ªìi quy - RNN** n√†y hay kh√¥ng. üß†  \n",
        "\n",
        "T·ª•i m√¨nh s·∫Ω gi·∫£m t·ªëc ƒë·ªô h·ªçc `learning_rate` ƒëi kho·∫£ng **20%**, t·ª©c l√†:  \n",
        "`learning_rate=0.000008`.  \n",
        "U·∫ßy, s·ªë nh·ªè thi·ªát s·ª±. üòÖ  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sHMM-WIxMeYU"
      },
      "outputs": [],
      "source": [
        "model2 = Sequential([\n",
        "    Embedding(vocab_size, embedding_dims),\n",
        "    Bidirectional(LSTM(embedding_dims, return_sequences=True)),\n",
        "    Bidirectional(LSTM(embedding_dims)),\n",
        "    Dense(24, activation=\"relu\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "adam = Adam(learning_rate=0.000008, beta_1=0.9, beta_2=0.999)\n",
        "model2.compile(loss=\"binary_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "t0v1ngpBM1Mg"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "history2 = model2.fit(train_padded, train_labels, epochs=epochs, validation_data=(test_padded, test_labels))\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r3wtdNYJM7hp"
      },
      "outputs": [],
      "source": [
        "training_time = timedelta(seconds=end_time - start_time)\n",
        "print(f\"Th·ªùi gian hu·∫•n luy·ªán: {training_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GFmOK0IFM85I"
      },
      "outputs": [],
      "source": [
        "# v·∫Ω bi·ªÉu ƒë·ªì hu·∫•n luy·ªán\n",
        "train_acc = history2.history[\"accuracy\"]\n",
        "val_acc = history2.history[\"val_accuracy\"]\n",
        "train_loss = history2.history[\"loss\"]\n",
        "val_loss = history2.history[\"val_loss\"]\n",
        "\n",
        "fig,axs = plt.subplots(1, 2, figsize=(12,4))\n",
        "axs = axs.flatten()\n",
        "axs[0].plot(train_acc, label=\"train\")\n",
        "axs[0].plot(val_acc, label=\"val\")\n",
        "axs[0].set_title(\"Accuracy\")\n",
        "axs[0].legend()\n",
        "axs[1].plot(train_loss, label=\"train\")\n",
        "axs[1].plot(val_loss, label=\"val\")\n",
        "axs[1].set_title(\"Loss\")\n",
        "axs[1].legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NduKK0cGx33r"
      },
      "outputs": [],
      "source": [
        "# ƒê·ªô hi·ªáu qu·∫£ c·ªßa m√¥ h√¨nh\n",
        "eval = model2.evaluate(test_padded, test_labels)\n",
        "print(f\"Accuracy: \", eval[1])\n",
        "print(f\"Loss: \", eval[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcglNp0Cyq6H"
      },
      "source": [
        "ƒê√¢y l√† k·∫øt qu·∫£ ·ªü l·∫ßn ch·∫°y c·ªßa m√¨nh"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![model2](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/model2.png?raw=true)"
      ],
      "metadata": {
        "id": "Lx341krWHcPs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZauIe2KiPZVy"
      },
      "source": [
        "U·∫ßy nh√¨n s∆° qua c√≥ v·∫ª vi·ªác gi·∫£m **learning rate** kh√¥ng t√°c ƒë·ªông nhi·ªÅu lƒÉm, h√¨nh d√°ng bi·ªÉu ƒë·ªì kh√¥ng thay ƒë·ªïi m·∫•y. Nh∆∞ng th·∫≠t s·ª±, t√¨nh tr·∫°ng Overfiting c√≥ v·∫ª ƒë√£ gi·∫£m ƒëi khi m√† gi·ªù ƒë√¢y ·ªü **epoch th·ª© 30** th√¨ **loss tr√™n val ch·ªâ c√≤n kho·∫£ng 0.65**. D·ªã l√† t√¨nh tr·∫°ng **Overfiting** ƒë√£ t·ªõi ch·∫≠m h∆°n ho·∫∑c √≠t h∆°n  v·ªõi tr∆∞·ªõc. V·∫≠y vi·ªác gi·∫£m ƒëi **learning rate** th·∫≠t s·ª± hi·ªáu qu·∫£."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsVGfxbe3igi"
      },
      "source": [
        "# Using Dropout  \n",
        "### Ti·∫øp ƒë·∫øn, t·ª•i m√¨nh s·∫Ω th·ª≠ s·ª≠ d·ª•ng **dropout** ƒë·ªÉ t·ªëi ∆∞u h√≥a nha. üéØ  \n",
        "\n",
        "Dropout l√† m·ªôt k·ªπ thu·∫≠t th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ gi·∫£m thi·ªÉu t√¨nh tr·∫°ng **Overfitting** b·∫±ng c√°ch t·∫°m th·ªùi \"t·∫Øt\" ng·∫´u nhi√™n m·ªôt s·ªë **neuron** trong qu√° tr√¨nh hu·∫•n luy·ªán. ƒêi·ªÅu n√†y gi√∫p m√¥ h√¨nh kh√¥ng qu√° ph·ª• thu·ªôc v√†o b·∫•t k·ª≥ **neuron** c·ª• th·ªÉ n√†o v√† tƒÉng kh·∫£ nƒÉng kh√°i qu√°t h√≥a. üöÄ  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL0nB7q5NAyP"
      },
      "outputs": [],
      "source": [
        "model3 = Sequential([\n",
        "    Embedding(vocab_size, embedding_dims),\n",
        "    Bidirectional(LSTM(embedding_dims, return_sequences=True, dropout=0.2)),\n",
        "    Bidirectional(LSTM(embedding_dims, dropout=0.2)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "adam = Adam(learning_rate=0.000008, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "model3.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "history3 = model3.fit(train_padded, train_labels, epochs=epochs, validation_data=(test_padded, test_labels))\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "id": "Y950xXds9ykp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# v·∫Ω bi·ªÉu ƒë·ªì hu·∫•n luy·ªán\n",
        "train_acc = history3.history[\"accuracy\"]\n",
        "val_acc = history3.history[\"val_accuracy\"]\n",
        "train_loss = history3.history[\"loss\"]\n",
        "val_loss = history3.history[\"val_loss\"]\n",
        "\n",
        "fig,axs = plt.subplots(1, 2, figsize=(12,4))\n",
        "axs = axs.flatten()\n",
        "axs[0].plot(train_acc, label=\"train\")\n",
        "axs[0].plot(val_acc, label=\"val\")\n",
        "axs[0].set_title(\"Accuracy\")\n",
        "axs[0].legend()\n",
        "axs[1].plot(train_loss, label=\"train\")\n",
        "axs[1].plot(val_loss, label=\"val\")\n",
        "axs[1].set_title(\"Loss\")\n",
        "axs[1].legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d_15QzDx_Xuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_time = str(timedelta(seconds=end_time - start_time))\n",
        "print(f\"Th·ªùi gian hu·∫•n luy·ªán m√¥ h√¨nh: {training_time}\")"
      ],
      "metadata": {
        "id": "qtIX0l0B994G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ƒê√°nh gi√° m√¥ h√¨nh\n",
        "eval = model3.evaluate(test_padded, test_labels)\n",
        "print(f\"Loss: {eval[0]}, Accuracy: {eval[1]}\")"
      ],
      "metadata": {
        "id": "jPWj3as5-Jmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ƒê√¢y l√† k·∫øt qu·∫£ ·ªü l·∫ßn ch·∫°y c·ªßa m√¨nh nha."
      ],
      "metadata": {
        "id": "0Av-y4TE_xa2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![model3](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/model3.png?raw=true)"
      ],
      "metadata": {
        "id": "stdVNCHEHvnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nh∆∞ m·ªçi ng∆∞·ªùi c√≥ th·ªÉ th·∫•y, vi·ªác **s·ª≠ d·ª•ng l·ªõp dropout** kh√¥ng ·∫£nh h∆∞·ªüng nhi·ªÅu ƒë·∫øn ch·ªâ s·ªë **accuracy**, √≠t nh·∫•t l√† n√≥ kh√¥ng gi·∫£m ƒëi ‚Äì ƒëi·ªÅu n√†y c√≥ v·∫ª kh√° t√≠ch c·ª±c. üòä\n",
        "\n",
        "Tr∆∞·ªõc ƒë√¢y, sau v√†i l·∫ßn s·ª≠ d·ª•ng **dropout**, ch√∫ng ta c√≥ m·ªôt ·∫•n t∆∞·ª£ng x·∫•u r·∫±ng vi·ªác t·∫Øt ƒëi m·ªôt v√†i n∆°-ron c√≥ th·ªÉ l√†m m√¥ h√¨nh h·ªçc √≠t h∆°n v√† tr·ªü n√™n k√©m hi·ªáu qu·∫£.\n",
        "\n",
        "Nh∆∞ng th·∫≠t b·∫•t ng·ªù! üéâ  \n",
        "\n",
        "·ªû l·∫ßn hu·∫•n luy·ªán n√†y, m√¥ h√¨nh ƒë√£ ch·ª©ng minh r·∫±ng ph∆∞∆°ng ph√°p **Dropout th·∫≠t s·ª± hi·ªáu qu·∫£**, ƒë·∫∑c bi·ªát khi ch·ªâ s·ªë **loss tr√™n val** ƒë√£ gi·∫£m ƒëi r√µ r·ªát, **ch·ªâ c√≤n kho·∫£ng d∆∞·ªõi 0.5**. ‚úÖ\n",
        "\n",
        "**K·∫øt lu·∫≠n:**  \n",
        "- Ch√∫ng ta c√≥ th·ªÉ **k·∫øt h·ª£p th√™m Dropout** ƒë·ªÉ c·∫£i thi·ªán m√¥ h√¨nh trong tr∆∞·ªùng h·ª£p n√†y.  \n",
        "- ƒê·ªëi v·ªõi c√°c m√¥ h√¨nh c√≥ ki·∫øn tr√∫c ph·ª©c t·∫°p, vi·ªác s·ª≠ d·ª•ng **Dropout** l·∫°i mang ƒë·∫øn k·∫øt qu·∫£ kh·∫£ quan v√† hi·ªáu qu·∫£. üåü"
      ],
      "metadata": {
        "id": "jZa9pdfi_1Hq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "M·ªçi ng∆∞·ªùi c√≥ th·ªÉ th·ª≠ th√™m nhi·ªÅu k·ªπ thu·∫≠t kh√°c n·ªØa ƒë·ªÉ ph√≤ng tr√°nh t√¨nh tr·∫°ng **Overfiting**, ch·∫≥ng h·∫°n nh∆∞:  \n",
        "\n",
        "- L√†m s·∫°ch v√† x·ª≠ l√Ω d·ªØ li·ªáu üìÇ  \n",
        "- Tinh ch·ªânh c√°c tham s·ªë, si√™u tham s·ªë gi·ªëng nh∆∞ ƒë√£ l√†m ·ªü **ch∆∞∆°ng 6** ‚öôÔ∏è  \n",
        "\n",
        "### **Ti·∫øp theo:**  \n",
        "T·ª•i m√¨nh s·∫Ω th·ª≠ s·ª≠ d·ª•ng **ph∆∞∆°ng ph√°p h·ªçc chuy·ªÉn giao (transfer learning)** b·∫±ng c√°ch s·ª≠ d·ª•ng c√°c b·ªô **Embedding** ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán s·∫µn. üõ†Ô∏è Qua ƒë√≥, ch√∫ng ta s·∫Ω ƒë√°nh gi√° xem th·ª≠ ph∆∞∆°ng ph√°p n√†y c√≥ mang l·∫°i hi·ªáu qu·∫£ nh∆∞ mong ƒë·ª£i kh√¥ng nh√©! üöÄ  "
      ],
      "metadata": {
        "id": "9eCohjcXCKDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Pretrained Embeddings with RNNs  \n",
        "### S·ª≠ d·ª•ng c√°c l·ªõp **Embedding** ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán s·∫µn v√†o m√¥ h√¨nh **RNN**. üåü  "
      ],
      "metadata": {
        "id": "bvIEdv_KC35F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "T·ª•i m√¨nh s·∫Ω s·ª≠ d·ª•ng c√°c **b·ªô Embeddings ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc** ƒë·ªÉ √°p d·ª•ng v√¥ m√¥ h√¨nh thay v√¨ ph·∫£i t·ª± hu·∫•n luy·ªán nha. üåü Gi·ªëng nh∆∞ c√°ch ch√∫ng ta load b·ªô **Embedding t·ª´ HuggingFace** h√¥m tr∆∞·ªõc v·∫≠y √°.  \n",
        "\n",
        "·ªû ƒë√¢y t·ª•i m√¨nh s·∫Ω s·ª≠ d·ª•ng **GloVe (Global Vectors for Word Representation)** nha. N√≥ c√≥ nhi·ªÅu phi√™n b·∫£n ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n nhi·ªÅu b·ªô d·ªØ li·ªáu kh√°c nhau. üõ†Ô∏è ·ªû ƒë√¢y tui ch·ªçn b·ªô s·ª≠ d·ª•ng d·ªØ li·ªáu **Twitter** v·ªõi 27 t·ª∑ tokens v√† kho·∫£ng 1.2 tri·ªáu t·ª´ v·ª±ng nha. Ch√∫ng c√≥ 4 phi√™n b·∫£n l·∫ßn l∆∞·ª£t l√† 25, 50, 100 v√† 200 **chi·ªÅu embeddings**.  \n",
        "\n",
        "‚û°Ô∏è ƒê·ªÉ ƒë∆°n gi·∫£n m√¨nh ch·ªâ ch·ªçn phi√™n b·∫£n 25 chi·ªÅu th√¥i nha, m·ªçi ng∆∞·ªùi c√≥ th·ªÉ th·ª≠ th√™m nh·ªØng c√°i kh√°c. üöÄ  "
      ],
      "metadata": {
        "id": "L63pOIhcF3gI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# T·∫£i b·ªô d·ªØ li·ªáu\n",
        "!wget --no-check-certificate \\\n",
        "  https://nlp.stanford.edu/data/glove.twitter.27B.zip \\\n",
        "  -O /tmp/glove.zip"
      ],
      "metadata": {
        "id": "hpLdREnBHOVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/tmp/glove.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/tmp/glove')\n"
      ],
      "metadata": {
        "id": "6u8FX31vDCuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir('/tmp/glove'))"
      ],
      "metadata": {
        "id": "wThh5b5-Lwu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sau khi gi·∫£i n√©n ra, th∆∞ m·ª•c c·ªßa m·ªçi ng∆∞·ªùi s·∫Ω c√≥ 4 file nh∆∞ n√†y, t∆∞∆°ng ·ª©ng v·ªõi 4 lo·∫°i chi·ªÅu l√† **200**, **100**, **50**, v√† **25**. üìÇ\n",
        "\n",
        "```python\n",
        "['glove.twitter.27B.200d.txt', 'glove.twitter.27B.100d.txt', 'glove.twitter.27B.50d.txt', 'glove.twitter.27B.25d.txt']\n",
        "```"
      ],
      "metadata": {
        "id": "bGawgz7VzhS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "T·ª•i m√¨nh s·∫Ω d√πng lo·∫°i **25 chi·ªÅu** nha. üìÑ M·ªçi ng∆∞·ªùi c√≥ th·ªÉ t·∫£i file v·ªÅ v√† ki·ªÉm tra n·ªôi dung b√™n trong th·ª≠.\n",
        "\n",
        "·ªû ƒë√¢y, m·ªói h√†ng c·ªßa ch√∫ng t∆∞∆°ng ·ª©ng v·ªõi **m·ªôt t·ª´**, v√† theo sau ƒë√≥ l√† **c√°c h·ªá s·ªë vector** t∆∞∆°ng ·ª©ng ƒë√£ h·ªçc ƒë∆∞·ª£c trong **kh√¥ng gian 25 chi·ªÅu**. üöÄ"
      ],
      "metadata": {
        "id": "QmDotKpt1An7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove_embeddings = dict()\n",
        "f = open(\"/tmp/glove/glove.twitter.27B.25d.txt\")\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  glove_embeddings[word] = coefs\n",
        "\n",
        "f.close()"
      ],
      "metadata": {
        "id": "On4m2Des0A7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quan s√°t 10 t·ª´ cu·ªëi c√πng trong t·ª´ ƒëi·ªÉn th·ª≠\n",
        "list(glove_embeddings.items())[-5:]"
      ],
      "metadata": {
        "id": "asvsaMDv0V2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "U·∫ßy, m·ªçi ng∆∞·ªùi c√≥ th·ªÉ th·∫•y ch√∫ng th·∫≠m ch√≠ c√≤n bao g·ªìm c·∫£ **c√°c vector bi·ªÉu di·ªÖn c·ªßa ti·∫øng Nh·∫≠t** n·ªØa! üå∏ B·ªô **embeddings** n√†y th·ª±c s·ª± r·∫•t phong ph√∫ v√† ƒëa d·∫°ng. üöÄ"
      ],
      "metadata": {
        "id": "vrBiNDmG3Hmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "B√¢y gi·ªù t·ª•i m√¨nh s·∫Ω ti·∫øn h√†nh t·∫°o **ma tr·∫≠n tr·ªçng s·ªë** cho l·ªõp **embeddings** th√¥ng qua bi·∫øn `embedding_matrix` nha. üåü\n",
        "\n",
        "C√°ch l√†m nh∆∞ sau:  \n",
        "- Duy·ªát qua **c√°c t·ª´ trong t·ª´ ƒëi·ªÉn** c·ªßa tokenizer ƒë√£ t·∫°o tr∆∞·ªõc ƒë√≥.  \n",
        "- **Tr√≠ch xu·∫•t c√°c vector bi·ªÉu di·ªÖn t∆∞∆°ng ·ª©ng** t·ª´ b·ªô **embeddings ƒë√£ hu·∫•n luy·ªán s·∫µn**.  \n",
        "- L∆∞u c√°c vector n√†y v√†o `embedding_matrix` ƒë·ªÉ l√†m tr·ªçng s·ªë ban ƒë·∫ßu cho l·ªõp **Embedding** trong m√¥ h√¨nh c·ªßa ch√∫ng ta. üí°"
      ],
      "metadata": {
        "id": "9M23Klih3VkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ƒê·ªãnh nghƒ©a l·∫°i s·ªë chi·ªÅu l√† 25\n",
        "embedding_dims = 25\n",
        "# ƒê·ªãnh nghƒ©a s·ªë l∆∞·ª£ng t·ª´ t∆∞∆°ng ·ª©ng v·ªõi t·ª´ ƒëi·ªÉn ban ƒë·∫ßu\n",
        "vocab_size = 20000\n",
        "# T·∫°o ma tr·∫≠n r·ªóng v·ªõi t·∫•t c·∫£ ph·∫ßn t·ª≠ l√† 0 tr∆∞·ªõc ƒë·ªÉ ti·∫øn h√†nh t·ª´ t·ª´ l∆∞u tr·ªçng s·ªë c√°c t·ª´.\n",
        "embeddings_matrix = np.zeros((vocab_size, embedding_dims))\n",
        "# Ti·∫øn h√†nh duy·ªát qua c√°c t·ª´ b√™n trong b·ªô t·ª´ ƒëi·ªÉn\n",
        "for word, index in tokenizer.word_index.items():\n",
        "  if index > vocab_size - 1: # N·∫øu v∆∞·ª£t qua s·ªë l∆∞·ª£ng t·ª´ th√¨ d·ª´ng v√≤ng l·∫∑p\n",
        "    break\n",
        "  else: # Ti·∫øn h√†nh l∆∞u c√°c tr·ªçng s·ªë l·∫°i t∆∞∆°ng ·ª©ng v·ªõi t·ª´ng t·ª´\n",
        "    embedding_vector = glove_embeddings.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embeddings_matrix[index] = embedding_vector\n"
      ],
      "metadata": {
        "id": "801otvtn0t1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ki·ªÉm tra ma tr·∫≠n tr·ªçng s·ªë l·ªõp embeddings\n",
        "embeddings_matrix"
      ],
      "metadata": {
        "id": "6oLTngbn6qdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "V·∫≠y l√† t·ª•i m√¨nh ƒë√£ chu·∫©n b·ªã xong ph·∫ßn **tr·ªçng s·ªë cho l·ªõp embeddings** r·ªìi nha! üåü  \n",
        "B√¢y gi·ªù, b∆∞·ªõc ti·∫øp theo l√†:\n",
        "\n",
        "üëâ **R√°p ph·∫ßn tr·ªçng s·ªë n√†y v√†o l·ªõp embeddings** trong m√¥ h√¨nh.  \n",
        "\n",
        "### C√°c b∆∞·ªõc th·ª±c hi·ªán:\n",
        "1. **T·∫°o l·ªõp Embedding** v·ªõi tham s·ªë `weights` ƒë∆∞·ª£c truy·ªÅn v√†o t·ª´ ma tr·∫≠n `embedding_matrix` ƒë√£ chu·∫©n b·ªã.  \n",
        "2. ƒê·∫∑t tham s·ªë `trainable=False` ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng c√°c vector embeddings ƒë√£ hu·∫•n luy·ªán s·∫µn s·∫Ω kh√¥ng b·ªã thay ƒë·ªïi trong qu√° tr√¨nh h·ªçc.  \n",
        "\n",
        "V·ªõi c√°c b∆∞·ªõc tr√™n, l·ªõp **Embedding** c·ªßa ch√∫ng ta s·∫Ω t·∫≠n d·ª•ng ƒë∆∞·ª£c to√†n b·ªô s·ª©c m·∫°nh c·ªßa b·ªô **GloVe embeddings** ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n d·ªØ li·ªáu l·ªõn. üöÄ"
      ],
      "metadata": {
        "id": "SMjJR4Vu6v9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model4 = Sequential([\n",
        "    Embedding(vocab_size, embedding_dims,\n",
        "               weights=[embeddings_matrix], trainable=False),\n",
        "    Bidirectional(LSTM(embedding_dims, return_sequences=True)),\n",
        "    Bidirectional(LSTM(embedding_dims)),\n",
        "    Dense(24, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "adam = Adam(learning_rate=0.00001)\n",
        "model4.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "dpwUu_407G0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=30\n",
        "start_time = time.time()\n",
        "history4 = model4.fit(train_padded, train_labels, epochs=epochs, validation_data=(test_padded, test_labels))\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "id": "DdfC29G38oit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_time = str(timedelta(seconds=end_time - start_time))\n",
        "print(f\"Th·ªùi gian hu·∫•n luy·ªán: {training_time}\")"
      ],
      "metadata": {
        "id": "ht0TcwHw_JXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# V·∫Ω bi·ªÉu ƒë·ªì qu√° tr√¨nh hu·∫•n luy·ªán\n",
        "train_acc = history4.history['accuracy']\n",
        "val_acc = history4.history['val_accuracy']\n",
        "train_loss = history4.history['loss']\n",
        "val_loss = history4.history['val_loss']\n",
        "\n",
        "fig,axs = plt.subplots(1, 2, figsize=(12,4))\n",
        "axs = axs.flatten()\n",
        "axs[0].plot(train_acc, label='train')\n",
        "axs[0].plot(val_acc, label='val')\n",
        "axs[0].set_title('Accuracy')\n",
        "axs[0].legend()\n",
        "\n",
        "axs[1].plot(train_loss, label='train')\n",
        "axs[1].plot(val_loss, label='val')\n",
        "axs[1].set_title('Loss')\n",
        "axs[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lZsk2UmP_Ts4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ƒê√¢y l√† k·∫øt qu·∫£ ·ªü l·∫ßn ch·∫°y c·ªßa m√¨nh."
      ],
      "metadata": {
        "id": "xQl866YDF8_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![model4](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/model4.png?raw=true)"
      ],
      "metadata": {
        "id": "FShyTbIiIleM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "U·∫ßy, m·ªçi ng∆∞·ªùi th·∫•y r√µ r√†ng r·ªìi ha! üåü  \n",
        "Ch√∫ng ta ƒë√£ **kh·∫Øc ph·ª•c ƒë∆∞·ª£c t√¨nh tr·∫°ng Overfitting** trong kho·∫£ng **30 epochs ƒë·∫ßu**. Th·∫≠t s·ª± l√† m·ªôt k·∫øt qu·∫£ **r·∫•t ·∫•n t∆∞·ª£ng**. üéâ  \n",
        "\n",
        "üëâ N·∫øu ti·∫øp t·ª•c **tƒÉng s·ªë l∆∞·ª£ng epochs l√™n** v√† hu·∫•n luy·ªán l√¢u h∆°n n·ªØa, c√≥ kh·∫£ nƒÉng r·∫±ng m√¥ h√¨nh s·∫Ω **c√≤n hi·ªáu qu·∫£ h∆°n** n·ªØa ƒë·∫•y. üöÄ  "
      ],
      "metadata": {
        "id": "CSbtG03aFJvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "M·∫∑c d√π m√¥ h√¨nh b√¢y gi·ªù ƒë√£ hi·ªáu qu·∫£ h∆°n v√† gi·∫£m thi·ªÉu t√¨nh tr·∫°ng **Overfiting** ƒëi m·ªôt c√°ch r√µ r·ªát, nh∆∞ng c√≥ th·ªÉ trong t∆∞∆°ng lai b·∫°n v·∫´n mu·ªën c·∫£i thi·ªán h∆°n n·ªØa. L√∫c n√†y, b·∫°n c√≥ th·ªÉ nghƒ© ƒë·∫øn c√°c ph∆∞∆°ng ph√°p nh∆∞ **ƒëi·ªÅu ch·ªânh l·∫°i c√°c tham s·ªë**, ƒëi·ªÉn h√¨nh l√† `**vocab_size**` m√† ch√∫ng ta ƒë√£ s·ª≠ d·ª•ng ·ªü ch∆∞∆°ng tr∆∞·ªõc ƒë·ªÉ h·∫°n ch·∫ø t√¨nh tr·∫°ng **Overfiting**. üåü  \n",
        "\n",
        "·ªû ch∆∞∆°ng tr∆∞·ªõc, th√¥ng qua vi·ªác **gi·∫£m k√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn**, ch√∫ng ta ƒë√£ gi√∫p m√¥ h√¨nh ch√∫ tr·ªçng v√†o **c√°c t·ª´ ph·ªï bi·∫øn h∆°n**.  \n",
        "\n",
        "> **V·∫≠y ch√∫ng ta c√≥ th·ªÉ ch·ªçn con s·ªë n√†o l√† h·ª£p l√Ω cho vocab_size?**  \n",
        "\n",
        "M√¨nh nghƒ© ch√∫ng ta c·∫ßn ph·∫£i **quan s√°t k·ªπ l∆∞·ª°ng** h∆°n tr∆∞·ªõc khi ƒë∆∞a ra quy·∫øt ƒë·ªãnh.  \n",
        "\n",
        "Bi·∫øt r·∫±ng **b·ªô Embedding ƒë∆∞·ª£c hu·∫•n luy·ªán s·∫µn** n√†y c√≥ ƒë·∫øn **1,2 tri·ªáu t·ª´**, nh∆∞ng ch√∫ng ta **kh√¥ng th·ªÉ ƒë·∫£m b·∫£o** r·∫±ng t·∫•t c·∫£ c√°c t·ª´ trong b·ªô d·ªØ li·ªáu hu·∫•n luy·ªán c·ªßa ch√∫ng ta ƒë·ªÅu c√≥ m·∫∑t trong **b·ªô Embedding** n√†y. üîç"
      ],
      "metadata": {
        "id": "PBAIK35_GB8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xs = []\n",
        "ys = [] # ƒê√°nh d·∫•u xem t·ª´ ·ªü index ƒë√≥ t·ªìn t·∫°i trong b·ªô Embedding hay ch∆∞a, n·∫øu c√≥ l√† 1, kh√¥ng c√≥ l√† 0\n",
        "cumulative_x = []\n",
        "cumulative_y = [] # T·ª∑ l·ªá c·ªßa t·ª´ ·ªü m·ªói b∆∞·ªõc th·ªùi gian, d√πng ƒë·ªÉ ki·ªÉm tra xem t·ª∑ l·ªá c√≥ bao nhi√™u t·ª´ c√≥ trong embedding Glove ƒë√£ hu·∫•n luy·ªán.\n",
        "total_y = 0\n",
        "for word, index in tokenizer.word_index.items():\n",
        "  xs.append(index)\n",
        "  cumulative_x.append(index)\n",
        "  if glove_embeddings.get(word) is not None:\n",
        "    total_y += 1\n",
        "    ys.append(1)\n",
        "  else:\n",
        "    ys.append(0)\n",
        "  cumulative_y.append(total_y/index) # Ki·ªÉm tra l·∫°i t·ª∑ l·ªá xu·∫•t hi·ªán c√°c t·ª´ trong t·ª´ng b∆∞·ªõc th·ªùi gian."
      ],
      "metadata": {
        "id": "IBeov1AQJIYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"T·ª∑ l·ªá c√°c t·ª´ trong b·ªô t·ª´ di·ªÉn tokenizer t·ªìn t·∫°i trong embedding ƒë∆∞·ª£c hu·∫•n luy·ªán s·∫µn: \", cumulative_y[-1])"
      ],
      "metadata": {
        "id": "_5AY08bnLqjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tr·ª±c quan h√≥a k·∫øt qu·∫£ l√™n b·∫±ng bi·ªÉu ƒë·ªì m·∫≠t ƒë·ªô.\n",
        "fig, ax = plt.subplots(figsize=(12, 2))\n",
        "ax.spines['top'].set_visible(False)\n",
        "plt.margins(x=0, y=None, tight=True)\n",
        "plt.fill(ys)"
      ],
      "metadata": {
        "id": "5iowhgfPLF90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "M·ªçi ng∆∞·ªùi c√≥ th·ªÉ th·∫•y c√≥ m·ªôt **ranh gi·ªõi r√µ r·ªát** ·ªü gi·ªØa kho·∫£ng **10.000 v√† 15.000**, t·∫°o th√†nh m·ªôt ƒë∆∞·ªùng ph√¢n c√°ch. Qua quan s√°t k·ªπ l∆∞·ª°ng, m√¨nh nghƒ© kho·∫£ng **13.000** l√† con s·ªë h·ª£p l√Ω.  \n",
        "\n",
        "T·ª´ th·ªùi ƒëi·ªÉm c√°c token c√≥ **index b·∫±ng 13.000** tr·ªü ƒëi, s·ªë l∆∞·ª£ng c√°c t·ª´ **kh√¥ng n·∫±m trong b·ªô Embedding** tƒÉng l√™n r√µ r·ªát.  \n",
        "\n",
        "B√¢y gi·ªù t·ª•i m√¨nh s·∫Ω s·ª≠ d·ª•ng c√°c bi·∫øn `cumulative_x` v√† `cumulative_y` ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì v·ªÅ **t·∫ßn su·∫•t xu·∫•t hi·ªán** c·ªßa c√°c t·ª´ trong **t·ª´ ƒëi·ªÉn Tokenizer** b√™n trong **b·ªô Embedding**, qua t·ª´ng th·ªùi ƒëi·ªÉm nha! üìä"
      ],
      "metadata": {
        "id": "nnfaliFrMcs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(cumulative_x, cumulative_y)\n",
        "plt.xlabel(\"Index c·ªßa t·ª´ trong t·ª´ ƒëi·ªÉn Tokenizer\")\n",
        "plt.ylabel(\"T·ª∑ l·ªá c√°c t·ª´ xu·∫•t hi·ªán trong t·ª´ng b∆∞·ªõc th·ªùi gian\")\n",
        "plt.axis([0, 25000, 0.915, 0.985]) # Quan s√°t trong kho·∫£ng x t·ª´ 0 ƒë·∫øn 25.000, y t·ª´ 0.915 ƒë·∫øn 0.985"
      ],
      "metadata": {
        "id": "_4ZPqEuBNrI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qua bi·ªÉu ƒë·ªì, m·ªçi ng∆∞·ªùi c√≥ th·ªÉ th·∫•y r√µ **ƒëi·ªÉm g√£y** xu·∫•t hi·ªán trong kho·∫£ng **10.000 ƒë·∫øn 15.000**.  \n",
        "\n",
        "M·ªçi ng∆∞·ªùi c√≥ th·ªÉ s·ª≠ d·ª•ng **`plt.axis()`** ƒë·ªÉ zoom to h√¨nh ·∫£nh trong kho·∫£ng ƒë√≥, gi√∫p quan s√°t chi ti·∫øt h∆°n, r√µ h∆°n nha. Sau khi quan s√°t k·ªπ, t√°c gi·∫£ quy·∫øt ƒë·ªãnh ch·ªçn k√≠ch th∆∞·ªõc cho b·ªô t·ª´ ƒëi·ªÉn l√† **13.200**, thay v√¨ **2.000** nh∆∞ ·ªü ch∆∞∆°ng tr∆∞·ªõc ho·∫∑c **20.000** nh∆∞ ·ªü m√¥ h√¨nh tr∆∞·ªõc. üìè‚ú®"
      ],
      "metadata": {
        "id": "PnyZGfw1OlMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ƒê·ªãnh nghƒ©a l·∫°i s·ªë chi·ªÅu l√† 25\n",
        "embedding_dims = 25\n",
        "# ƒê·ªãnh nghƒ©a l·∫°i s·ªë l∆∞·ª£ng t·ª´\n",
        "vocab_size = 13200\n",
        "# T·∫°o ma tr·∫≠n r·ªóng v·ªõi t·∫•t c·∫£ ph·∫ßn t·ª≠ l√† 0 tr∆∞·ªõc ƒë·ªÉ ti·∫øn h√†nh t·ª´ t·ª´ l∆∞u tr·ªçng s·ªë c√°c t·ª´.\n",
        "embeddings_matrix = np.zeros((vocab_size, embedding_dims))\n",
        "# Ti·∫øn h√†nh duy·ªát qua c√°c t·ª´ b√™n trong b·ªô t·ª´ ƒëi·ªÉn\n",
        "for word, index in tokenizer.word_index.items():\n",
        "  if index > vocab_size - 1: # N·∫øu v∆∞·ª£t qua s·ªë l∆∞·ª£ng t·ª´ th√¨ d·ª´ng v√≤ng l·∫∑p\n",
        "    break\n",
        "  else: # Ti·∫øn h√†nh l∆∞u c√°c tr·ªçng s·ªë l·∫°i t∆∞∆°ng ·ª©ng v·ªõi t·ª´ng t·ª´\n",
        "    embedding_vector = glove_embeddings.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embeddings_matrix[index] = embedding_vector\n"
      ],
      "metadata": {
        "id": "mGI-BDW9RZQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model5 = Sequential([\n",
        "    Embedding(vocab_size, embedding_dims,\n",
        "               weights=[embeddings_matrix], trainable=False),\n",
        "    Bidirectional(LSTM(embedding_dims, return_sequences=True)),\n",
        "    Bidirectional(LSTM(embedding_dims)),\n",
        "    Dense(24, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "adam = Adam(learning_rate=0.00001)\n",
        "model5.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "FBlmOh-hSvZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=30\n",
        "start_time = time.time()\n",
        "history5 = model5.fit(train_padded, train_labels, epochs=epochs, validation_data=(test_padded, test_labels))\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "id": "LjTYxO6rSvZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_time = str(timedelta(seconds=end_time - start_time))\n",
        "print(f\"Th·ªùi gian hu·∫•n luy·ªán: {training_time}\")"
      ],
      "metadata": {
        "id": "fv4yzSW2SvZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# V·∫Ω bi·ªÉu ƒë·ªì qu√° tr√¨nh hu·∫•n luy·ªán\n",
        "train_acc = history5.history['accuracy']\n",
        "val_acc = history5.history['val_accuracy']\n",
        "train_loss = history5.history['loss']\n",
        "val_loss = history5.history['val_loss']\n",
        "\n",
        "fig,axs = plt.subplots(1, 2, figsize=(12,4))\n",
        "axs = axs.flatten()\n",
        "axs[0].plot(train_acc, label='train')\n",
        "axs[0].plot(val_acc, label='val')\n",
        "axs[0].set_title('Accuracy')\n",
        "axs[0].legend()\n",
        "\n",
        "axs[1].plot(train_loss, label='train')\n",
        "axs[1].plot(val_loss, label='val')\n",
        "axs[1].set_title('Loss')\n",
        "axs[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3vdeN8N3SvZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval = model5.evaluate(test_padded, test_labels)\n",
        "print(\"Loss: \", eval[0])\n",
        "print(\"Accuracy: \", eval[1])"
      ],
      "metadata": {
        "id": "B_oRs-4pS3gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ƒê√°nh gi√° m√¥ h√¨nh\n",
        "eval = model5.evaluate(test_padded, test_labels)\n",
        "print(\"Loss: \", eval[0])\n",
        "print(\"Accuracy: \", eval[1])"
      ],
      "metadata": {
        "id": "i49fBuzIW5j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ƒê√¢y l√† k·∫øt qu·∫£ ·ªü l·∫ßn ch·∫°y c·ªßa m√¨nh."
      ],
      "metadata": {
        "id": "T7vul3q6YUKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![model5](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/model5.png?raw=true)"
      ],
      "metadata": {
        "id": "J-bPl4sgJRal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "M·ªçi ng∆∞·ªùi c√≥ th·ªÉ th·∫•y ƒë∆∞·ªùng **val_loss** v√† **train_loss** ƒë√£ tr·ªü n√™n **m∆∞·ª£t h∆°n** m·ªôt ch√∫t. Trong kho·∫£ng **30 epochs** ƒë·∫ßu, m√¥ h√¨nh kh√¥ng b·ªã **Overfit**, v√† s·ª± ch√™nh l·ªách gi·ªØa **train_loss** v√† **val_loss** c≈©ng √≠t h∆°n. ‚ú®\n",
        "\n",
        "B√¢y gi·ªù, t·ª•i m√¨nh s·∫Ω ti·∫øp t·ª•c hu·∫•n luy·ªán **l√¢u h∆°n** ƒë·ªÉ ki·ªÉm tra:  \n",
        "- M√¥ h√¨nh c√≥ ti·∫øp t·ª•c **c·∫£i thi·ªán** kh√¥ng?  \n",
        "- **Gi·ªõi h·∫°n** c·ªßa n√≥ ƒë·∫°t ƒë∆∞·ª£c ·ªü ƒë√¢u?  \n",
        "- Khi n√†o th√¨ m√¥ h√¨nh b·∫Øt ƒë·∫ßu xu·∫•t hi·ªán **Overfit**?  \n",
        "\n",
        "M√¨nh s·∫Ω t·∫°m th·ªùi ch·ªçn s·ªë epochs l√† **100** nha. ‚è≥"
      ],
      "metadata": {
        "id": "nnc57mYhWywq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model6 = Sequential([\n",
        "    Embedding(vocab_size, embedding_dims,\n",
        "               weights=[embeddings_matrix], trainable=False),\n",
        "    Bidirectional(LSTM(embedding_dims, return_sequences=True)),\n",
        "    Bidirectional(LSTM(embedding_dims)),\n",
        "    Dense(24, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "adam = Adam(learning_rate=0.00001)\n",
        "model6.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Y4OrhhbHX7qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=100\n",
        "start_time = time.time()\n",
        "history6 = model6.fit(train_padded, train_labels, epochs=epochs, validation_data=(test_padded, test_labels))\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "id": "HVpNjO5kX7qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_time = str(timedelta(seconds=end_time - start_time))\n",
        "print(f\"Th·ªùi gian hu·∫•n luy·ªán: {training_time}\")"
      ],
      "metadata": {
        "id": "O7LEx3coX7qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# V·∫Ω bi·ªÉu ƒë·ªì qu√° tr√¨nh hu·∫•n luy·ªán\n",
        "train_acc = history6.history['accuracy']\n",
        "val_acc = history6.history['val_accuracy']\n",
        "train_loss = history6.history['loss']\n",
        "val_loss = history6.history['val_loss']\n",
        "\n",
        "fig,axs = plt.subplots(1, 2, figsize=(12,4))\n",
        "axs = axs.flatten()\n",
        "axs[0].plot(train_acc, label='train')\n",
        "axs[0].plot(val_acc, label='val')\n",
        "axs[0].set_title('Accuracy')\n",
        "axs[0].legend()\n",
        "\n",
        "axs[1].plot(train_loss, label='train')\n",
        "axs[1].plot(val_loss, label='val')\n",
        "axs[1].set_title('Loss')\n",
        "axs[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jifa4zGHX7qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval = model6.evaluate(test_padded, test_labels)\n",
        "print(\"Loss: \", eval[0])\n",
        "print(\"Accuracy: \", eval[1])"
      ],
      "metadata": {
        "id": "85douhpOX7qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ƒê√¢y l√† k·∫øt qu·∫£ ·ªü l·∫ßn ch·∫°y c·ªßa m√¨nh nha."
      ],
      "metadata": {
        "id": "mggQ4lDOKhp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![model6](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/model6.png?raw=true)"
      ],
      "metadata": {
        "id": "SjabQmp6Kkv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "U·∫ßy, ·∫•n t∆∞·ª£ng th·∫≠t ƒë·∫•y, ƒë·∫øn epoch th·ª© 100 r·ªìi, m·∫∑c d√π t·ªëc ƒë·ªô gi·∫£m ƒëi c·ªßa **loss** ƒë√£ gi·∫£m nh∆∞ng m√† v·∫´n ch∆∞a th·∫•y t√¨nh tr·∫°ng **Overfit** x·∫£y ra. ƒê·ªìng th·ªùi **val loss** c·ªßa m√¥ h√¨nh v·∫´n c√≥ d·∫•u hi·ªáu  ti·∫øp t·ª•c gi·∫£m, ·ªü epoch 100 gi·∫£m ch·ªâ c√≤n kho·∫£ng **0.525**, v√† v·∫´n c√≥ kh·∫£ nƒÉng gi·∫£m n·ªØa trong t∆∞∆°ng lai khi hu·∫•n luy·ªán th√™m.\n",
        "\n",
        "D·ªã l√† qu√° tr√¨nh **Overfit** ƒë√£ t·ªõi ch·∫≠m h∆°n r·∫•t nhi·ªÅu, c√≥ th·ªÉ m·ªçi ng∆∞·ªùi train th√™m s·∫Ω c√≤n c·∫£i thi·ªán h∆°n nhi·ªÅu n·ªØa, cho ƒë·∫øn khi **train loss** gi·∫£m m√† **val loss** l·∫°i tƒÉng th√¨ t√¨nh tr·∫°ng **Overfit** m·ªõi th·ª±c s·ª± b·∫Øt ƒë·∫ßu.\n",
        "\n",
        "Ph∆∞∆°ng ph√°p t·ªëi ∆∞u h√≥a n√†y c·ªßa ch√∫ng ta th·∫≠t s·ª± hi·ªáu qu·∫£. üåü\n",
        "\n",
        "Trong th·ª±c t·∫ø th√¨ m·ªçi ng∆∞·ªùi c·ª© hu·∫•n luy·ªán v·ªõi m·ªôt s·ªë epoch kh√° l·ªõn cho t·ªõi khi n√†o h·∫øt th√¨ th√¥i n·∫øu m·ªçi ng∆∞·ªùi c√≥ d∆∞ t√†i nguy√™n. Trong qu√° tr√¨nh hu·∫•n luy·ªán ƒë√≥, ch√∫ng ta s·∫Ω l∆∞u l·∫°i **tr·ªçng s·ªë cho ra k·∫øt qu·∫£ ƒë√°nh gi√° cao nh·∫•t** c·ªßa m√¥ h√¨nh.\n",
        "\n",
        "M·ªçi ng∆∞·ªùi ch·ªâ c·∫ßn t·∫°o **check_point** ƒë·ªÉ l∆∞u l·∫°i **tr·ªçng s·ªë hi·ªáu qu·∫£ nh·∫•t** cho m√¥ h√¨nh l√† ƒë∆∞·ª£c. Th∆∞·ªùng th√¨ vi·ªác n√†y s·∫Ω d·ª±a tr√™n thang ƒëo, c√≥ 2 tr·ªçng s·ªë th∆∞·ªùng ƒë∆∞·ª£c l∆∞u  l√† tr·ªçng s·ªë khi·∫øn m√¥ h√¨nh c√≥ **accuracy cao nh·∫•t** v√† tr·ªçng s·ªë khi·∫øn m√¥ h√¨nh c√≥ **loss th·∫•p nh·∫•t**\n",
        "\n",
        "M·ªçi ng∆∞·ªùi c√≥ th·ªÉ th·ª≠ ƒëo·∫°n code n√†y ƒë·ªÉ l∆∞u l·∫°i tr·ªçng s·ªë nha. M·∫•u ch·ªët ch√≠nh l√† ·ªü ph·∫ßn `callbacks` ƒë·ªÉ thi·∫øt l·∫≠p **checkpoint**.\n",
        "\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Callback ƒë·ªÉ l∆∞u model weights c√≥ accuracy cao nh·∫•t\n",
        "checkpoint_accuracy = ModelCheckpoint(\n",
        "    filepath='best_accuracy_model.h5',  # ƒê∆∞·ªùng d·∫´n l∆∞u tr·ªØ weights\n",
        "    monitor='val_accuracy',             # Theo d√µi metric val_accuracy\n",
        "    save_best_only=True,                # Ch·ªâ l∆∞u weights n·∫øu t·ªët h∆°n model tr∆∞·ªõc ƒë√≥\n",
        "    mode='max',                         # L∆∞u d·ª±a tr√™n gi√° tr·ªã l·ªõn nh·∫•t\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Callback ƒë·ªÉ l∆∞u model weights c√≥ loss th·∫•p nh·∫•t\n",
        "checkpoint_loss = ModelCheckpoint(\n",
        "    filepath='best_loss_model.h5',      # ƒê∆∞·ªùng d·∫´n l∆∞u tr·ªØ weights\n",
        "    monitor='val_loss',                 # Theo d√µi metric val_loss\n",
        "    save_best_only=True,                # Ch·ªâ l∆∞u weights n·∫øu t·ªët h∆°n model tr∆∞·ªõc ƒë√≥\n",
        "    mode='min',                         # L∆∞u d·ª±a tr√™n gi√° tr·ªã nh·ªè nh·∫•t\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Th√™m v√†o qu√° tr√¨nh hu·∫•n luy·ªán m√¥ h√¨nh\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    train_labels,\n",
        "    validation_data=(val_data, val_labels),\n",
        "    epochs=50,\n",
        "    callbacks=[checkpoint_accuracy, checkpoint_loss]\n",
        ")\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "HJPhs4_8ao65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ú® Trong tr∆∞·ªùng h·ª£p m·ªçi ng∆∞·ªùi mu·ªën l∆∞u h·∫øt t·∫•t c·∫£ l·∫°i lu√¥n th√¨ ph·∫£i l√†m sao? M·ªçi ng∆∞·ªùi ch·ªâ c·∫ßn t·∫Øt t√≠nh nƒÉng `save_best_only` l√† ƒë∆∞·ª£c nha, chuy·ªÉn tham s·ªë n√†y sang **False** v√† ƒëi·ªÅu ch·ªânh c√∫ ph√°p filepath l·∫°i. üòä\n",
        "\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# T·∫°o callback ƒë·ªÉ l∆∞u weights sau m·ªói epoch\n",
        "checkpoint_all_epochs = ModelCheckpoint(\n",
        "    filepath='weights_epoch_{epoch:02d}.h5',  # T√™n file bao g·ªìm s·ªë epoch\n",
        "    save_best_only=False,                    # L∆∞u ·ªü m·ªçi epoch, kh√¥ng ch·ªâ best weights\n",
        "    verbose=1                                # Hi·ªÉn th·ªã log l∆∞u file\n",
        ")\n",
        "\n",
        "# Hu·∫•n luy·ªán m√¥ h√¨nh\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    train_labels,\n",
        "    validation_data=(val_data, val_labels),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[checkpoint_all_epochs]\n",
        ")\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "vTDhW1nXbnSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B√¢y gi·ªù ch√∫ng ta ti·∫øn h√†nh ki·ªÉm tra m√¥ h√¨nh v·ªõi c√°c c√¢u th·ª±c t·∫ø th·ª≠ nh√°."
      ],
      "metadata": {
        "id": "axCgSM-edR9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# D·ªØ li·ªáu c√¢u v√† nh√£n\n",
        "test_real_sentences = [\n",
        "    \"Oh great, another Monday morning. Just what I needed!\",\n",
        "    \"Thanks for showing up on time, as always.\",\n",
        "    \"Wow, that‚Äôs exactly how I told you not to do it.\",\n",
        "    \"You‚Äôre so organized; I can totally see that from the mess on your desk.\",\n",
        "    \"Oh sure, because ignoring the problem will definitely make it go away.\",\n",
        "    \"I really appreciate your help with this project.\",\n",
        "    \"The view from this hill is absolutely stunning.\",\n",
        "    \"You‚Äôve done a great job organizing the event.\",\n",
        "    \"I‚Äôm so happy to hear about your promotion. Congratulations!\",\n",
        "    \"This book has some very insightful ideas.\"\n",
        "]\n",
        "\n",
        "correct_labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]  # 1: M·ªâa mai, 0: Kh√¥ng m·ªâa mai"
      ],
      "metadata": {
        "id": "oIlfkoMqdQfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_real_sequences = tokenizer.texts_to_sequences(test_real_sentences)\n",
        "max_len = 100\n",
        "test_real_padded = pad_sequences(test_real_sequences, maxlen=max_len, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "ur_3hrPJdp5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model6.predict(test_real_padded)\n",
        "print(f\"K·∫øt qu·∫£ d·ª± ƒëo√°n theo t·ª∑ l·ªá: {result}\")\n",
        "result_binary = [1 if x > 0.5 else 0 for x in result]\n",
        "print(f\"K·∫øt qu·∫£ d·ª± ƒëo√°n theo nh√£n: {result_binary}\")"
      ],
      "metadata": {
        "id": "DfLpI_hoeCMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ú®Training Model with Pretrained Embedding from HuggingFace  \n",
        "### Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi b·ªô Embedding l·∫•y t·ª´ HuggingFace  \n",
        "\n",
        "Ph·∫ßn n√†y kh√¥ng c√≥ trong s√°ch m√† l√† m√¨nh t·ª± l√†m th√™m nha, xem th·ª≠ hi·ªáu qu·∫£ cao h∆°n bao nhi√™u.  \n",
        "\n",
        "V√¨ ƒëo·∫°n code d∆∞·ªõi ƒë√¢y kh√° r·ªëi n√™n m·ªçi ng∆∞·ªùi ƒë·ªçc k·ªπ ph·∫ßn m√¥ t·∫£ n√†y gi√∫p m√¨nh nha. Trong l√∫c l√†m, m√¨nh g·∫∑p v·∫•n ƒë·ªÅ v·ªÅ **gi·ªõi h·∫°n b·ªô nh·ªõ v√† GPU** trong su·ªët qu√° tr√¨nh hu·∫•n luy·ªán n√™n m√¨nh kh√¥ng th·ªÉ hu·∫•n luy·ªán tr√™n **Colab** ƒë∆∞·ª£c, thay v√†o ƒë√≥ s·ª≠ d·ª•ng **Kaggle** k·∫øt h·ª£p v·ªõi m·ªôt v√†i m·∫πo. M·ªçi ng∆∞·ªùi c√≥ th·ªÉ b·∫≠t b√†i tr√™n **Kaggle** ƒë·ªÉ coi ƒë·∫ßy ƒë·ªß [·ªü ƒë√¢y](https://www.kaggle.com/code/trnhhunhthnhkhang/chapter-7) nha. üòä\""
      ],
      "metadata": {
        "id": "LZ4ivKldok30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úçÔ∏è M√¥ t·∫£ qu√° tr√¨nh th·ª±c hi·ªán:\n",
        "\n",
        "1. Load b·ªô **Embedding**: **bert-base-uncased**.  \n",
        "2. Ti·∫øn h√†nh **Embedding** b·ªô d·ªØ li·ªáu c·ªßa t·ª•i m√¨nh: nh∆∞ m√¨nh n√≥i v√¨ b·ªô nh·ªõ RAM g·∫∑p gi·ªõi h·∫°n n√™n m√¨nh ƒë√£ l∆∞u l·∫°i v√†o trong file `.h5` xong gi·∫£i ph√≥ng b·ªô nh·ªõ ·ªü t·ª´ng ƒë·ª£t.  \n",
        "3. Sau khi **Embedding** xong h·∫øt th√¨ ti·∫øn h√†nh l·∫•y ng∆∞·ª£c b·ªô d·ªØ li·ªáu t·ª´ file `.h5` ra ƒë·ªÉ t·∫°o dataset hu·∫•n luy·ªán m√¥ h√¨nh. üìÇ"
      ],
      "metadata": {
        "id": "l3LqZ_B4IFIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K·∫øt qu·∫£\n",
        "Th·ªùi gian hu·∫•n luy·ªán: **5h 22m 28.845990s**\n",
        "\n",
        "N√≥i chung l√† r·∫•t l√¢u =))\n",
        "\n",
        "Nh∆∞ng k·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c kh√¥ng ·ªïn l·∫Øm, m√¨nh th·∫•y kh√° l√† t·ªá, c√≥ th·ªÉ do vi·ªác tƒÉng chi·ªÅu d·ªØ li·ªáu l√™n r·∫•t nhi·ªÅu (768) nh∆∞ng m√¥ h√¨nh l·∫°i kh√¥ng th·ªÉ quan  s√°t h·∫øt ƒë∆∞·ª£c d·∫´n ƒë·∫øn hi·ªáu qu·∫£ t·ªá, m·ªçi ng∆∞·ªùi c√≥ g√≥p √Ω c·∫£i ti·∫øn n√†o th√¨ c√≥ th·ªÉ th·ª≠ ho·∫∑c li√™n l·∫°c v·ªõi m√¨nh nha. M√¨nh c·∫£m ∆°n nha."
      ],
      "metadata": {
        "id": "RVz-QLtfH-_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![model_hf](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/model_hf.png?raw=true)"
      ],
      "metadata": {
        "id": "jzrrB3bvLiXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h5py -q"
      ],
      "metadata": {
        "id": "16UM_KCvBBuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import timedelta\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "import h5py\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "hEGCtmvG1ku6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure TensorFlow to allow memory growth\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "A19ScLwo7cWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "K.clear_session()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "wiIlOB9X2L8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# T·∫£i pretrained embedding t·ª´ Hugging Face\n",
        "model_name = \"bert-base-uncased\"  # Pretrained model name\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "pretrained_model = TFAutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Freeze pretrained embedding ƒë·ªÉ kh√¥ng train l·∫°i\n",
        "pretrained_model.trainable = False\n"
      ],
      "metadata": {
        "id": "nqbdsRXUpFu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 64  # V√¨ k√≠ch th∆∞·ªõc qu√° l·ªõn d·∫´n ƒë·∫øn tr√†n ram n√™n m√¨nh bu·ªôc gi·∫£m t·ª´ 100 xu·ªëng c√≤n 64.\n",
        "batch_size = 16   # M√¨nh gi·∫£m lu√¥n batch_size ƒë·ªÉ l∆∞·ª£ng th√¥ng tin x·ª≠ l√Ω m·ªôt l·∫ßn kh√¥ng b·ªã tr√†n ram."
      ],
      "metadata": {
        "id": "Y8Q2XLXm7eMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_save_embeddings(texts, h5_file_path, dataset_name):\n",
        "    \"\"\"\n",
        "    X·ª≠ l√Ω vƒÉn b·∫£n theo batch, chuy·ªÉn ƒë·ªïi ch√∫ng th√†nh embeddings v√† l∆∞u v√†o t·ªáp HDF5.\n",
        "\n",
        "    Args:\n",
        "        texts (list): Danh s√°ch c√°c vƒÉn b·∫£n c·∫ßn x·ª≠ l√Ω.\n",
        "        h5_file_path (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn t·ªáp HDF5 ƒë·ªÉ l∆∞u embeddings.\n",
        "        dataset_name (str): T√™n dataset trong t·ªáp HDF5.\n",
        "    \"\"\"\n",
        "    num_samples = len(texts)\n",
        "    with h5py.File(h5_file_path, 'w') as h5f:\n",
        "        # X√°c ƒë·ªãnh chi·ªÅu c·ªßa embedding t·ª´ m√¥ h√¨nh pretrained\n",
        "        embedding_dim = pretrained_model.config.hidden_size  # 768 cho BERT-base\n",
        "        # T·∫°o dataset v·ªõi k√≠ch th∆∞·ªõc ph√π h·ª£p\n",
        "        h5f.create_dataset(\n",
        "            dataset_name,\n",
        "            shape=(num_samples, max_length, embedding_dim),\n",
        "            dtype='float32'\n",
        "        )\n",
        "\n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "            # Tokenize batch\n",
        "            inputs = tokenizer(\n",
        "                batch_texts,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=max_length,\n",
        "                return_tensors=\"tf\"\n",
        "            )\n",
        "            # L·∫•y embeddings t·ª´ m√¥ h√¨nh pretrained\n",
        "            outputs = pretrained_model(**inputs)\n",
        "            # L∆∞u to√†n b·ªô sequence embeddings (kh√¥ng gi·∫£m chi·ªÅu)\n",
        "            batch_embeddings = outputs.last_hidden_state.numpy()  # Shape: (batch_size, max_length, embedding_dim)\n",
        "            # L∆∞u v√†o HDF5\n",
        "            h5f[dataset_name][i:i + batch_size] = batch_embeddings\n",
        "            # Gi·∫£i ph√≥ng b·ªô nh·ªõ\n",
        "            del batch_texts, inputs, outputs, batch_embeddings\n",
        "            gc.collect()\n",
        "\n",
        "            # In ti·∫øn ƒë·ªô\n",
        "            if (i // batch_size) % 100 == 0:\n",
        "                print(f\"ƒê√£ x·ª≠ l√Ω {i} / {num_samples} m·∫´u\")\n",
        "\n",
        "# X·ª≠ l√Ω v√† l∆∞u embeddings cho d·ªØ li·ªáu hu·∫•n luy·ªán\n",
        "preprocess_and_save_embeddings(\n",
        "    texts=train_sequences,\n",
        "    h5_file_path='train_embeddings.h5',\n",
        "    dataset_name='train'\n",
        ")\n",
        "\n",
        "# X·ª≠ l√Ω v√† l∆∞u embeddings cho d·ªØ li·ªáu ki·ªÉm tra\n",
        "preprocess_and_save_embeddings(\n",
        "    texts=test_sequences,\n",
        "    h5_file_path='test_embeddings.h5',\n",
        "    dataset_name='test'\n",
        ")"
      ],
      "metadata": {
        "id": "FTk1kucw28yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HDF5DatasetGenerator:\n",
        "    def __init__(self, h5_file_path, dataset_name, labels, batch_size):\n",
        "        self.h5_file_path = h5_file_path\n",
        "        self.dataset_name = dataset_name\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.h5f = h5py.File(h5_file_path, 'r')  # M·ªü t·ªáp HDF5 ·ªü ch·∫ø ƒë·ªô ƒë·ªçc\n",
        "        self.dataset = self.h5f[dataset_name]\n",
        "        self.num_samples = self.dataset.shape[0]\n",
        "\n",
        "    def __del__(self):\n",
        "        self.h5f.close()  # ƒê√≥ng t·ªáp HDF5 khi ƒë·ªëi t∆∞·ª£ng b·ªã h·ªßy\n",
        "\n",
        "    def generator(self):\n",
        "        for i in range(0, self.num_samples, self.batch_size):\n",
        "            batch_embeddings = self.dataset[i:i + self.batch_size]\n",
        "            batch_labels = self.labels[i:i + self.batch_size]\n",
        "            yield batch_embeddings, batch_labels"
      ],
      "metadata": {
        "id": "q4hjnspRLun3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tf_dataset(h5_file_path, dataset_name, labels, batch_size):\n",
        "    \"\"\"\n",
        "    T·∫°o TensorFlow Dataset t·ª´ embeddings ƒë∆∞·ª£c l∆∞u trong t·ªáp HDF5.\n",
        "\n",
        "    Args:\n",
        "        h5_file_path (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn t·ªáp HDF5 ch·ª©a embeddings.\n",
        "        dataset_name (str): T√™n dataset trong t·ªáp HDF5.\n",
        "        labels (numpy.array): M·∫£ng nh√£n t∆∞∆°ng ·ª©ng v·ªõi embeddings.\n",
        "        batch_size (int): K√≠ch th∆∞·ªõc batch.\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset: ƒê·ªëi t∆∞·ª£ng TensorFlow Dataset.\n",
        "    \"\"\"\n",
        "    dataset_generator = HDF5DatasetGenerator(h5_file_path, dataset_name, labels, batch_size)\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        dataset_generator.generator,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(None, max_length, pretrained_model.config.hidden_size), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "# Chuy·ªÉn ƒë·ªïi nh√£n th√†nh numpy arrays\n",
        "train_labels = np.array(train_labels)  # ƒê·∫£m b·∫£o nh√£n ·ªü ƒë·ªãnh d·∫°ng numpy array\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "# T·∫°o TensorFlow Datasets\n",
        "train_dataset = create_tf_dataset(\n",
        "    h5_file_path='train_embeddings.h5',\n",
        "    dataset_name='train',\n",
        "    labels=train_labels,\n",
        "    batch_size=batch_size  # B·∫°n c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh d·ª±a tr√™n gi·ªõi h·∫°n b·ªô nh·ªõ\n",
        ")\n",
        "\n",
        "test_dataset = create_tf_dataset(\n",
        "    h5_file_path='test_embeddings.h5',\n",
        "    dataset_name='test',\n",
        "    labels=test_labels,\n",
        "    batch_size=batch_size  # B·∫°n c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh d·ª±a tr√™n gi·ªõi h·∫°n b·ªô nh·ªõ\n",
        ")\n",
        "\n",
        "\n",
        "# T·ªëi ∆∞u h√≥a hi·ªáu su·∫•t c·ªßa dataset\n",
        "train_dataset = train_dataset.shuffle(buffer_size=10000).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "6bKvOOeaBb2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/GPU:0'):\n",
        "    # ƒê·ªãnh nghƒ©a v√† compile m√¥ h√¨nh ·ªü ƒë√¢y\n",
        "    model_hf = Sequential([\n",
        "        Input(shape=(max_length, pretrained_model.config.hidden_size)),\n",
        "        Bidirectional(LSTM(pretrained_model.config.hidden_size, return_sequences=True)),\n",
        "        Bidirectional(LSTM(pretrained_model.config.hidden_size)),\n",
        "        Dense(24, activation=\"relu\"),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "\n",
        "    # Compile m√¥ h√¨nh\n",
        "    adam = Adam(learning_rate=1e-4)\n",
        "    model_hf.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=adam,\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "\n",
        "# Hi·ªÉn th·ªã ki·∫øn tr√∫c m√¥ h√¨nh\n",
        "model_hf.summary()"
      ],
      "metadata": {
        "id": "mlxBMKirBkOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in model_hf.layers:\n",
        "    for var in layer.trainable_variables:\n",
        "        print(f\"L·ªõp: {layer.name}, Bi·∫øn: {var.name}, Thi·∫øt b·ªã: {var.device}\")\n"
      ],
      "metadata": {
        "id": "wD1YkFWkQR7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "start_time = time.time()\n",
        "history_hf = model_hf.fit(train_dataset, epochs=epochs, validation_data=test_dataset)\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "id": "tMPkO5vdC2Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_time = str(timedelta(seconds=end_time - start_time))\n",
        "print(f\"Th·ªùi gian hu·∫•n luy·ªán: {training_time}\")"
      ],
      "metadata": {
        "id": "If13J_qRrgca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# V·∫Ω bi·ªÉu ƒë·ªì qu√° tr√¨nh hu·∫•n luy·ªán\n",
        "train_acc = history_hf.history['accuracy']\n",
        "val_acc = history_hf.history['val_accuracy']\n",
        "train_loss = history_hf.history['loss']\n",
        "val_loss = history_hf.history['val_loss']\n",
        "\n",
        "fig,axs = plt.subplots(1, 2, figsize=(12,4))\n",
        "axs = axs.flatten()\n",
        "axs[0].plot(train_acc, label='train')\n",
        "axs[0].plot(val_acc, label='val')\n",
        "axs[0].set_title('Accuracy')\n",
        "axs[0].legend()\n",
        "\n",
        "axs[1].plot(train_loss, label='train')\n",
        "axs[1].plot(val_loss, label='val')\n",
        "axs[1].set_title('Loss')\n",
        "axs[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "04O-sD9GrlNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ú® Summary  \n",
        "###  T·ªïng k·∫øt ch∆∞∆°ng 7.  \n",
        "\n",
        "V·∫≠y l√† m·ªçi ng∆∞·ªùi ƒë√£ c√≥ th·ªÉ ƒëi ƒë·∫øn cu·ªëi v√† ho√†n th√†nh h·∫øt ch∆∞∆°ng 7 r·ªìi. C·∫£m ∆°n m·ªçi ng∆∞·ªùi ƒë√£ theo d√µi r·∫•t nhi·ªÅu. üôå V·ªÅ ph·∫ßn n·ªôi dung ch√∫ng ta ƒë√£ h·ªçc trong ch∆∞∆°ng n√†y bao g·ªìm:  \n",
        "\n",
        "- M·∫°ng h·ªìi quy l√† g√¨? Th·∫ø n√†o l√† m·∫°ng h·ªìi quy? C∆° ch·∫ø ho·∫°t ƒë·ªông c·ªßa ch√∫ng.  \n",
        "- B·ªô nh·ªõ ng·∫Øn h·∫°n d√†i **(LSTM)**.  \n",
        "- C√°ch √°p d·ª•ng **LSTM** v√†o m√¥ h√¨nh ƒë·ªÉ tƒÉng ƒë·ªô hi·ªáu qu·∫£.  \n",
        "- Ti·∫øp c·∫≠n v·ªõi t√¨nh tr·∫°ng **Overfit** trong m·∫°ng h·ªìi quy **RNN** v√† c√°ch ƒë·ªÉ t·ªëi ∆∞u ch√∫ng. üåü"
      ],
      "metadata": {
        "id": "TYZAfv4jwU5I"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FQHyC-A4Gqg_",
        "6uSuCn9G0Mu-",
        "hrjvRgqGK280",
        "pLP75bnQDM6b"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}