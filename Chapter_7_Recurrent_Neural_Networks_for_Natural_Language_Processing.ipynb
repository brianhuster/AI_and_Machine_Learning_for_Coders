{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/Chapter_7_Recurrent_Neural_Networks_for_Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQHyC-A4Gqg_"
      },
      "source": [
        "# Chapter 7: Recurrent Neural Networks for Natural Language Processing 🌐  \n",
        "\n",
        "Chào mọi người nha, tụi mình lại gặp nhau rồi 🌻.  \n",
        "Ở chương này, tụi mình sẽ cùng tìm hiểu về việc sử dụng các **mạng hồi quy (Recurrent Neural Networks - RNNs)** để xử lý các bài toán về **ngôn ngữ tự nhiên (Natural Language Processing - NLP)** nha. 😊"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVRrBIfdtFiC"
      },
      "source": [
        "![RNN image](https://www.simplilearn.com/ice9/free_resources_article_thumb/Fully_connected_Recurrent_Neural_Network.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esbtLHrzI3m-"
      },
      "source": [
        "### Trước khi bước vào chương mới, tụi mình cùng ôn lại 2 chương trước đó nha 📖  \n",
        "\n",
        "Ở **Chương 5**, mọi người đã được tìm hiểu về việc **mã hóa các câu** thành dạng **chuỗi** hay **tensor** của số thông qua cơ chế **Tokenizer**.  \n",
        "\n",
        "Ở **Chương 6**, chúng ta lại tìm hiểu thêm về **vector biểu diễn** hay **cơ chế embedding**, thông qua việc đưa các từ lên **không gian cao hơn** giúp **truy xuất được nhiều ngữ nghĩa hơn**, đồng thời xây dựng **mối liên kết giữa các vectors** đại diện từ có **nhóm ngữ nghĩa tương đương nhau**.  \n",
        "\n",
        "Tuy nhiên, bạn cũng nhận thấy rằng về cơ bản thì các phương pháp trên đều hoạt động giống như **một túi chứa các từ**, không phân biệt về **thứ tự của chữ**. Trong khi đó, trong **ngôn ngữ**, **thứ tự** của các từ lại đóng vai trò vô cùng **quan trọng về mặt ngữ nghĩa**.  \n",
        "\n",
        "Ví dụ:  \n",
        "- Từ **\"blue\"** đứng một mình có vẻ khá thiếu nghĩa, nó có thể là **buồn** hoặc chỉ **màu xanh**, cũng giống như từ **\"sky\"** vậy. Nhưng khi kết hợp lại, **\"blue sky\"** sẽ có nghĩa là **\"bầu trời xanh\"**, trong đó **tính từ bổ nghĩa cho danh từ**.  \n",
        "- Các từ khác như **\"writing desk\"**, **\"rain cloud\"** cũng tương tự.  \n",
        "- Hoặc ví dụ như từ **\"brain\"** và **\"storm\"**, một từ nghĩa là **não**, một từ là **bão**, nhưng khi kết hợp thành **\"brainstorm\"**, chúng lại có nghĩa là **động não**.  \n",
        "\n",
        "Một ví dụ khác rõ hơn là trong các cụm **idioms**, khi ghép các từ lại, chúng mang **nghĩa bóng** thay vì **nghĩa đen**.  \n",
        "\n",
        "---\n",
        "\n",
        "Các mô hình cơ bản, thông thường trước đó **không quan tâm đến thứ tự từ**, chúng chỉ biết rằng chúng có những từ đó và đưa lên **không gian chiều cao hơn**, sau đó đến các lớp tuyến tính để tính toán. Điều này **không mang tính ý nghĩa cao** khi xét theo chuỗi.  \n",
        "\n",
        "Mọi người có thể liên tưởng đến trường hợp **antigram** mà tụi mình đã tìm hiểu trong **chương 4** – ở đây cũng tương tự, nhưng là ở cấp từ thay vì ký tự.  \n",
        "\n",
        "---\n",
        "\n",
        "Chính vì những **mặt hạn chế ngữ nghĩa** này mà ta cần một phương pháp tiếp cận mới để giải quyết vấn đề. Mô hình cần có khả năng **nhìn lại quá khứ**, **nhớ lại được những từ phía trước**. Đây chính là khởi đầu cho **cơ chế hồi quy, tái phát, lặp lại hay nhớ lại (recurrence)** trong kiến trúc của các mô hình NLP sau này. 🔄✨  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGjmONzfrpOA"
      },
      "source": [
        "![RNN image](https://research.aimultiple.com/wp-content/uploads/2021/08/rnn-text.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwTnA1Joryzi"
      },
      "source": [
        "Trong **chương 7** này, mọi người sẽ tìm hiểu về các **phương pháp để hiện thực hóa cơ chế hồi quy** nha. 🌟  \n",
        "\n",
        "Tụi mình sẽ **đào sâu hơn về thông tin ngữ nghĩa của các chuỗi**, cách mà chúng được học cũng như tìm hiểu về **một loại kiến trúc mô hình mới** có khả năng hiểu tốt hơn các văn bản.  \n",
        "\n",
        "Và đó chính là **Mạng nơ-ron hồi quy - Recurrent Neural Network (RNN)**. 🔄🧠  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uSuCn9G0Mu-"
      },
      "source": [
        "# The Basic of Recurrence 🌀  \n",
        "### Khái niệm cơ bản về cơ chế hồi quy, tái phát, nhớ lại - Recurrence 🎯  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhVlT-xs3K7N"
      },
      "source": [
        "Trước khi tụi mình đi sâu vào hiểu cách mà **cơ chế hồi quy (Recurrence)** hoạt động, tụi mình sẽ nhìn lại về những **giới hạn** của các mô hình trước đó một lần nữa nha. 🧠  \n",
        "\n",
        "Về cơ bản thì mọi thứ vẫn hoạt động theo kiểu: bạn có **dữ liệu** này, bạn có **nhãn** của chúng này, và rồi chúng ta đẩy tất cả vào mô hình, ép chúng học và tìm ra được **các quy luật bên trong**. 📊 Sau đó, chúng ta sẽ sử dụng những quy luật có được, hay còn được gọi là trọng số của mô hình, để dự đoán các dữ liệu sau này. 🔮  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QORUlxs6bYIU"
      },
      "source": [
        "![c7_model](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/c7_model.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndy_g_Sh4w7D"
      },
      "source": [
        "Tuy nhiên, mọi người có thể thấy rằng dữ liệu được đưa vào và xử lý một cách tổng thể, không có sự cố gắng tìm kiếm các **chi tiết liên quan đến trình tự xuất hiện của dữ liệu**. 🤔 Điều này phần nào làm cho mô hình trở nên **\"ngu\"** hơn trong việc nắm bắt ngữ cảnh và mối liên kết giữa các phần tử trong dữ liệu văn bản. 😅"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPI29jNrb2Do"
      },
      "source": [
        "![c7_stupid](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/c7_stupid.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp7G0v5a-ooN"
      },
      "source": [
        "Mình sẽ lấy ví dụ một số trường hợp cụ thể về sự **\"ngu\"** của nó nha. 😅  \n",
        "\n",
        "Nếu bạn đưa 2 câu như sau:  \n",
        "> Today I am **blue**, because the **sky** is gray. 🌥️  \n",
        "\n",
        "and  \n",
        "\n",
        "> Today I am happy, and there's a beautiful **blue sky**. 🌞  \n",
        "\n",
        "Đối với chúng ta sẽ hiểu ngay rằng từ **\"blue\"** trong 2 câu này có ý nghĩa hoàn toàn khác nhau:  \n",
        "- Ở câu đầu, **\"blue\"** có nghĩa là **\"buồn\"**. 😔  \n",
        "- Ở câu sau, **\"blue\"** lại mang ý nghĩa là **\"màu xanh\"**. 💙  \n",
        "\n",
        "Tuy nhiên, với mô hình thì khác. 🤖 Chúng chỉ đơn giản hiểu rằng cả 2 câu trên đều chứa từ **\"blue\"** và **\"sky\"**, do đó chúng cho rằng 2 câu này tương tự và có ý nghĩa gần giống nhau trên không gian vector biểu diễn. 😵‍💫  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jpHXwLZAQ-l"
      },
      "source": [
        "> **Vậy làm thế nào để chúng có thể khôn ra được?** 🤔  \n",
        "\n",
        "Đó là cho chúng hiểu và nắm bắt được **thứ tự các từ kết hợp với nhau**. Mà để làm được việc này, chúng ta cần mô hình **nhớ lại được kiến thức trước đó**. 🧠 Đây chính là nền tảng chính cho việc xử lý các dữ liệu có cấu trúc dạng **chuỗi**.  \n",
        "\n",
        "Chắc mọi người ai cũng gặp qua cụm **\"hồi quy tiền kiếp\"** rồi ha. Nói đơn giản thì hồi quy có nghĩa là **\"nhớ lại\"** nhưng mà nói cho nó sang hơn =))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPdxzRXrgSwX"
      },
      "source": [
        "### Bây giờ tụi mình mới chính thức đi vào tìm hiểu **bản chất** của cơ chế **hồi quy**, qua đó hiểu được cách thức hoạt động của mạng **RNN** nha. 🔍✨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgRS_fMhB9Xv"
      },
      "source": [
        "Tụi mình sẽ lấy ví dụ đơn giản về **dãy Fibonacci nha**. 🌟 Đây là một dạng cơ bản của chuỗi tuần tự khi mà **số phía sau bằng tổng của hai số trước**, do đó chúng có **mối quan hệ mật thiết** về mặt **trình tự theo thời gian**. 🕰️➕"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHz8PwZXCKS_"
      },
      "source": [
        "![fibonacci numbers](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/fibonacci_number.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMQGq-wMDA0V"
      },
      "source": [
        "![image.png](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/fibonacci_algorithms.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp1Ubf0aGbTr"
      },
      "source": [
        "Ý tưởng của mạng hồi quy - **RNN** cũng sẽ hoạt động tương tự như vậy nha. 💡  \n",
        "\n",
        "Ở đây mình định nghĩa mỗi **ô cộng màu cam** sẽ đại diện cho **phép tính toán tại 1 thời điểm hay mỗi trạng thái thời gian**. Vậy là ta có tổng cộng **3 trạng thái thời gian**. ⏳➕  \n",
        "\n",
        "***Mọi người nhớ kĩ giúp mình đoạn này nha!*** 🧠  \n",
        "\n",
        "- Ở **trạng thái thứ nhất**, thuật toán sẽ lấy **dữ liệu khởi đầu** là **1** và **2**, sau đó tính ra **output của trạng thái 1** là **3**.  \n",
        "- Tiếp đến, sang phép tính ở **trạng thái thứ 2**, thuật toán lấy **dữ liệu tiếp theo là số 2** và kết hợp với **output ở trạng thái 1 (là 3)**. Kết quả **output ở trạng thái 2** này là **5**.  \n",
        "- Tương tự như vậy, ở **trạng thái thứ 3**, thuật toán sẽ tiếp tục tính toán dựa trên dữ liệu và trạng thái trước đó.  \n",
        "\n",
        "Qua quá trình trên, khi đến trạng thái cuối cùng, **kết quả đầu ra của chúng ta sẽ có sự đóng góp ý nghĩa của các kết quả, trạng thái trước đó** trong chuỗi. Từ đó, mô hình có thể nắm bắt được **ý nghĩa của toàn bộ chuỗi**. 🌐  \n",
        "\n",
        "Dưới đây là **hình vẽ thu nhỏ** mô tả khái quát **quá trình hoạt động của nơ-ron hồi quy**:  \n",
        "✨ *(Bạn có thể hình dung các ô nối tiếp nhau, truyền thông tin qua lại giữa các trạng thái)*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOYTbAiJNMdT"
      },
      "source": [
        "![architecture](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/architecture_rnn.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNl9ClPRIiPZ"
      },
      "source": [
        "**Hình này mô tả tổng quát về cách mà một nơ-ron hồi quy hoạt động.** 🌀  \n",
        "\n",
        "- Ta gọi **F** là **quá trình tính toán của mô hình tại mỗi thời điểm**.  \n",
        "- **t** đại diện cho **thời điểm xảy ra phép toán**.  \n",
        "- **x** là **dữ liệu đầu vào** và **y** là **kết quả đầu ra**.  \n",
        "\n",
        "🌟 **Tại mỗi thời điểm t**, mô hình sẽ:  \n",
        "1. Nhận **dữ liệu đầu vào** là **x(t)**.  \n",
        "2. Thực hiện **phép tính toán thông qua F** để tính ra **kết quả đầu ra** là **y(t)**.  \n",
        "3. Kết quả **y(t)** này sẽ được **đưa vào tổng hợp** với dữ liệu ở **thời điểm tiếp theo t+1**.  \n",
        "\n",
        "💡 **Quy trình này lặp lại liên tục**, dữ liệu trong quá khứ sẽ được tổng hợp dần vào cho đến khi hết chuỗi, đảm bảo mô hình có khả năng **ghi nhớ ngữ cảnh và mối liên hệ giữa các phần tử trong chuỗi**.  \n",
        "\n",
        "🎯 Đây chính là **nền tảng quan trọng** của cơ chế hồi quy trong **RNN**, giúp mô hình hiểu được trình tự và mối liên hệ giữa các từ trong bài toán ngôn ngữ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK4K6L2KebEV"
      },
      "source": [
        "![detail_neural](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/detail_architecture_rnn.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHp1uPQdeGyn"
      },
      "source": [
        "💡 **Phân tích chi tiết hoạt động của nơ-ron hồi quy qua từng trạng thái thời gian:**  \n",
        "\n",
        "1️⃣ **Thời điểm 0 (x₀, y₀):**  \n",
        "   - Nơ-ron thực hiện phép tính toán **F** với dữ liệu đầu vào **x₀**.  \n",
        "   - Trả về kết quả đầu ra **y₀**.  \n",
        "   - **y₀** sẽ được đưa vào tổng hợp cùng với dữ liệu tại thời điểm tiếp theo.\n",
        "\n",
        "2️⃣ **Thời điểm 1 (x₁, y₁):**  \n",
        "   - Nơ-ron tiếp tục thực hiện phép tính toán **F**, lần này với đầu vào là **x₁** và kết quả từ thời điểm trước đó **y₀**.  \n",
        "   - Trả về kết quả **y₁**, đại diện cho trạng thái ở thời điểm này.  \n",
        "   - Kết quả **y₁** lại tiếp tục được đưa vào tổng hợp cho thời điểm tiếp theo.\n",
        "\n",
        "3️⃣ **Thời điểm 2 (x₂, y₂):**  \n",
        "   - Tương tự, **F** được tính với đầu vào **x₂** và **y₁** từ trạng thái trước.  \n",
        "   - Trả về kết quả **y₂**, đại diện cho trạng thái tại thời điểm này.  \n",
        "\n",
        "🌟 **Quá trình này tiếp tục lặp lại cho đến khi hết chuỗi dữ liệu**, đảm bảo rằng **kết quả ở mỗi thời điểm đều có sự tổng hợp từ các trạng thái trước đó**.  \n",
        "\n",
        "🎯 **Ý nghĩa:**  \n",
        "- Nơ-ron hồi quy không chỉ xử lý dữ liệu hiện tại mà còn **ghi nhớ và tích hợp thông tin từ các trạng thái trước đó**.  \n",
        "- Điều này giúp mô hình hiểu được **bối cảnh và trình tự dữ liệu**, làm tăng khả năng phân tích và dự đoán trong các bài toán liên quan đến chuỗi. 🚀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDKQZ32oKTPF"
      },
      "source": [
        "*Lưu ý: Mình chỉ dùng dãy số Fibonacci để mọi người có thể dễ hiểu và mường tượng cách mà kiến trúc mạng hồi quy - RNN hoạt động. Còn về chi tiết thì phép toán mà mô hình thực hiện bên trong sẽ phức tạp hơn nhiều.* 😊"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrjvRgqGK280"
      },
      "source": [
        "# Extending Recurrence for Language  \n",
        "### 🌐 Tụi mình sẽ mở rộng tìm hiểu về tính hồi quy trong xử lý ngôn ngữ tự nhiên nha. 💬  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA7oi4bQpf2_"
      },
      "source": [
        "Ở phần trước, tụi mình đã tìm hiểu về cơ chế hoạt động của **RNN**, cách mà chúng có thể nắm bắt được ngữ cảnh xuyên suốt một câu hay chuỗi rồi ha mọi người. 💡 Điều này thật sự là một bước đột phá và cải tiến ở thời điểm đó. Các kiến trúc liên quan đến **RNN** sẽ được ứng dụng và sử dụng nhiều ở các chương tiếp theo. 📘\n",
        "\n",
        "Tuy nhiên, tại thời điểm đó, người ta cũng nhận thấy một **nhược điểm chí tử** với kiến trúc **RNN cơ bản**:  \n",
        "\n",
        "➡️ **Dữ liệu về ngữ cảnh sẽ bị mất dần theo thời gian.**\n",
        "\n",
        "> **Vậy tại sao lại có điều này xảy ra?** 🤔\n",
        "\n",
        "Tụi mình sẽ phân tích lại cách thức hoạt động của **RNN** thông qua ví dụ về dãy **Fibonacci** nha.  \n",
        "Chúng ta biết rằng kết quả của một số phía sau sẽ bằng **hai số trước cộng lại**, do đó giá trị của các số phía trước sẽ có **tác động và ảnh hưởng** đến số phía sau. 💬"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43uZVOGWwtlM"
      },
      "source": [
        "![image.png](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/fibonacci_algorithms.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ3gxvGpwt7P"
      },
      "source": [
        "**Số 1 và số 2 sẽ có tác động mạnh, ảnh hưởng trực tiếp tới kết quả đầu ra ở thời điểm đầu tiên là 3**. Tương tự như vậy, **số 2 và số 3 sẽ có tác động mạnh đến kết quả đầu ra là 5 ở thời điểm thứ 2**. Tuy nhiên, **tác động của số 1 và số 2 ban đầu đến số 5 ở thời điểm 2 đã yếu hơn**, bởi giờ đây tổng của chúng trên 5 **chỉ còn 3/5**. Tương tự như vậy, chuỗi càng dài về sau thì tác động của các số ở đầu sẽ bị yếu đi cho đến khi bạn không còn nhận thấy được sự đóng góp, ảnh hưởng của nó nữa. Ví dụ, khi đến **số Fibonacci thứ 8 là 89 thì tác động của 1 và 2 chỉ còn là 3/89**. 🔄\n",
        "\n",
        "Qua đó, mọi người thấy rằng với **chuỗi càng dài thì khả năng mất mát dữ liệu, ngữ cảnh càng lớn**, và mô hình sẽ dần **quên đi các dữ liệu trước đó**. 😔  \n",
        "\n",
        "---\n",
        "\n",
        "**Ví dụ cụ thể trong các câu văn** ha. Ở đây, token **< ? >** đại diện cho từ không biết trước, nó có thể là bất kỳ từ nào.  \n",
        "\n",
        "> **Today has a beautiful blue < ? >.**  \n",
        "\n",
        "Thì ở đây, từ **\"blue\"** sẽ có **tác động, ảnh hưởng lớn nhất đến từ tiếp theo**, do đó mô hình có thể đoán từ tiếp theo là **\"sky\"**. 🌤️  \n",
        "\n",
        "Tuy nhiên, với những câu dài hơn như:  \n",
        "\n",
        "> **I lived in Ireland, so in high school I had to learn how to speak < ? >.**  \n",
        "\n",
        "Ở đây, mọi người có thể đoán từ ẩn đó là **một ngôn ngữ** gì đó, và thông thường chúng ta sẽ xác định chính thông qua từ **\"speak\", \"high school\" và \"Ireland\"**, đúng không? 🗣️ Tuy nhiên, **hai từ \"high school\" và \"Ireland\" lại ở quá xa so với từ ẩn**, khiến mức độ ảnh hưởng của chúng đến từ ẩn **không nhiều**. Do đó, mô hình không nhận định và dự đoán được từ tiếp theo chuẩn xác, kết quả có thể sẽ sai nhiều hơn.  \n",
        "\n",
        "Ví dụ, từ ẩn đúng ra phải là **\"Gaelic\"**, tuy nhiên lại bị dự đoán thành **\"Ireland\"** hoặc một từ bất kỳ khác. 🚩"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cImWsR-1zQVJ"
      },
      "source": [
        "### Vậy vấn đề bây giờ của chúng là làm cách nào khắc phục vấn đề mất mát ngữ cảnh trên? 🤔\n",
        "\n",
        "Để làm được việc này, chúng ta cần phải **gia tăng khả năng nhớ, bộ nhớ ngắn hạn của RNN** để có thể nhớ được lâu hơn với các chuỗi dài hơn. Và đó cũng là ý tưởng khởi đầu cho sự ra đời của một kiến trúc mới, nâng cấp hơn mang tên bộ nhớ ngắn hạn dài - **Long Short-term Memory (LSTM)**. 🧠💡\n",
        "\n",
        "---\n",
        "\n",
        "*P/s: Thiệt sự là mọi người nên đọc tên em nó bằng tiếng Anh nha, đọc sang tiếng Việt thấy nó kì kì quá =)) , mình cũng không biết dịch sao cho hợp lý. 🙃*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTrqo8T84Kds"
      },
      "source": [
        "### Tiếp theo, tụi mình sẽ tìm hiểu về **LSTM**. 🚀\n",
        "\n",
        "Nhưng mà theo tiêu chí của tụi mình là **không đi sâu quá về thuật toán**, nên mình sẽ tìm hiểu cách chúng hoạt động ở **cấp độ ứng dụng** thôi nha. 🛠️\n",
        "\n",
        "Nếu các bạn vẫn muốn tìm hiểu sâu hơn về **code và lý thuyết chi tiết**, có thể tham khảo thử [blog này](https://nttuan8.com/bai-14-long-short-term-memory-lstm/). 📚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0WQVJx65FA0"
      },
      "source": [
        "![lstm](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/lstm.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULw_zmlR5JIC"
      },
      "source": [
        "Mọi người hiểu đơn giản là **LSTM** sẽ nâng cấp **RNN cơ bản** lên bằng cách thêm vào một cái gọi là **ô trạng thái (Cell state)**. 🌟  \n",
        "\n",
        "Điều này cho phép ngữ cảnh được **duy trì** không chỉ qua từng bước mà còn xuyên suốt cả **toàn bộ chuỗi các bước**. 🛠️  \n",
        "\n",
        "*💡 Lưu ý:* Chúng là **nơ-ron**, do đó sẽ học theo cách mà nơ-ron hoạt động, giúp đảm bảo các **ngữ cảnh quan trọng** sẽ được học dần theo thời gian. 🧠"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRouFFvE-ZWT"
      },
      "source": [
        "Một phần **quan trọng** và **cải tiến** vượt bậc của **LSTM** chính là khả năng **Bidirectional**. 🚀  \n",
        "\n",
        "Điều này cho phép mô hình **học theo cả 2 chiều** của chuỗi, tức là:  \n",
        "- **Từ trước về sau (forward)**. ⏩  \n",
        "- **Từ sau ra trước (backward)**. ⏪  \n",
        "\n",
        "Mọi người thường gọi nó là **BLSTM (Bidirectional LSTM)** nha. 🌟  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5srMz4-g-U8K"
      },
      "source": [
        "![bblstm](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/blstm.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dxX2BFN6I_E"
      },
      "source": [
        "Ở cách này, kết quả đánh giá tại mỗi bước thời gian sẽ được thực hiện **theo cả hai hướng**:  \n",
        "\n",
        "1. **Forward**: Từ **step 0 đến step n** ⏩.  \n",
        "2. **Backward**: Từ **step n đến step 0** ⏪.  \n",
        "\n",
        "Tại mỗi bước thời gian, kết quả của **y** là **sự tổng hợp** của cả hai quá trình trên. Điều này giúp mô hình **hiểu sâu hơn** và **nắm bắt ngữ cảnh tốt hơn** từ chuỗi dữ liệu.\n",
        "\n",
        "Tụi mình cùng **phân tích kỹ hơn** với biểu đồ dưới đây nha! 📊  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA7nxAAc_Fwx"
      },
      "source": [
        "![detail_blstm](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/detail_blstm.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6Oi3UZp_Ind"
      },
      "source": [
        "Ở đây, chúng ta có:  \n",
        "\n",
        "- **3 nơ-ron** tương ứng với **3 trạng thái thời gian**: **F1, F2, F3**.  \n",
        "- **Mũi tên** đại diện cho hướng đi **forward** ⏩ hoặc **backward** ⏪.  \n",
        "\n",
        "### Điểm nổi bật:  \n",
        "- **BLSTM** tổng hợp cả hai hướng (**forward** và **backward**) tại mỗi bước thời gian để tạo ra kết quả **y**.  \n",
        "- **Ô trạng thái hai chiều (Cell state)** của BLSTM giúp lưu giữ và quản lý nội dung chuỗi hoặc câu văn một cách hiệu quả hơn.  \n",
        "\n",
        "---\n",
        "\n",
        "#### Tụi mình sẽ đi vào ví dụ minh họa:  \n",
        "\n",
        "Câu văn:  \n",
        "> **I lived in Ireland, so in high school I had to learn how to speak < ? >.**  \n",
        "\n",
        "- Nhờ việc học theo **cả hai chiều**, mô hình của chúng ta có thể nhận biết được **ảnh hưởng của các từ quan trọng** như **\"Ireland\"** và **\"high school\"** tốt hơn, từ đó đoán chính xác từ ẩn là **\"Gaelic\"**.  \n",
        "- Nếu đảo ngược là, **từ ẩn là \"Ireland\"**, mô hình vẫn **dự đoán chính xác** được nhờ cơ chế **backward**.  \n",
        "\n",
        "---\n",
        "\n",
        "#### Vậy lợi ích của BLSTM là:  \n",
        "- **Hiểu ngữ cảnh sâu hơn** trong văn bản.  \n",
        "- **Khả năng dự đoán từ ẩn chính xác hơn** nhờ tổng hợp thông tin hai chiều.  \n",
        "\n",
        "---\n",
        "\n",
        "### *Lưu ý:*\n",
        "- **Tài nguyên tính toán cũng sẽ tăng gấp đôi** do phải thực hiện cả **forward** và **backward** trên chuỗi dữ liệu.  \n",
        "- Do đó mình đề xuất mọi người nên sử dụng **GPU** nha. 💻⚡  \n",
        "\n",
        "Trong các chương tiếp theo, tụi mình sẽ ứng dụng **BLSTM** rất nhiều nha. 🚀\n",
        "\n",
        "Bây giờ tụi mình sẽ tiến hành thử xây dựng một vài mô hình phân loại văn bản với **BLSTM**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLP75bnQDM6b"
      },
      "source": [
        "# Creating a Text Classifier with RNNs  \n",
        "### Tụi mình sẽ xây dựng một mô hình phân loại văn bản với RNN nha. 🌟  \n",
        "\n",
        "Ở đây tụi mình dùng lại luôn bài toán với bộ dữ liệu **Sarcam** trước đó nha. Các bước tiền xử lý dữ liệu thì vẫn như nhau. 😊  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKrEbryTvMH0"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.15 -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Keras-Preprocessing"
      ],
      "metadata": {
        "id": "4XxCbLmlzyeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cài đặt các thư viện cần thiết\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import bs4 as BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import numpy as np\n",
        "import string\n",
        "import kagglehub\n",
        "import pandas"
      ],
      "metadata": {
        "id": "2O5CWCb12Xvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGpWIBrerya_"
      },
      "outputs": [],
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"rmisra/news-headlines-dataset-for-sarcasm-detection\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCm5rOyPFgEv"
      },
      "outputs": [],
      "source": [
        "# Kiểm tra các thư mục hiện có\n",
        "import os\n",
        "for dir_name in os.listdir(path):\n",
        "  print(dir_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0uNnTl4uwkB"
      },
      "outputs": [],
      "source": [
        "dataset = pandas.read_json(path + \"/Sarcasm_Headlines_Dataset.json\", lines=True)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kAJGPX9u3HY"
      },
      "outputs": [],
      "source": [
        "sentences = dataset[\"headline\"].values\n",
        "labels = dataset[\"is_sarcastic\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wyil7CYew5gQ"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "table = str.maketrans('', '', string.punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBF1L_k6Fxrd"
      },
      "outputs": [],
      "source": [
        "# Tiến hành làm sạch văn bản\n",
        "sentences_preprocessed = []\n",
        "for s in sentences:\n",
        "  sentence = s.lower()\n",
        "  # Tiến hành xử lý các kí tự đặc biệt, phòng trường hợp người dùng viết liền chúng với các từ.\n",
        "  sentence = sentence.replace(\",\", \" , \")\n",
        "  sentence = sentence.replace(\".\", \" . \")\n",
        "  sentence = sentence.replace(\"/\", \" / \")\n",
        "  sentence = sentence.replace(\"?\", \" ? \")\n",
        "  sentence = sentence.replace(\"!\", \" ! \")\n",
        "  sentence = sentence.replace(\"-\", \" - \")\n",
        "  sentence = sentence.replace(\"(\", \" ( \")\n",
        "  sentence = sentence.replace(\")\", \" ) \")\n",
        "  sentence = sentence.replace(\"$\", \" $ \")\n",
        "  sentence = sentence.replace(\"%\", \" % \")\n",
        "  sentence = sentence.replace(\"&\", \" & \")\n",
        "\n",
        "  # Loại bỏ các thẻ HTML bằng BeautifulSoup\n",
        "  soup = BeautifulSoup.BeautifulSoup(sentence)\n",
        "  sentence = soup.get_text()\n",
        "\n",
        "  # Loại bỏ các kí tự đặc biệt\n",
        "  sentence = sentence.translate(table)\n",
        "\n",
        "  # Chuẩn hóa lại các kí tự khoảng trắng dư thừa liên tiếp thành 1 khoảng trắng\n",
        "  sentence = re.sub('\\s+', ' ', sentence)\n",
        "\n",
        "  # Tiến hành lọc và loại bỏ từ dừng\n",
        "  words = sentence.split()\n",
        "  filtered_sentence = []\n",
        "\n",
        "  for w in words:\n",
        "    if w not in stop_words:\n",
        "      filtered_sentence.append(w)\n",
        "\n",
        "  filtered_sentence = \" \".join(filtered_sentence)\n",
        "  # Thêm vào danh sách headline đã tiền xử lý\n",
        "  sentences_preprocessed.append(filtered_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jo1m60P3xTRt"
      },
      "outputs": [],
      "source": [
        "# Chia tập train test, ở đâu mình chia theo tác giả giống bài trước luôn đi.\n",
        "\n",
        "training_size = 24000\n",
        "train_sequences = sentences_preprocessed[0:training_size]\n",
        "train_labels = labels[0:training_size]\n",
        "test_sequences = sentences_preprocessed[training_size:]\n",
        "test_labels = labels[training_size:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch30upznz0cL"
      },
      "source": [
        "### Tiến hành mã hóa dữ liệu, lưu ý ở phần này sẽ khác với trước đó nha. 🛠️  \n",
        "\n",
        "Bây giờ chúng ta sẽ sử dụng bộ mã hóa với kích thước từ vựng là **20.000**, lớn hơn rất nhiều so với trước, và số chiều của các vector biểu diễn cũng được tăng lên **64**. 📊  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTyDiqt0yaG-"
      },
      "outputs": [],
      "source": [
        "# Tạo tokenizer\n",
        "vocab_size = 20000\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlUf_baTzGSp"
      },
      "outputs": [],
      "source": [
        "train_sequences_encoded = tokenizer.texts_to_sequences(train_sequences)\n",
        "test_sequences_encoded = tokenizer.texts_to_sequences(test_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCTk-6_yzWJw"
      },
      "outputs": [],
      "source": [
        "# Tiến hành padding cho chuỗi\n",
        "max_len = 100\n",
        "train_padded = pad_sequences(train_sequences_encoded, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
        "test_padded = pad_sequences(test_sequences_encoded, maxlen=max_len, padding=\"post\", truncating=\"post\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOV4u4JP0tew"
      },
      "source": [
        "# Simple Model use LSTM  \n",
        "\n",
        "### Bây giờ tụi mình sẽ tiến hành xây dựng một mô hình đơn giản với LSTM trước nha. ✨  \n",
        "\n",
        "Ở đây sẽ có một điểm đặc biệt khác với trước. Nếu như ở chương trước, dữ liệu của mọi người sau khi qua lớp **Embedding** cần phải được tổng hợp bằng lớp **Global Average Pooling** để chuyển thành vector trước khi đưa vào lớp **Dense**, nhưng ở đây chúng ta không cần vậy nữa.  \n",
        "\n",
        "Khi làm việc với các lớp **RNN** như **LSTM**, mọi người không cần phải tổng hợp vector mà có thể đưa trực tiếp đầu ra của lớp **Embedding** vào lớp **RNN** như **LSTM** luôn. 🔄  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import timedelta"
      ],
      "metadata": {
        "id": "wXlhQV3h-pGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dims = 64\n",
        "epochs = 30"
      ],
      "metadata": {
        "id": "wix2gc4b-9eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiiTKUbE13gA"
      },
      "outputs": [],
      "source": [
        "# Xây dụng mô hình\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dims),\n",
        "    Bidirectional(LSTM(embedding_dims)),\n",
        "    Dense(24, activation=\"relu\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6JYdJXM3R5D"
      },
      "source": [
        "Ở đây tác giả có chỉnh **learning_rate** xuống rất nhỏ, chỉ **0.00001**. 🔧 Điều này khiến việc hội tụ sẽ diễn ra **rất lâu**, cũng như **số lượng phép tính toán có thể tăng thêm**. Vì vậy, khuyến khích mọi người sử dụng **GPU** để tăng tốc độ huấn luyện nha. 🚀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiuV-NRuxiAl"
      },
      "outputs": [],
      "source": [
        "adam = Adam(learning_rate=0.00001, beta_1=0.9, beta_2=0.999)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-DDfXsG29--"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rudg8CNeqeYh"
      },
      "source": [
        "Tụi mình sẽ tiến hành huấn luyện trong khoảng **30 epochs** thôi nha ⏳ vì thời gian xử lý của những mô hình này khá lâu. Nếu tình trạng **overfiting** diễn ra ngay từ một số epochs đầu, thì những epochs sau đều trở nên **vô nghĩa**. Khi đó, việc để số lượng epochs lớn chỉ làm **tốn thêm thời gian và tài nguyên** thôi. 🚦"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcoimprX21bV"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "history = model.fit(train_padded, train_labels, epochs=epochs, validation_data=(test_padded, test_labels))\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8DBqDhk35qj"
      },
      "outputs": [],
      "source": [
        "print(f\"Thời gian huấn luyện: {str(timedelta(seconds=end_time - start_time))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rppzbDyS8aCl"
      },
      "outputs": [],
      "source": [
        "# Vẽ biểu đồ đánh giá quá trình\n",
        "train_acc = history.history[\"accuracy\"]\n",
        "val_acc = history.history[\"val_accuracy\"]\n",
        "train_loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
        "axs = axs.flatten()\n",
        "axs[0].plot(train_acc, label=\"train\")\n",
        "axs[0].plot(val_acc, label=\"val\")\n",
        "axs[0].set_title(\"Accuracy\")\n",
        "axs[0].legend()\n",
        "axs[1].plot(train_loss, label=\"train\")\n",
        "axs[1].plot(val_loss, label=\"val\")\n",
        "axs[1].set_title(\"Loss\")\n",
        "axs[1].legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgWENRS2KOj7"
      },
      "outputs": [],
      "source": [
        "# Độ hiệu quả của mô hình\n",
        "eval = model.evaluate(test_padded, test_labels)\n",
        "print(f\"Accuracy: \", eval[1])\n",
        "print(f\"Loss: \", eval[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9WpPlod9gn2"
      },
      "source": [
        "Đây là kết quả ở lần chạy của mình nha."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![model](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/model.png?raw=true)"
      ],
      "metadata": {
        "id": "aHE89EpGHEFW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6LuQl499qpZ"
      },
      "source": [
        "Mọi người có thể thấy mô hình chứng minh được độ hiệu quả của mình ở **30 epochs** đầu thông qua kết quả kiểm tra trên tập **val**. 📉 Đường **loss** giảm xuống dần và có phần **thấp hơn** tí so với biểu đồ ở **chương 6**, mặc dù kích thước từ vựng của chúng ta lên đến tận **20.000** và số chiều **embedding** là **64**. 🧩\n",
        "\n",
        "Tuy nhiên, mọi người vẫn có thể thấy được tình trạng **Overfit** đang có xu hướng xảy ra. 🚨 Đường **loss val** có dấu hiệu **đi ngang**, không giảm nữa ở khoảng **epoch 15 đến 25**, và sau đó có dấu hiệu **tăng dần** lên. Mình có thử huấn luyện ở **100 epochs** và thật sự càng về sau tình trạng **Overfit** hiện ra rõ rệt.\n",
        "\n",
        "> **Vậy điều này do đâu mà ra?** 🤔\n",
        "\n",
        "Có thể mô hình của chúng ta đã bắt đầu đạt đến **giới hạn**, điểm hội tụ của nó với hướng tiếp cận và kiến trúc hiện tại. Nếu càng học thêm, mô hình sẽ bị chú trọng **quá vào dữ liệu train** và làm mất đi tính **khái quát** của chúng. Khi đó, càng học lâu ở các epochs về sau, mặc dù mô hình sẽ tăng độ hiệu quả trên tập **train**, nhưng **loss** của chúng trên **val** sẽ ngày càng **tăng dần**, dẫn đến tình trạng **Overfit** diễn ra nặng hơn. Khi đó, mô hình sẽ không còn hiệu quả và không thể áp dụng thực tế nữa. 🚫"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbFsm-_i_nA-"
      },
      "source": [
        "# Stacking LSTMs  \n",
        "### Tụi mình sẽ tiến hành xây dựng mô hình với **nhiều lớp LSTM hơn** nha. 🌟  \n",
        "\n",
        "Việc sử dụng **nhiều lớp LSTM xếp chồng** lên nhau (stacking) sẽ giúp mô hình có khả năng **học các đặc trưng phức tạp hơn**, tận dụng được sức mạnh của nhiều tầng biểu diễn. Với cách này, chúng ta có thể cải thiện độ chính xác của mô hình, đặc biệt với những bài toán có tính **ngữ nghĩa cao**.  \n",
        "\n",
        "*Lưu ý: Mặc dù stacking các lớp LSTM sẽ tăng khả năng biểu diễn của mô hình, nhưng cũng sẽ **tăng thời gian huấn luyện và tài nguyên sử dụng**, nên mọi người cần cân nhắc kỹ về số lượng lớp sao cho hợp lý nha!* 🚀  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IbX4zWaf_xFk"
      },
      "outputs": [],
      "source": [
        "model1 = Sequential([\n",
        "    Embedding(vocab_size, embedding_dims),\n",
        "    Bidirectional(LSTM(embedding_dims, return_sequences=True)),\n",
        "    Bidirectional(LSTM(embedding_dims)),\n",
        "    Dense(24, activation=\"relu\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "adam = Adam(learning_rate=0.00001, beta_1=0.9, beta_2=0.999)\n",
        "model1.compile(loss=\"binary_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk__FrOVBlog"
      },
      "source": [
        "Ở **lớp LSTM cuối**, mọi người cũng có thể điều chỉnh tham số `return_sequences=True` nha. 🌟 Điều này sẽ khiến lớp **LSTM** trả về **chuỗi các giá trị** tại mỗi bước thời gian tức chuỗi đầu ra đầy đủ, thay vì chỉ trả về **chuỗi giá trị cuối cùng**.  \n",
        "\n",
        "💡 **Khi nào cần sử dụng `return_sequences=True`?**\n",
        "- Nếu bài toán yêu cầu phân tích toàn bộ chuỗi đầu ra của mô hình thay vì chỉ dựa vào trạng thái cuối cùng.  \n",
        "- Điều này đặc biệt hữu ích khi cần xử lý các bài toán như **gắn nhãn chuỗi (sequence labeling)** hoặc **dịch máy (machine translation)**.  \n",
        "\n",
        "✨ Tuy nhiên, trong bài toán phân loại hiện tại, việc trả về giá trị cuối cùng (`return_sequences=False`) là phù hợp hơn. Phần phân tích sâu hơn về tính ứng dụng của `return_sequences=True`mọi người có thể tham khảo ở [blog](https://vi.eitca.org/tr%C3%AD-tu%E1%BB%87-nh%C3%A2n-t%E1%BA%A1o/eitc-ai-tff-tensorflow-nguy%C3%AAn-t%E1%BA%AFc-c%C6%A1-b%E1%BA%A3n/x%E1%BB%AD-l%C3%BD-ng%C3%B4n-ng%E1%BB%AF-t%E1%BB%B1-nhi%C3%AAn-v%E1%BB%9Bi-tensorflow/b%E1%BB%99-nh%E1%BB%9B-ng%E1%BA%AFn-h%E1%BA%A1n-d%C3%A0i-h%E1%BA%A1n-cho-nlp/ki%E1%BB%83m-tra-%C4%91%C3%A1nh-gi%C3%A1-tr%C3%AD-nh%E1%BB%9B-ng%E1%BA%AFn-h%E1%BA%A1n-d%C3%A0i-cho-nlp/t%E1%BA%A7m-quan-tr%E1%BB%8Dng-c%E1%BB%A7a-vi%E1%BB%87c-%C4%91%E1%BA%B7t-tham-s%E1%BB%91-return_sequences-th%C3%A0nh-true-khi-x%E1%BA%BFp-ch%E1%BB%93ng-nhi%E1%BB%81u-l%E1%BB%9Bp-lstm-l%C3%A0-g%C3%AC/) này nha. 🌟"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "R2iXAjKlDLdt"
      },
      "outputs": [],
      "source": [
        "model1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm5V2jkeDOXw"
      },
      "source": [
        "Vì tụi mình đã thêm một lớp **LSTM** nữa, số lượng tham số của mô hình đã tăng thêm khoảng **100.000**, tương đương với tầm **8%**. 📈 Điều này dẫn đến:  \n",
        "\n",
        "- **Thời gian huấn luyện** sẽ lâu hơn. ⏳  \n",
        "- **Chi phí tính toán** cũng tăng lên. 💻  \n",
        "\n",
        "✨ **Tuy nhiên**, nếu kiến trúc mới này mang lại **hiệu quả tốt hơn**, thì sự đánh đổi này hoàn toàn **xứng đáng** nha! 😊"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BK-qRdPJCemL"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "history1 = model1.fit(train_padded, train_labels, epochs=epochs, validation_data=(test_padded, test_labels))\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4aGq316jCiK-"
      },
      "outputs": [],
      "source": [
        "training_time = timedelta(seconds=end_time - start_time)\n",
        "print(f\"Thời gian huấn luyện: {training_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cBZrLj47Cv8x"
      },
      "outputs": [],
      "source": [
        "# vẽ biểu đồ huấn luyện\n",
        "train_acc = history1.history[\"accuracy\"]\n",
        "val_acc = history1.history[\"val_accuracy\"]\n",
        "train_loss = history1.history[\"loss\"]\n",
        "val_loss = history1.history[\"val_loss\"]\n",
        "\n",
        "fig,axs = plt.subplots(1, 2, figsize=(12,4))\n",
        "axs = axs.flatten()\n",
        "axs[0].plot(train_acc, label=\"train\")\n",
        "axs[0].plot(val_acc, label=\"val\")\n",
        "axs[0].set_title(\"Accuracy\")\n",
        "axs[0].legend()\n",
        "axs[1].plot(train_loss, label=\"train\")\n",
        "axs[1].plot(val_loss, label=\"val\")\n",
        "axs[1].set_title(\"Loss\")\n",
        "axs[1].legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "A2SxcNypKfW6"
      },
      "outputs": [],
      "source": [
        "# Độ hiệu quả của mô hình\n",
        "eval = model1.evaluate(test_padded, test_labels)\n",
        "print(f\"Accuracy: \", eval[1])\n",
        "print(f\"Loss: \", eval[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avzvmm1BPyf2"
      },
      "source": [
        "Đây là kết quả ở lần chạy của mình nha."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![model1](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/model1.png?raw=true)"
      ],
      "metadata": {
        "id": "3WFh2EcfHWBx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b6LHXNBJMo5"
      },
      "source": [
        "Tương tự như mô hình có **1 lớp LSTM** trước đó, mô hình này cũng gặp phải tình trạng **Overfitting**. 😔  \n",
        "\n",
        "Thậm chí, quá trình **Overfitting** còn xảy ra **sớm hơn**, cụ thể:  \n",
        "\n",
        "- **Epoch thứ 10**: Đường **loss trên tập val** đã bắt đầu tăng. 📈  \n",
        "- **Epoch thứ 30**: **Loss trên tập val** đạt khoảng **0.7**, cao hơn đáng kể so với mô hình trước đó. 🔺  \n",
        "\n",
        "Điều này cho thấy việc thêm lớp **LSTM** không phải lúc nào cũng mang lại hiệu quả nếu không được xử lý và tối ưu đúng cách. 🚧"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh0qoBQ1K3Hb"
      },
      "source": [
        "> **Vậy có cách nào để tối ưu hóa chúng không?** 🤔  \n",
        "\n",
        "Tạm thời, tụi mình sẽ thử nghiệm nhiều cách khác nhau để **tối ưu hóa** mô hình, giảm thiểu tình trạng **Overfitting** nha. 🚀  \n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj218DMbLFh7"
      },
      "source": [
        "# Optimizing Stacked LSTM  \n",
        "### Tối ưu hóa mô hình sử dụng **các lớp LSTM xếp chồng** 🛠️  \n",
        "\n",
        "Trong phần này, tụi mình sẽ thử áp dụng các phương pháp **tối ưu hóa** để cải thiện mô hình **Stacked LSTM** nhằm giảm thiểu tình trạng **Overfitting** và nâng cao hiệu quả tổng thể của mô hình. 🌟  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK0uW7dkLTzc"
      },
      "source": [
        "Như mọi người thấy ở **chương 6** trước đó, cách đơn giản và hiệu quả nhất mà tụi mình đã tiếp xúc là giảm tốc độ học `learning_rate`. 🌟  \n",
        "\n",
        "Cách này đáng để thử nghiệm xem nó có mang lại hiệu quả tích cực đối với **mô hình mạng hồi quy - RNN** này hay không. 🧠  \n",
        "\n",
        "Tụi mình sẽ giảm tốc độ học `learning_rate` đi khoảng **20%**, tức là:  \n",
        "`learning_rate=0.000008`.  \n",
        "Uầy, số nhỏ thiệt sự. 😅  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sHMM-WIxMeYU"
      },
      "outputs": [],
      "source": [
        "model2 = Sequential([\n",
        "    Embedding(vocab_size, embedding_dims),\n",
        "    Bidirectional(LSTM(embedding_dims, return_sequences=True)),\n",
        "    Bidirectional(LSTM(embedding_dims)),\n",
        "    Dense(24, activation=\"relu\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "adam = Adam(learning_rate=0.000008, beta_1=0.9, beta_2=0.999)\n",
        "model2.compile(loss=\"binary_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "t0v1ngpBM1Mg"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "history2 = model2.fit(train_padded, train_labels, epochs=epochs, validation_data=(test_padded, test_labels))\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r3wtdNYJM7hp"
      },
      "outputs": [],
      "source": [
        "training_time = timedelta(seconds=end_time - start_time)\n",
        "print(f\"Thời gian huấn luyện: {training_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GFmOK0IFM85I"
      },
      "outputs": [],
      "source": [
        "# vẽ biểu đồ huấn luyện\n",
        "train_acc = history2.history[\"accuracy\"]\n",
        "val_acc = history2.history[\"val_accuracy\"]\n",
        "train_loss = history2.history[\"loss\"]\n",
        "val_loss = history2.history[\"val_loss\"]\n",
        "\n",
        "fig,axs = plt.subplots(1, 2, figsize=(12,4))\n",
        "axs = axs.flatten()\n",
        "axs[0].plot(train_acc, label=\"train\")\n",
        "axs[0].plot(val_acc, label=\"val\")\n",
        "axs[0].set_title(\"Accuracy\")\n",
        "axs[0].legend()\n",
        "axs[1].plot(train_loss, label=\"train\")\n",
        "axs[1].plot(val_loss, label=\"val\")\n",
        "axs[1].set_title(\"Loss\")\n",
        "axs[1].legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NduKK0cGx33r"
      },
      "outputs": [],
      "source": [
        "# Độ hiệu quả của mô hình\n",
        "eval = model2.evaluate(test_padded, test_labels)\n",
        "print(f\"Accuracy: \", eval[1])\n",
        "print(f\"Loss: \", eval[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcglNp0Cyq6H"
      },
      "source": [
        "Đây là kết quả ở lần chạy của mình"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![model2](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/model2.png?raw=true)"
      ],
      "metadata": {
        "id": "Lx341krWHcPs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZauIe2KiPZVy"
      },
      "source": [
        "Uầy nhìn sơ qua có vẻ việc giảm **learning rate** không tác động nhiều lăm, hình dáng biểu đồ không thay đổi mấy. Nhưng thật sự, tình trạng Overfiting có vẻ đã giảm đi khi mà giờ đây ở **epoch thứ 30** thì **loss trên val chỉ còn khoảng 0.65**. Dị là tình trạng **Overfiting** đã tới chậm hơn hoặc ít hơn  với trước. Vậy việc giảm đi **learning rate** thật sự hiệu quả."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsVGfxbe3igi"
      },
      "source": [
        "# Using Dropout  \n",
        "### Tiếp đến, tụi mình sẽ thử sử dụng **dropout** để tối ưu hóa nha. 🎯  \n",
        "\n",
        "Dropout là một kỹ thuật thường được sử dụng để giảm thiểu tình trạng **Overfitting** bằng cách tạm thời \"tắt\" ngẫu nhiên một số **neuron** trong quá trình huấn luyện. Điều này giúp mô hình không quá phụ thuộc vào bất kỳ **neuron** cụ thể nào và tăng khả năng khái quát hóa. 🚀  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL0nB7q5NAyP"
      },
      "outputs": [],
      "source": [
        "model3 = Sequential([\n",
        "    Embedding(vocab_size, embedding_dims),\n",
        "    Bidirectional(LSTM(embedding_dims, return_sequences=True, dropout=0.2)),\n",
        "    Bidirectional(LSTM(embedding_dims, dropout=0.2)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "adam = Adam(learning_rate=0.000008, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "model3.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "history3 = model3.fit(train_padded, train_labels, epochs=epochs, validation_data=(test_padded, test_labels))\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "id": "Y950xXds9ykp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vẽ biểu đồ huấn luyện\n",
        "train_acc = history3.history[\"accuracy\"]\n",
        "val_acc = history3.history[\"val_accuracy\"]\n",
        "train_loss = history3.history[\"loss\"]\n",
        "val_loss = history3.history[\"val_loss\"]\n",
        "\n",
        "fig,axs = plt.subplots(1, 2, figsize=(12,4))\n",
        "axs = axs.flatten()\n",
        "axs[0].plot(train_acc, label=\"train\")\n",
        "axs[0].plot(val_acc, label=\"val\")\n",
        "axs[0].set_title(\"Accuracy\")\n",
        "axs[0].legend()\n",
        "axs[1].plot(train_loss, label=\"train\")\n",
        "axs[1].plot(val_loss, label=\"val\")\n",
        "axs[1].set_title(\"Loss\")\n",
        "axs[1].legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d_15QzDx_Xuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_time = str(timedelta(seconds=end_time - start_time))\n",
        "print(f\"Thời gian huấn luyện mô hình: {training_time}\")"
      ],
      "metadata": {
        "id": "qtIX0l0B994G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Đánh giá mô hình\n",
        "eval = model3.evaluate(test_padded, test_labels)\n",
        "print(f\"Loss: {eval[0]}, Accuracy: {eval[1]}\")"
      ],
      "metadata": {
        "id": "jPWj3as5-Jmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đây là kết quả ở lần chạy của mình nha."
      ],
      "metadata": {
        "id": "0Av-y4TE_xa2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![model3](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/model3.png?raw=true)"
      ],
      "metadata": {
        "id": "stdVNCHEHvnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Như mọi người có thể thấy, việc **sử dụng lớp dropout** không ảnh hưởng nhiều đến chỉ số **accuracy**, ít nhất là nó không giảm đi – điều này có vẻ khá tích cực. 😊\n",
        "\n",
        "Trước đây, sau vài lần sử dụng **dropout**, chúng ta có một ấn tượng xấu rằng việc tắt đi một vài nơ-ron có thể làm mô hình học ít hơn và trở nên kém hiệu quả.\n",
        "\n",
        "Nhưng thật bất ngờ! 🎉  \n",
        "\n",
        "Ở lần huấn luyện này, mô hình đã chứng minh rằng phương pháp **Dropout thật sự hiệu quả**, đặc biệt khi chỉ số **loss trên val** đã giảm đi rõ rệt, **chỉ còn khoảng dưới 0.5**. ✅\n",
        "\n",
        "**Kết luận:**  \n",
        "- Chúng ta có thể **kết hợp thêm Dropout** để cải thiện mô hình trong trường hợp này.  \n",
        "- Đối với các mô hình có kiến trúc phức tạp, việc sử dụng **Dropout** lại mang đến kết quả khả quan và hiệu quả. 🌟"
      ],
      "metadata": {
        "id": "jZa9pdfi_1Hq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mọi người có thể thử thêm nhiều kỹ thuật khác nữa để phòng tránh tình trạng **Overfiting**, chẳng hạn như:  \n",
        "\n",
        "- Làm sạch và xử lý dữ liệu 📂  \n",
        "- Tinh chỉnh các tham số, siêu tham số giống như đã làm ở **chương 6** ⚙️  \n",
        "\n",
        "### **Tiếp theo:**  \n",
        "Tụi mình sẽ thử sử dụng **phương pháp học chuyển giao (transfer learning)** bằng cách sử dụng các bộ **Embedding** đã được huấn luyện sẵn. 🛠️ Qua đó, chúng ta sẽ đánh giá xem thử phương pháp này có mang lại hiệu quả như mong đợi không nhé! 🚀  "
      ],
      "metadata": {
        "id": "9eCohjcXCKDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Pretrained Embeddings with RNNs  \n",
        "### Sử dụng các lớp **Embedding** đã được huấn luyện sẵn vào mô hình **RNN**. 🌟  "
      ],
      "metadata": {
        "id": "bvIEdv_KC35F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tụi mình sẽ sử dụng các **bộ Embeddings đã được huấn luyện trước** để áp dụng vô mô hình thay vì phải tự huấn luyện nha. 🌟 Giống như cách chúng ta load bộ **Embedding từ HuggingFace** hôm trước vậy á.  \n",
        "\n",
        "Ở đây tụi mình sẽ sử dụng **GloVe (Global Vectors for Word Representation)** nha. Nó có nhiều phiên bản được huấn luyện trên nhiều bộ dữ liệu khác nhau. 🛠️ Ở đây tui chọn bộ sử dụng dữ liệu **Twitter** với 27 tỷ tokens và khoảng 1.2 triệu từ vựng nha. Chúng có 4 phiên bản lần lượt là 25, 50, 100 và 200 **chiều embeddings**.  \n",
        "\n",
        "➡️ Để đơn giản mình chỉ chọn phiên bản 25 chiều thôi nha, mọi người có thể thử thêm những cái khác. 🚀  "
      ],
      "metadata": {
        "id": "L63pOIhcF3gI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tải bộ dữ liệu\n",
        "!wget --no-check-certificate \\\n",
        "  https://nlp.stanford.edu/data/glove.twitter.27B.zip \\\n",
        "  -O /tmp/glove.zip"
      ],
      "metadata": {
        "id": "hpLdREnBHOVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/tmp/glove.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/tmp/glove')\n"
      ],
      "metadata": {
        "id": "6u8FX31vDCuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir('/tmp/glove'))"
      ],
      "metadata": {
        "id": "wThh5b5-Lwu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sau khi giải nén ra, thư mục của mọi người sẽ có 4 file như này, tương ứng với 4 loại chiều là **200**, **100**, **50**, và **25**. 📂\n",
        "\n",
        "```python\n",
        "['glove.twitter.27B.200d.txt', 'glove.twitter.27B.100d.txt', 'glove.twitter.27B.50d.txt', 'glove.twitter.27B.25d.txt']\n",
        "```"
      ],
      "metadata": {
        "id": "bGawgz7VzhS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tụi mình sẽ dùng loại **25 chiều** nha. 📄 Mọi người có thể tải file về và kiểm tra nội dung bên trong thử.\n",
        "\n",
        "Ở đây, mỗi hàng của chúng tương ứng với **một từ**, và theo sau đó là **các hệ số vector** tương ứng đã học được trong **không gian 25 chiều**. 🚀"
      ],
      "metadata": {
        "id": "QmDotKpt1An7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove_embeddings = dict()\n",
        "f = open(\"/tmp/glove/glove.twitter.27B.25d.txt\")\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  glove_embeddings[word] = coefs\n",
        "\n",
        "f.close()"
      ],
      "metadata": {
        "id": "On4m2Des0A7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quan sát 10 từ cuối cùng trong từ điển thử\n",
        "list(glove_embeddings.items())[-5:]"
      ],
      "metadata": {
        "id": "asvsaMDv0V2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uầy, mọi người có thể thấy chúng thậm chí còn bao gồm cả **các vector biểu diễn của tiếng Nhật** nữa! 🌸 Bộ **embeddings** này thực sự rất phong phú và đa dạng. 🚀"
      ],
      "metadata": {
        "id": "vrBiNDmG3Hmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bây giờ tụi mình sẽ tiến hành tạo **ma trận trọng số** cho lớp **embeddings** thông qua biến `embedding_matrix` nha. 🌟\n",
        "\n",
        "Cách làm như sau:  \n",
        "- Duyệt qua **các từ trong từ điển** của tokenizer đã tạo trước đó.  \n",
        "- **Trích xuất các vector biểu diễn tương ứng** từ bộ **embeddings đã huấn luyện sẵn**.  \n",
        "- Lưu các vector này vào `embedding_matrix` để làm trọng số ban đầu cho lớp **Embedding** trong mô hình của chúng ta. 💡"
      ],
      "metadata": {
        "id": "9M23Klih3VkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Định nghĩa lại số chiều là 25\n",
        "embedding_dims = 25\n",
        "# Định nghĩa số lượng từ tương ứng với từ điển ban đầu\n",
        "vocab_size = 20000\n",
        "# Tạo ma trận rỗng với tất cả phần tử là 0 trước để tiến hành từ từ lưu trọng số các từ.\n",
        "embeddings_matrix = np.zeros((vocab_size, embedding_dims))\n",
        "# Tiến hành duyệt qua các từ bên trong bộ từ điển\n",
        "for word, index in tokenizer.word_index.items():\n",
        "  if index > vocab_size - 1: # Nếu vượt qua số lượng từ thì dừng vòng lặp\n",
        "    break\n",
        "  else: # Tiến hành lưu các trọng số lại tương ứng với từng từ\n",
        "    embedding_vector = glove_embeddings.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embeddings_matrix[index] = embedding_vector\n"
      ],
      "metadata": {
        "id": "801otvtn0t1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kiểm tra ma trận trọng số lớp embeddings\n",
        "embeddings_matrix"
      ],
      "metadata": {
        "id": "6oLTngbn6qdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vậy là tụi mình đã chuẩn bị xong phần **trọng số cho lớp embeddings** rồi nha! 🌟  \n",
        "Bây giờ, bước tiếp theo là:\n",
        "\n",
        "👉 **Ráp phần trọng số này vào lớp embeddings** trong mô hình.  \n",
        "\n",
        "### Các bước thực hiện:\n",
        "1. **Tạo lớp Embedding** với tham số `weights` được truyền vào từ ma trận `embedding_matrix` đã chuẩn bị.  \n",
        "2. Đặt tham số `trainable=False` để đảm bảo rằng các vector embeddings đã huấn luyện sẵn sẽ không bị thay đổi trong quá trình học.  \n",
        "\n",
        "Với các bước trên, lớp **Embedding** của chúng ta sẽ tận dụng được toàn bộ sức mạnh của bộ **GloVe embeddings** đã được huấn luyện trên dữ liệu lớn. 🚀"
      ],
      "metadata": {
        "id": "SMjJR4Vu6v9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model4 = Sequential([\n",
        "    Embedding(vocab_size, embedding_dims,\n",
        "               weights=[embeddings_matrix], trainable=False),\n",
        "    Bidirectional(LSTM(embedding_dims, return_sequences=True)),\n",
        "    Bidirectional(LSTM(embedding_dims)),\n",
        "    Dense(24, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "adam = Adam(learning_rate=0.00001)\n",
        "model4.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "dpwUu_407G0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=30\n",
        "start_time = time.time()\n",
        "history4 = model4.fit(train_padded, train_labels, epochs=epochs, validation_data=(test_padded, test_labels))\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "id": "DdfC29G38oit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_time = str(timedelta(seconds=end_time - start_time))\n",
        "print(f\"Thời gian huấn luyện: {training_time}\")"
      ],
      "metadata": {
        "id": "ht0TcwHw_JXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vẽ biểu đồ quá trình huấn luyện\n",
        "train_acc = history4.history['accuracy']\n",
        "val_acc = history4.history['val_accuracy']\n",
        "train_loss = history4.history['loss']\n",
        "val_loss = history4.history['val_loss']\n",
        "\n",
        "fig,axs = plt.subplots(1, 2, figsize=(12,4))\n",
        "axs = axs.flatten()\n",
        "axs[0].plot(train_acc, label='train')\n",
        "axs[0].plot(val_acc, label='val')\n",
        "axs[0].set_title('Accuracy')\n",
        "axs[0].legend()\n",
        "\n",
        "axs[1].plot(train_loss, label='train')\n",
        "axs[1].plot(val_loss, label='val')\n",
        "axs[1].set_title('Loss')\n",
        "axs[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lZsk2UmP_Ts4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đây là kết quả ở lần chạy của mình."
      ],
      "metadata": {
        "id": "xQl866YDF8_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![model4](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/model4.png?raw=true)"
      ],
      "metadata": {
        "id": "FShyTbIiIleM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uầy, mọi người thấy rõ ràng rồi ha! 🌟  \n",
        "Chúng ta đã **khắc phục được tình trạng Overfitting** trong khoảng **30 epochs đầu**. Thật sự là một kết quả **rất ấn tượng**. 🎉  \n",
        "\n",
        "👉 Nếu tiếp tục **tăng số lượng epochs lên** và huấn luyện lâu hơn nữa, có khả năng rằng mô hình sẽ **còn hiệu quả hơn** nữa đấy. 🚀  "
      ],
      "metadata": {
        "id": "CSbtG03aFJvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mặc dù mô hình bây giờ đã hiệu quả hơn và giảm thiểu tình trạng **Overfiting** đi một cách rõ rệt, nhưng có thể trong tương lai bạn vẫn muốn cải thiện hơn nữa. Lúc này, bạn có thể nghĩ đến các phương pháp như **điều chỉnh lại các tham số**, điển hình là `**vocab_size**` mà chúng ta đã sử dụng ở chương trước để hạn chế tình trạng **Overfiting**. 🌟  \n",
        "\n",
        "Ở chương trước, thông qua việc **giảm kích thước từ điển**, chúng ta đã giúp mô hình chú trọng vào **các từ phổ biến hơn**.  \n",
        "\n",
        "> **Vậy chúng ta có thể chọn con số nào là hợp lý cho vocab_size?**  \n",
        "\n",
        "Mình nghĩ chúng ta cần phải **quan sát kỹ lưỡng** hơn trước khi đưa ra quyết định.  \n",
        "\n",
        "Biết rằng **bộ Embedding được huấn luyện sẵn** này có đến **1,2 triệu từ**, nhưng chúng ta **không thể đảm bảo** rằng tất cả các từ trong bộ dữ liệu huấn luyện của chúng ta đều có mặt trong **bộ Embedding** này. 🔍"
      ],
      "metadata": {
        "id": "PBAIK35_GB8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xs = []\n",
        "ys = [] # Đánh dấu xem từ ở index đó tồn tại trong bộ Embedding hay chưa, nếu có là 1, không có là 0\n",
        "cumulative_x = []\n",
        "cumulative_y = [] # Tỷ lệ của từ ở mỗi bước thời gian, dùng để kiểm tra xem tỷ lệ có bao nhiêu từ có trong embedding Glove đã huấn luyện.\n",
        "total_y = 0\n",
        "for word, index in tokenizer.word_index.items():\n",
        "  xs.append(index)\n",
        "  cumulative_x.append(index)\n",
        "  if glove_embeddings.get(word) is not None:\n",
        "    total_y += 1\n",
        "    ys.append(1)\n",
        "  else:\n",
        "    ys.append(0)\n",
        "  cumulative_y.append(total_y/index) # Kiểm tra lại tỷ lệ xuất hiện các từ trong từng bước thời gian."
      ],
      "metadata": {
        "id": "IBeov1AQJIYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tỷ lệ các từ trong bộ từ diển tokenizer tồn tại trong embedding được huấn luyện sẵn: \", cumulative_y[-1])"
      ],
      "metadata": {
        "id": "_5AY08bnLqjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trực quan hóa kết quả lên bằng biểu đồ mật độ.\n",
        "fig, ax = plt.subplots(figsize=(12, 2))\n",
        "ax.spines['top'].set_visible(False)\n",
        "plt.margins(x=0, y=None, tight=True)\n",
        "plt.fill(ys)"
      ],
      "metadata": {
        "id": "5iowhgfPLF90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mọi người có thể thấy có một **ranh giới rõ rệt** ở giữa khoảng **10.000 và 15.000**, tạo thành một đường phân cách. Qua quan sát kỹ lưỡng, mình nghĩ khoảng **13.000** là con số hợp lý.  \n",
        "\n",
        "Từ thời điểm các token có **index bằng 13.000** trở đi, số lượng các từ **không nằm trong bộ Embedding** tăng lên rõ rệt.  \n",
        "\n",
        "Bây giờ tụi mình sẽ sử dụng các biến `cumulative_x` và `cumulative_y` để vẽ biểu đồ về **tần suất xuất hiện** của các từ trong **từ điển Tokenizer** bên trong **bộ Embedding**, qua từng thời điểm nha! 📊"
      ],
      "metadata": {
        "id": "nnfaliFrMcs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(cumulative_x, cumulative_y)\n",
        "plt.xlabel(\"Index của từ trong từ điển Tokenizer\")\n",
        "plt.ylabel(\"Tỷ lệ các từ xuất hiện trong từng bước thời gian\")\n",
        "plt.axis([0, 25000, 0.915, 0.985]) # Quan sát trong khoảng x từ 0 đến 25.000, y từ 0.915 đến 0.985"
      ],
      "metadata": {
        "id": "_4ZPqEuBNrI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qua biểu đồ, mọi người có thể thấy rõ **điểm gãy** xuất hiện trong khoảng **10.000 đến 15.000**.  \n",
        "\n",
        "Mọi người có thể sử dụng **`plt.axis()`** để zoom to hình ảnh trong khoảng đó, giúp quan sát chi tiết hơn, rõ hơn nha. Sau khi quan sát kỹ, tác giả quyết định chọn kích thước cho bộ từ điển là **13.200**, thay vì **2.000** như ở chương trước hoặc **20.000** như ở mô hình trước. 📏✨"
      ],
      "metadata": {
        "id": "PnyZGfw1OlMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Định nghĩa lại số chiều là 25\n",
        "embedding_dims = 25\n",
        "# Định nghĩa lại số lượng từ\n",
        "vocab_size = 13200\n",
        "# Tạo ma trận rỗng với tất cả phần tử là 0 trước để tiến hành từ từ lưu trọng số các từ.\n",
        "embeddings_matrix = np.zeros((vocab_size, embedding_dims))\n",
        "# Tiến hành duyệt qua các từ bên trong bộ từ điển\n",
        "for word, index in tokenizer.word_index.items():\n",
        "  if index > vocab_size - 1: # Nếu vượt qua số lượng từ thì dừng vòng lặp\n",
        "    break\n",
        "  else: # Tiến hành lưu các trọng số lại tương ứng với từng từ\n",
        "    embedding_vector = glove_embeddings.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embeddings_matrix[index] = embedding_vector\n"
      ],
      "metadata": {
        "id": "mGI-BDW9RZQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model5 = Sequential([\n",
        "    Embedding(vocab_size, embedding_dims,\n",
        "               weights=[embeddings_matrix], trainable=False),\n",
        "    Bidirectional(LSTM(embedding_dims, return_sequences=True)),\n",
        "    Bidirectional(LSTM(embedding_dims)),\n",
        "    Dense(24, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "adam = Adam(learning_rate=0.00001)\n",
        "model5.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "FBlmOh-hSvZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=30\n",
        "start_time = time.time()\n",
        "history5 = model5.fit(train_padded, train_labels, epochs=epochs, validation_data=(test_padded, test_labels))\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "id": "LjTYxO6rSvZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_time = str(timedelta(seconds=end_time - start_time))\n",
        "print(f\"Thời gian huấn luyện: {training_time}\")"
      ],
      "metadata": {
        "id": "fv4yzSW2SvZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vẽ biểu đồ quá trình huấn luyện\n",
        "train_acc = history5.history['accuracy']\n",
        "val_acc = history5.history['val_accuracy']\n",
        "train_loss = history5.history['loss']\n",
        "val_loss = history5.history['val_loss']\n",
        "\n",
        "fig,axs = plt.subplots(1, 2, figsize=(12,4))\n",
        "axs = axs.flatten()\n",
        "axs[0].plot(train_acc, label='train')\n",
        "axs[0].plot(val_acc, label='val')\n",
        "axs[0].set_title('Accuracy')\n",
        "axs[0].legend()\n",
        "\n",
        "axs[1].plot(train_loss, label='train')\n",
        "axs[1].plot(val_loss, label='val')\n",
        "axs[1].set_title('Loss')\n",
        "axs[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3vdeN8N3SvZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval = model5.evaluate(test_padded, test_labels)\n",
        "print(\"Loss: \", eval[0])\n",
        "print(\"Accuracy: \", eval[1])"
      ],
      "metadata": {
        "id": "B_oRs-4pS3gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Đánh giá mô hình\n",
        "eval = model5.evaluate(test_padded, test_labels)\n",
        "print(\"Loss: \", eval[0])\n",
        "print(\"Accuracy: \", eval[1])"
      ],
      "metadata": {
        "id": "i49fBuzIW5j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đây là kết quả ở lần chạy của mình."
      ],
      "metadata": {
        "id": "T7vul3q6YUKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![model5](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/model5.png?raw=true)"
      ],
      "metadata": {
        "id": "J-bPl4sgJRal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mọi người có thể thấy đường **val_loss** và **train_loss** đã trở nên **mượt hơn** một chút. Trong khoảng **30 epochs** đầu, mô hình không bị **Overfit**, và sự chênh lệch giữa **train_loss** và **val_loss** cũng ít hơn. ✨\n",
        "\n",
        "Bây giờ, tụi mình sẽ tiếp tục huấn luyện **lâu hơn** để kiểm tra:  \n",
        "- Mô hình có tiếp tục **cải thiện** không?  \n",
        "- **Giới hạn** của nó đạt được ở đâu?  \n",
        "- Khi nào thì mô hình bắt đầu xuất hiện **Overfit**?  \n",
        "\n",
        "Mình sẽ tạm thời chọn số epochs là **100** nha. ⏳"
      ],
      "metadata": {
        "id": "nnc57mYhWywq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model6 = Sequential([\n",
        "    Embedding(vocab_size, embedding_dims,\n",
        "               weights=[embeddings_matrix], trainable=False),\n",
        "    Bidirectional(LSTM(embedding_dims, return_sequences=True)),\n",
        "    Bidirectional(LSTM(embedding_dims)),\n",
        "    Dense(24, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "adam = Adam(learning_rate=0.00001)\n",
        "model6.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Y4OrhhbHX7qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=100\n",
        "start_time = time.time()\n",
        "history6 = model6.fit(train_padded, train_labels, epochs=epochs, validation_data=(test_padded, test_labels))\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "id": "HVpNjO5kX7qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_time = str(timedelta(seconds=end_time - start_time))\n",
        "print(f\"Thời gian huấn luyện: {training_time}\")"
      ],
      "metadata": {
        "id": "O7LEx3coX7qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vẽ biểu đồ quá trình huấn luyện\n",
        "train_acc = history6.history['accuracy']\n",
        "val_acc = history6.history['val_accuracy']\n",
        "train_loss = history6.history['loss']\n",
        "val_loss = history6.history['val_loss']\n",
        "\n",
        "fig,axs = plt.subplots(1, 2, figsize=(12,4))\n",
        "axs = axs.flatten()\n",
        "axs[0].plot(train_acc, label='train')\n",
        "axs[0].plot(val_acc, label='val')\n",
        "axs[0].set_title('Accuracy')\n",
        "axs[0].legend()\n",
        "\n",
        "axs[1].plot(train_loss, label='train')\n",
        "axs[1].plot(val_loss, label='val')\n",
        "axs[1].set_title('Loss')\n",
        "axs[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jifa4zGHX7qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval = model6.evaluate(test_padded, test_labels)\n",
        "print(\"Loss: \", eval[0])\n",
        "print(\"Accuracy: \", eval[1])"
      ],
      "metadata": {
        "id": "85douhpOX7qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đây là kết quả ở lần chạy của mình nha."
      ],
      "metadata": {
        "id": "mggQ4lDOKhp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![model6](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/model6.png?raw=true)"
      ],
      "metadata": {
        "id": "SjabQmp6Kkv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uầy, ấn tượng thật đấy, đến epoch thứ 100 rồi, mặc dù tốc độ giảm đi của **loss** đã giảm nhưng mà vẫn chưa thấy tình trạng **Overfit** xảy ra. Đồng thời **val loss** của mô hình vẫn có dấu hiệu  tiếp tục giảm, ở epoch 100 giảm chỉ còn khoảng **0.525**, và vẫn có khả năng giảm nữa trong tương lai khi huấn luyện thêm.\n",
        "\n",
        "Dị là quá trình **Overfit** đã tới chậm hơn rất nhiều, có thể mọi người train thêm sẽ còn cải thiện hơn nhiều nữa, cho đến khi **train loss** giảm mà **val loss** lại tăng thì tình trạng **Overfit** mới thực sự bắt đầu.\n",
        "\n",
        "Phương pháp tối ưu hóa này của chúng ta thật sự hiệu quả. 🌟\n",
        "\n",
        "Trong thực tế thì mọi người cứ huấn luyện với một số epoch khá lớn cho tới khi nào hết thì thôi nếu mọi người có dư tài nguyên. Trong quá trình huấn luyện đó, chúng ta sẽ lưu lại **trọng số cho ra kết quả đánh giá cao nhất** của mô hình.\n",
        "\n",
        "Mọi người chỉ cần tạo **check_point** để lưu lại **trọng số hiệu quả nhất** cho mô hình là được. Thường thì việc này sẽ dựa trên thang đo, có 2 trọng số thường được lưu  là trọng số khiến mô hình có **accuracy cao nhất** và trọng số khiến mô hình có **loss thấp nhất**\n",
        "\n",
        "Mọi người có thể thử đoạn code này để lưu lại trọng số nha. Mấu chốt chính là ở phần `callbacks` để thiết lập **checkpoint**.\n",
        "\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Callback để lưu model weights có accuracy cao nhất\n",
        "checkpoint_accuracy = ModelCheckpoint(\n",
        "    filepath='best_accuracy_model.h5',  # Đường dẫn lưu trữ weights\n",
        "    monitor='val_accuracy',             # Theo dõi metric val_accuracy\n",
        "    save_best_only=True,                # Chỉ lưu weights nếu tốt hơn model trước đó\n",
        "    mode='max',                         # Lưu dựa trên giá trị lớn nhất\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Callback để lưu model weights có loss thấp nhất\n",
        "checkpoint_loss = ModelCheckpoint(\n",
        "    filepath='best_loss_model.h5',      # Đường dẫn lưu trữ weights\n",
        "    monitor='val_loss',                 # Theo dõi metric val_loss\n",
        "    save_best_only=True,                # Chỉ lưu weights nếu tốt hơn model trước đó\n",
        "    mode='min',                         # Lưu dựa trên giá trị nhỏ nhất\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Thêm vào quá trình huấn luyện mô hình\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    train_labels,\n",
        "    validation_data=(val_data, val_labels),\n",
        "    epochs=50,\n",
        "    callbacks=[checkpoint_accuracy, checkpoint_loss]\n",
        ")\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "HJPhs4_8ao65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✨ Trong trường hợp mọi người muốn lưu hết tất cả lại luôn thì phải làm sao? Mọi người chỉ cần tắt tính năng `save_best_only` là được nha, chuyển tham số này sang **False** và điều chỉnh cú pháp filepath lại. 😊\n",
        "\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Tạo callback để lưu weights sau mỗi epoch\n",
        "checkpoint_all_epochs = ModelCheckpoint(\n",
        "    filepath='weights_epoch_{epoch:02d}.h5',  # Tên file bao gồm số epoch\n",
        "    save_best_only=False,                    # Lưu ở mọi epoch, không chỉ best weights\n",
        "    verbose=1                                # Hiển thị log lưu file\n",
        ")\n",
        "\n",
        "# Huấn luyện mô hình\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    train_labels,\n",
        "    validation_data=(val_data, val_labels),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[checkpoint_all_epochs]\n",
        ")\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "vTDhW1nXbnSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bây giờ chúng ta tiến hành kiểm tra mô hình với các câu thực tế thử nhá."
      ],
      "metadata": {
        "id": "axCgSM-edR9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dữ liệu câu và nhãn\n",
        "test_real_sentences = [\n",
        "    \"Oh great, another Monday morning. Just what I needed!\",\n",
        "    \"Thanks for showing up on time, as always.\",\n",
        "    \"Wow, that’s exactly how I told you not to do it.\",\n",
        "    \"You’re so organized; I can totally see that from the mess on your desk.\",\n",
        "    \"Oh sure, because ignoring the problem will definitely make it go away.\",\n",
        "    \"I really appreciate your help with this project.\",\n",
        "    \"The view from this hill is absolutely stunning.\",\n",
        "    \"You’ve done a great job organizing the event.\",\n",
        "    \"I’m so happy to hear about your promotion. Congratulations!\",\n",
        "    \"This book has some very insightful ideas.\"\n",
        "]\n",
        "\n",
        "correct_labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]  # 1: Mỉa mai, 0: Không mỉa mai"
      ],
      "metadata": {
        "id": "oIlfkoMqdQfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_real_sequences = tokenizer.texts_to_sequences(test_real_sentences)\n",
        "max_len = 100\n",
        "test_real_padded = pad_sequences(test_real_sequences, maxlen=max_len, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "ur_3hrPJdp5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model6.predict(test_real_padded)\n",
        "print(f\"Kết quả dự đoán theo tỷ lệ: {result}\")\n",
        "result_binary = [1 if x > 0.5 else 0 for x in result]\n",
        "print(f\"Kết quả dự đoán theo nhãn: {result_binary}\")"
      ],
      "metadata": {
        "id": "DfLpI_hoeCMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✨Training Model with Pretrained Embedding from HuggingFace  \n",
        "### Huấn luyện mô hình với bộ Embedding lấy từ HuggingFace  \n",
        "\n",
        "Phần này không có trong sách mà là mình tự làm thêm nha, xem thử hiệu quả cao hơn bao nhiêu.  \n",
        "\n",
        "Vì đoạn code dưới đây khá rối nên mọi người đọc kỹ phần mô tả này giúp mình nha. Trong lúc làm, mình gặp vấn đề về **giới hạn bộ nhớ và GPU** trong suốt quá trình huấn luyện nên mình không thể huấn luyện trên **Colab** được, thay vào đó sử dụng **Kaggle** kết hợp với một vài mẹo. Mọi người có thể bật bài trên **Kaggle** để coi đầy đủ [ở đây](https://www.kaggle.com/code/trnhhunhthnhkhang/chapter-7) nha. 😊\""
      ],
      "metadata": {
        "id": "LZ4ivKldok30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✍️ Mô tả quá trình thực hiện:\n",
        "\n",
        "1. Load bộ **Embedding**: **bert-base-uncased**.  \n",
        "2. Tiến hành **Embedding** bộ dữ liệu của tụi mình: như mình nói vì bộ nhớ RAM gặp giới hạn nên mình đã lưu lại vào trong file `.h5` xong giải phóng bộ nhớ ở từng đợt.  \n",
        "3. Sau khi **Embedding** xong hết thì tiến hành lấy ngược bộ dữ liệu từ file `.h5` ra để tạo dataset huấn luyện mô hình. 📂"
      ],
      "metadata": {
        "id": "l3LqZ_B4IFIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kết quả\n",
        "Thời gian huấn luyện: **5h 22m 28.845990s**\n",
        "\n",
        "Nói chung là rất lâu =))\n",
        "\n",
        "Nhưng kết quả đạt được không ổn lắm, mình thấy khá là tệ, có thể do việc tăng chiều dữ liệu lên rất nhiều (768) nhưng mô hình lại không thể quan  sát hết được dẫn đến hiệu quả tệ, mọi người có góp ý cải tiến nào thì có thể thử hoặc liên lạc với mình nha. Mình cảm ơn nha."
      ],
      "metadata": {
        "id": "RVz-QLtfH-_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![model_hf](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_7/model_hf.png?raw=true)"
      ],
      "metadata": {
        "id": "jzrrB3bvLiXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h5py -q"
      ],
      "metadata": {
        "id": "16UM_KCvBBuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import timedelta\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "import h5py\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "hEGCtmvG1ku6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure TensorFlow to allow memory growth\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "A19ScLwo7cWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "K.clear_session()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "wiIlOB9X2L8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tải pretrained embedding từ Hugging Face\n",
        "model_name = \"bert-base-uncased\"  # Pretrained model name\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "pretrained_model = TFAutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Freeze pretrained embedding để không train lại\n",
        "pretrained_model.trainable = False\n"
      ],
      "metadata": {
        "id": "nqbdsRXUpFu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 64  # Vì kích thước quá lớn dẫn đến tràn ram nên mình buộc giảm từ 100 xuống còn 64.\n",
        "batch_size = 16   # Mình giảm luôn batch_size để lượng thông tin xử lý một lần không bị tràn ram."
      ],
      "metadata": {
        "id": "Y8Q2XLXm7eMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_save_embeddings(texts, h5_file_path, dataset_name):\n",
        "    \"\"\"\n",
        "    Xử lý văn bản theo batch, chuyển đổi chúng thành embeddings và lưu vào tệp HDF5.\n",
        "\n",
        "    Args:\n",
        "        texts (list): Danh sách các văn bản cần xử lý.\n",
        "        h5_file_path (str): Đường dẫn đến tệp HDF5 để lưu embeddings.\n",
        "        dataset_name (str): Tên dataset trong tệp HDF5.\n",
        "    \"\"\"\n",
        "    num_samples = len(texts)\n",
        "    with h5py.File(h5_file_path, 'w') as h5f:\n",
        "        # Xác định chiều của embedding từ mô hình pretrained\n",
        "        embedding_dim = pretrained_model.config.hidden_size  # 768 cho BERT-base\n",
        "        # Tạo dataset với kích thước phù hợp\n",
        "        h5f.create_dataset(\n",
        "            dataset_name,\n",
        "            shape=(num_samples, max_length, embedding_dim),\n",
        "            dtype='float32'\n",
        "        )\n",
        "\n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "            # Tokenize batch\n",
        "            inputs = tokenizer(\n",
        "                batch_texts,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=max_length,\n",
        "                return_tensors=\"tf\"\n",
        "            )\n",
        "            # Lấy embeddings từ mô hình pretrained\n",
        "            outputs = pretrained_model(**inputs)\n",
        "            # Lưu toàn bộ sequence embeddings (không giảm chiều)\n",
        "            batch_embeddings = outputs.last_hidden_state.numpy()  # Shape: (batch_size, max_length, embedding_dim)\n",
        "            # Lưu vào HDF5\n",
        "            h5f[dataset_name][i:i + batch_size] = batch_embeddings\n",
        "            # Giải phóng bộ nhớ\n",
        "            del batch_texts, inputs, outputs, batch_embeddings\n",
        "            gc.collect()\n",
        "\n",
        "            # In tiến độ\n",
        "            if (i // batch_size) % 100 == 0:\n",
        "                print(f\"Đã xử lý {i} / {num_samples} mẫu\")\n",
        "\n",
        "# Xử lý và lưu embeddings cho dữ liệu huấn luyện\n",
        "preprocess_and_save_embeddings(\n",
        "    texts=train_sequences,\n",
        "    h5_file_path='train_embeddings.h5',\n",
        "    dataset_name='train'\n",
        ")\n",
        "\n",
        "# Xử lý và lưu embeddings cho dữ liệu kiểm tra\n",
        "preprocess_and_save_embeddings(\n",
        "    texts=test_sequences,\n",
        "    h5_file_path='test_embeddings.h5',\n",
        "    dataset_name='test'\n",
        ")"
      ],
      "metadata": {
        "id": "FTk1kucw28yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HDF5DatasetGenerator:\n",
        "    def __init__(self, h5_file_path, dataset_name, labels, batch_size):\n",
        "        self.h5_file_path = h5_file_path\n",
        "        self.dataset_name = dataset_name\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.h5f = h5py.File(h5_file_path, 'r')  # Mở tệp HDF5 ở chế độ đọc\n",
        "        self.dataset = self.h5f[dataset_name]\n",
        "        self.num_samples = self.dataset.shape[0]\n",
        "\n",
        "    def __del__(self):\n",
        "        self.h5f.close()  # Đóng tệp HDF5 khi đối tượng bị hủy\n",
        "\n",
        "    def generator(self):\n",
        "        for i in range(0, self.num_samples, self.batch_size):\n",
        "            batch_embeddings = self.dataset[i:i + self.batch_size]\n",
        "            batch_labels = self.labels[i:i + self.batch_size]\n",
        "            yield batch_embeddings, batch_labels"
      ],
      "metadata": {
        "id": "q4hjnspRLun3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tf_dataset(h5_file_path, dataset_name, labels, batch_size):\n",
        "    \"\"\"\n",
        "    Tạo TensorFlow Dataset từ embeddings được lưu trong tệp HDF5.\n",
        "\n",
        "    Args:\n",
        "        h5_file_path (str): Đường dẫn đến tệp HDF5 chứa embeddings.\n",
        "        dataset_name (str): Tên dataset trong tệp HDF5.\n",
        "        labels (numpy.array): Mảng nhãn tương ứng với embeddings.\n",
        "        batch_size (int): Kích thước batch.\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset: Đối tượng TensorFlow Dataset.\n",
        "    \"\"\"\n",
        "    dataset_generator = HDF5DatasetGenerator(h5_file_path, dataset_name, labels, batch_size)\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        dataset_generator.generator,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(None, max_length, pretrained_model.config.hidden_size), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "# Chuyển đổi nhãn thành numpy arrays\n",
        "train_labels = np.array(train_labels)  # Đảm bảo nhãn ở định dạng numpy array\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "# Tạo TensorFlow Datasets\n",
        "train_dataset = create_tf_dataset(\n",
        "    h5_file_path='train_embeddings.h5',\n",
        "    dataset_name='train',\n",
        "    labels=train_labels,\n",
        "    batch_size=batch_size  # Bạn có thể điều chỉnh dựa trên giới hạn bộ nhớ\n",
        ")\n",
        "\n",
        "test_dataset = create_tf_dataset(\n",
        "    h5_file_path='test_embeddings.h5',\n",
        "    dataset_name='test',\n",
        "    labels=test_labels,\n",
        "    batch_size=batch_size  # Bạn có thể điều chỉnh dựa trên giới hạn bộ nhớ\n",
        ")\n",
        "\n",
        "\n",
        "# Tối ưu hóa hiệu suất của dataset\n",
        "train_dataset = train_dataset.shuffle(buffer_size=10000).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "6bKvOOeaBb2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/GPU:0'):\n",
        "    # Định nghĩa và compile mô hình ở đây\n",
        "    model_hf = Sequential([\n",
        "        Input(shape=(max_length, pretrained_model.config.hidden_size)),\n",
        "        Bidirectional(LSTM(pretrained_model.config.hidden_size, return_sequences=True)),\n",
        "        Bidirectional(LSTM(pretrained_model.config.hidden_size)),\n",
        "        Dense(24, activation=\"relu\"),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "\n",
        "    # Compile mô hình\n",
        "    adam = Adam(learning_rate=1e-4)\n",
        "    model_hf.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=adam,\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "\n",
        "# Hiển thị kiến trúc mô hình\n",
        "model_hf.summary()"
      ],
      "metadata": {
        "id": "mlxBMKirBkOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in model_hf.layers:\n",
        "    for var in layer.trainable_variables:\n",
        "        print(f\"Lớp: {layer.name}, Biến: {var.name}, Thiết bị: {var.device}\")\n"
      ],
      "metadata": {
        "id": "wD1YkFWkQR7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "start_time = time.time()\n",
        "history_hf = model_hf.fit(train_dataset, epochs=epochs, validation_data=test_dataset)\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "id": "tMPkO5vdC2Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_time = str(timedelta(seconds=end_time - start_time))\n",
        "print(f\"Thời gian huấn luyện: {training_time}\")"
      ],
      "metadata": {
        "id": "If13J_qRrgca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vẽ biểu đồ quá trình huấn luyện\n",
        "train_acc = history_hf.history['accuracy']\n",
        "val_acc = history_hf.history['val_accuracy']\n",
        "train_loss = history_hf.history['loss']\n",
        "val_loss = history_hf.history['val_loss']\n",
        "\n",
        "fig,axs = plt.subplots(1, 2, figsize=(12,4))\n",
        "axs = axs.flatten()\n",
        "axs[0].plot(train_acc, label='train')\n",
        "axs[0].plot(val_acc, label='val')\n",
        "axs[0].set_title('Accuracy')\n",
        "axs[0].legend()\n",
        "\n",
        "axs[1].plot(train_loss, label='train')\n",
        "axs[1].plot(val_loss, label='val')\n",
        "axs[1].set_title('Loss')\n",
        "axs[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "04O-sD9GrlNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✨ Summary  \n",
        "###  Tổng kết chương 7.  \n",
        "\n",
        "Vậy là mọi người đã có thể đi đến cuối và hoàn thành hết chương 7 rồi. Cảm ơn mọi người đã theo dõi rất nhiều. 🙌 Về phần nội dung chúng ta đã học trong chương này bao gồm:  \n",
        "\n",
        "- Mạng hồi quy là gì? Thế nào là mạng hồi quy? Cơ chế hoạt động của chúng.  \n",
        "- Bộ nhớ ngắn hạn dài **(LSTM)**.  \n",
        "- Cách áp dụng **LSTM** vào mô hình để tăng độ hiệu quả.  \n",
        "- Tiếp cận với tình trạng **Overfit** trong mạng hồi quy **RNN** và cách để tối ưu chúng. 🌟"
      ],
      "metadata": {
        "id": "TYZAfv4jwU5I"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FQHyC-A4Gqg_",
        "6uSuCn9G0Mu-",
        "hrjvRgqGK280",
        "pLP75bnQDM6b"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}